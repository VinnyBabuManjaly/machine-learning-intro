{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2689e5",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Regularization in machine learning is a technique to optimize models by controlling model complexity and preventing overfitting or underfitting. It achieves this by adding a penalty term to the original loss function, penalizing large model parameters (weights) to keep the model simpler and more generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815c956",
   "metadata": {},
   "source": [
    "### Types of Regularization: Ridge (L2) and LASSO (L1)\n",
    "\n",
    "- **Ridge Regularization (L2)**: Uses the L2 norm of the weight vector, which is the sum of the squares of the weights. The loss function minimized is the original error plus a penalty term proportional to the squared magnitude of the coefficients:  \n",
    "  $$\n",
    "  \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^p w_j^2\n",
    "  $$\n",
    "  Here, $\\alpha$ is the regularization strength controlling how much to shrink the coefficients. Ridge regression shrinks coefficients towards zero but does not set them exactly to zero, thus retaining all features but reducing their impact. It helps control multicollinearity and reduces overfitting by simplifying the model.\n",
    "\n",
    "- **LASSO Regularization (L1)**: Uses the L1 norm, the sum of absolute values of the weights:  \n",
    "  $$\n",
    "  \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^p |w_j|\n",
    "  $$\n",
    "  LASSO can shrink some coefficients to exactly zero, effectively performing feature selection by excluding non-important features from the model. This sparsity helps interpretability and model simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94402410",
   "metadata": {},
   "source": [
    "### Role of the Alpha Parameter\n",
    "\n",
    "The parameter $\\alpha$ controls model complexity:  \n",
    "\n",
    "- Small $\\alpha$ means weak regularization and a complex model closer to standard linear regression (weights are larger). \n",
    "\n",
    "- Large $\\alpha$ means strong regularization, shrinking weights towards zero and resulting in a simpler model that uses features less aggressively.  \n",
    "\n",
    "As $\\alpha$ increases, the sum of squares of weights decreases, reducing the model's expressiveness and training error generally increases due to reduced overfitting.\n",
    "\n",
    "This relationship can be visualized by plotting training mean squared error against $\\alpha$ or its inverse. Training error decreases as regularization weakens (small $\\alpha$) and increases with strong regularization (large $\\alpha$). Unlike polynomial model complexity jumps, $\\alpha$ offers a smooth control over complexity.\n",
    "\n",
    "### Intuition Behind Regularization\n",
    "\n",
    "Regularization constrains the size of model coefficients so that the model is less sensitive to the noise and variance in the training data. \n",
    "\n",
    "Ridge regression can be seen as minimizing residual sum of squares with an added constraint on the L2 norm of coefficients, geometrically represented by a circular constraint spaceâ€”the coefficients shrink as the constraint tightens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63458b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3077.41593882723\n",
      "Model coefficients: [  45.36737726  -76.66608563  291.33883165  198.99581745   -0.53030959\n",
      "  -28.57704987 -144.51190505  119.26006559  230.22160832  112.14983004]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset (Boston housing dataset as example)\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Ridge regression model with alpha (regularization strength)\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit model on training data\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Access coefficients\n",
    "print(\"Model coefficients:\", ridge_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004998d6",
   "metadata": {},
   "source": [
    "### Choosing Alpha\n",
    "Changing the `alpha` parameter will affect the coefficients and the model's complexity: smaller `alpha` fits training data closer but risks overfitting, larger `alpha` leads to simpler models with less overfit.\n",
    "\n",
    "The optimal $\\alpha$ can be found using cross-validation techniques that balance bias and variance to minimize generalization error. Visual tools like the `AlphaSelection` visualizer in Python's yellowbrick library can help visualize the impact of different $\\alpha$ values on errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac4788",
   "metadata": {},
   "source": [
    "### Summary of Ridge vs LASSO\n",
    "\n",
    "| Aspect               | Ridge (L2)                        | LASSO (L1)                           |\n",
    "|----------------------|---------------------------------|------------------------------------|\n",
    "| Penalty              | Sum of squares of coefficients  | Sum of absolute values of coefficients |\n",
    "| Effect on coefficients| Shrinks but does not zero out   | Can shrink some coefficients exactly to zero |\n",
    "| Feature selection    | No                              | Yes                                |\n",
    "| Use case             | When all features are relevant  | When feature selection and sparsity are desired |\n",
    "| Solution path       | Smooth shrinking                | Sparse solutions, feature elimination |\n",
    "\n",
    "In essence, regularization, through tuning $\\alpha$, controls model complexity and helps in preventing overfitting by shrinking coefficients, with Ridge maintaining all features and LASSO promoting sparsity.\n",
    "\n",
    "This detailed overview covers the principles, intuition, and practical Python usage of regularization techniques, especially Ridge regression, as well as the effect and choice of the alpha parameter in controlling model complexity and preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1d6ae",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[1](https://www.geeksforgeeks.org/machine-learning/regularization-in-machine-learning/)\n",
    "[2](https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning)\n",
    "[3](https://fiveable.me/modern-statistical-prediction-and-machine-learning/unit-7/ridge-regression-l2-regularization/study-guide/8Rdu2Bh1ceyKIQ9x)\n",
    "[4](https://www.reddit.com/r/MachineLearning/comments/1j8gvlh/d_how_does_l1_regularization_perform_feature/)\n",
    "[5](https://blog.truegeometry.com/api/exploreHTML/4e91938acfbdbea3ad87b2bcca8bc40d.exploreHTML)\n",
    "[6](https://machinelearningmastery.com/ridge-regression-with-python/)\n",
    "[7](https://www.scikit-yb.org/en/latest/api/regressor/alphas.html)\n",
    "[8](https://www.ibm.com/think/topics/regularization)\n",
    "[9](https://c3.ai/introduction-what-is-machine-learning/regularization/)\n",
    "[10](https://www.techtarget.com/searchenterpriseai/feature/Machine-learning-regularization-explained-with-examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
