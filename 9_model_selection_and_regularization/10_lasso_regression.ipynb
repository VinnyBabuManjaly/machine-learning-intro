{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd5cf99",
   "metadata": {},
   "source": [
    "## LASSO regression\n",
    "\n",
    "LASSO regression is a type of linear regression that uses L1 regularization, which adds a penalty proportional to the absolute values of the coefficients multiplied by a tuning parameter alpha. \n",
    "\n",
    "This contrasts with ridge regression, which uses L2 regularization (penalty proportional to the squared values of coefficients). \n",
    "\n",
    "The key difference is that LASSO tends to produce sparse models by forcing many coefficients exactly to zero, effectively performing automatic feature selection, while ridge regression only shrinks coefficients but keeps all features in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081e360",
   "metadata": {},
   "source": [
    "### Key Characteristics of LASSO Regression:\n",
    "\n",
    "- Uses **L1 regularization**: sum of absolute values of coefficients times alpha.\n",
    "\n",
    "- Can shrink some coefficients to exactly zero, hence performs feature selection.\n",
    "\n",
    "- Typically slower and can produce convergence warnings due to the harder optimization problem.\n",
    "\n",
    "- Useful when you want a simpler, interpretable model with fewer features.\n",
    "\n",
    "- The zeroing of coefficients emerges organically from the global optimization, not by sequential feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1a201",
   "metadata": {},
   "source": [
    "| Aspect                 | Ridge Regression (L2)                | LASSO Regression (L1)                     |\n",
    "|------------------------|------------------------------------|------------------------------------------|\n",
    "| Regularization Type    | Penalizes sum of squared coefficients | Penalizes sum of absolute coefficients    |\n",
    "| Feature Selection      | No feature selection, all features retained with smaller weights | Performs automatic feature selection by zeroing some coefficients |\n",
    "| Model Complexity       | Includes all features, shrunk coefficients | Simpler model with fewer features         |\n",
    "| Numerical Stability    | More stable and faster              | May encounter convergence warnings       |\n",
    "| Use Case               | When all features are relevant      | When only some features matter            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff2e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 1.4873521072935119\n",
      "Development set MSE: 2802.6510986801695\n",
      "Selected Features and their coefficients:\n",
      "{'age': np.float64(0.05934148216469456), 'sex': np.float64(-8.257945720277121), 'bmi': np.float64(26.20705687774852), 'bp': np.float64(15.179997187720314), 's1': np.float64(-5.173265909034207), 's3': np.float64(-11.174893258381422), 's5': np.float64(22.180295881633334), 's6': np.float64(1.8630807438295154)}\n"
     ]
    }
   ],
   "source": [
    "# Example Python Implementation of LASSO Regression Using GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train/dev sets\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline with scaling and LASSO regression\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso', Lasso(max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define range of alpha values to search\n",
    "alpha_values = np.logspace(-5, 1, 30)\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {'lasso__alpha': alpha_values}\n",
    "\n",
    "# Setup GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(pipe, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Fit on training data (GridSearchCV internally does cross-validation)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best alpha and performance on dev set\n",
    "best_alpha = grid_search.best_estimator_.named_steps['lasso'].alpha\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_dev)\n",
    "dev_mse = mean_squared_error(y_dev, y_pred)\n",
    "print(f\"Development set MSE: {dev_mse}\")\n",
    "\n",
    "# Extract non-zero coefficients\n",
    "coefficients = grid_search.best_estimator_.named_steps['lasso'].coef_\n",
    "non_zero_coefs = {data.feature_names[i]: coef for i, coef in enumerate(coefficients) if coef != 0}\n",
    "print(\"Selected Features and their coefficients:\")\n",
    "print(non_zero_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b1039",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- This pipeline scales features to z-scores before fitting LASSO.\n",
    "\n",
    "- Grid search tests multiple alpha values to find the best regularization strength that minimizes the dev set error.\n",
    "\n",
    "- LASSO's automatic feature selection results in a sparse model with many coefficients set to zero.\n",
    "\n",
    "- This is beneficial when interpretability or feature sparsity is desired, unlike ridge regression which retains all features but shrinks their impact.\n",
    "\n",
    "This matches the conceptual description where LASSO yields a smaller subset of features chosen through global optimization, showing its power for feature selection compared to the shrinkage-only effect of ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e537d8c",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[1](https://www.geeksforgeeks.org/machine-learning/ridge-regression-vs-lasso-regression/)\n",
    "[2](https://www.shiksha.com/online-courses/articles/ridge-regression-vs-lasso-regression/)\n",
    "[3](https://www.r-bloggers.com/2024/01/understanding-lasso-and-ridge-regression-3/)\n",
    "[4](https://www.reddit.com/r/datascience/comments/q1heaz/lasso_vs_ridge_regression/)\n",
    "[5](https://www.datacamp.com/tutorial/tutorial-lasso-ridge-regression)\n",
    "[6](https://www.youtube.com/watch?v=Xm2C_gTAl8c)\n",
    "[7](https://www.statology.org/when-to-use-ridge-lasso-regression/)\n",
    "[8](https://www.reddit.com/r/learndatascience/comments/qn200d/when_to_use_lassoridge_regression/)\n",
    "[9](https://www.tutorialspoint.com/ridge-and-lasso-regression-explained)\n",
    "[10](https://www.sciencedirect.com/science/article/pii/S1877705817341474)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
