{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e44ddf",
   "metadata": {},
   "source": [
    "Gradient descent and stochastic (and mini-batch) gradient descent are all ways to move parameters downhill on a loss surface, but they differ in how much data they use per update, how accurate each update is, and how fast they run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52abb1b5",
   "metadata": {},
   "source": [
    "### Core difference: how much data each step uses\n",
    "\n",
    "- **Gradient Descent (GD, “batch GD”)**  \n",
    "  - Each update uses the **entire dataset** to compute the gradient.  \n",
    "  - The direction is the *true* steepest descent direction for that loss and current parameters.  \n",
    "  - Updates are accurate but expensive when the dataset is huge.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD, batch size = 1)**  \n",
    "  - Each update uses **just one training example** at a time.  \n",
    "  - The gradient is a very noisy approximation of the true gradient.  \n",
    "  - Each update is cheap, and you can do many updates per pass over the data.\n",
    "\n",
    "- **Mini‑batch Gradient Descent (batch size between 1 and full)**  \n",
    "  - Each update uses a **small subset** of the data, e.g. 32 examples.  \n",
    "  - Gradient is less noisy than pure SGD, much cheaper than full GD.  \n",
    "  - This is the most common choice in modern large‑scale training.\n",
    "\n",
    "You can think of **batch size** as a knob:\n",
    "\n",
    "- Batch size = dataset size → classic gradient descent.  \n",
    "- Batch size = 1 → stochastic gradient descent.  \n",
    "- Batch size in between → mini‑batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544763b8",
   "metadata": {},
   "source": [
    "### Quality vs speed: the trade‑off\n",
    "\n",
    "The video emphasizes that batch size controls a trade‑off:\n",
    "\n",
    "- **Batch size = full dataset**  \n",
    "  - Gradient quality: best (exact).  \n",
    "  - Runtime per update: worst (must scan all points).  \n",
    "  - Convergence path: smoothest, “true” steepest path.\n",
    "\n",
    "- **Batch size = 1 (pure SGD)**  \n",
    "  - Gradient quality: worst (maximally noisy).  \n",
    "  - Runtime per update: best (one sample).  \n",
    "  - Convergence path: extremely jagged; steps jump around.\n",
    "\n",
    "- **Intermediate batch size (e.g., 32)**  \n",
    "  - Gradient quality: decent; noise is reduced by averaging over several samples.  \n",
    "  - Runtime per update: manageable, especially on GPUs/CPUs that like vectorization.  \n",
    "  - Convergence path: noisy but not chaotic; a good practical balance.\n",
    "\n",
    "**32** is a commonly used batch size in practice, for reasons like hardware, statistical properties, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bcce34",
   "metadata": {},
   "source": [
    "### Intuition: “seeing the whole board” vs “seeing a few squares”\n",
    "\n",
    "- **Batch gradient descent** is like playing chess while seeing the **entire board**.  \n",
    "  - Every move is carefully chosen using the full situation.  \n",
    "  - But each move is slow and computationally expensive.\n",
    "\n",
    "- **Stochastic gradient descent** is like trying to win chess while seeing only **a few squares** at a time.  \n",
    "  - Every move is based on incomplete and noisy information.  \n",
    "  - Individual moves might look suboptimal if you knew the full board.  \n",
    "  - Yet, if you repeat this process many times and eventually see all parts of the board, you can still reach a strong position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22156ad",
   "metadata": {},
   "source": [
    "### Why SGD works (at a high level)\n",
    "\n",
    "- When you loop over the dataset many times (multiple epochs), each data point is used repeatedly in different steps.  \n",
    "- On **average**, these many noisy updates behave similarly to what you would get from using the full gradient every time.  \n",
    "- For convex losses like mean squared error, repeating this process can bring you **very close** to the same minimum as full gradient descent.\n",
    "\n",
    "So: each single step is “blind” and noisy, but across many passes, the noise partially cancels and the direction trends toward the true downhill direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e0a67",
   "metadata": {},
   "source": [
    "### Visual intuition from 2D loss surfaces\n",
    "\n",
    "**Gradient Descent (full batch):**\n",
    "\n",
    "- Starting at the origin, it follows a **smooth, curving path** directly downhill toward the minimum.  \n",
    "- Every step is aligned with the true steepest descent direction because it uses all data.  \n",
    "- The path looks like a clean trajectory through a bowl.\n",
    "\n",
    "**Stochastic / small-batch Gradient Descent:**\n",
    "\n",
    "- Same surface, but each step uses only a subset of data, so each gradient estimate is a **crude approximation**.  \n",
    "- The path:  \n",
    "  - First jump might not align with the true steepest direction, but still reduces loss.  \n",
    "  - Subsequent jumps wander around in a **somewhat random zig‑zag**.  \n",
    "  - Over time, the path winds toward the region of the minimum, but with lots of small detours.\n",
    "\n",
    "For **convex** loss functions (like mean squared error), both methods eventually end up **very near** the true minimum, although SGD does so in a noisy fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94c124",
   "metadata": {},
   "source": [
    "### Practical usage in libraries\n",
    "\n",
    "- In many classic scikit‑learn models, optimization is done using **full‑dataset gradient-based methods** (or equivalent exact solvers).  \n",
    "  - Dataset sizes in those examples are small enough that this is practical.  \n",
    "  - Convergence is smooth and robust, and you don’t have to worry about batch sizes.\n",
    "\n",
    "- In **large‑scale environments** (deep learning, huge datasets):  \n",
    "  - It is far more common to use **mini‑batch gradient descent / SGD**, because:  \n",
    "    - Computing full gradients is too slow and memory hungry.  \n",
    "    - Mini‑batches allow fast, frequent updates.  \n",
    "    - GPUs and accelerators are well-suited to mini‑batch operations.\n",
    "\n",
    "So, in real-world machine learning (especially deep learning), “gradient descent” almost always means **mini‑batch SGD variants**, not pure full‑batch GD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2dd335",
   "metadata": {},
   "source": [
    "### Practical takeaway for you\n",
    "\n",
    "- **Conceptual understanding:**  \n",
    "  - Full GD: accurate but expensive; smooth path; uses all data each step.  \n",
    "  - SGD / mini‑batch: noisy but cheap; jagged path; uses subsets; scales to huge data.\n",
    "\n",
    "- **Batch size as a knob:**  \n",
    "  - Smaller batch → more noise, faster each step.  \n",
    "  - Larger batch → less noise, slower each step.\n",
    "\n",
    "- **In practice:**  \n",
    "  - Rarely these algorithms are implemented by hand outside of learning exercises.  \n",
    "  - Libraries (scikit‑learn, PyTorch, TensorFlow, etc.) implement them under the hood.  \n",
    "  - Still, understanding these ideas helps interpret model behavior, choose hyperparameters, and debug training issues as a serious practitioner."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
