{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443a6523",
   "metadata": {},
   "source": [
    "Gradient descent and the normal equation are two different ways to choose the linear regression parameters $w_0, w_1$, but both aim to minimize the same loss (typically mean squared error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341ec41",
   "metadata": {},
   "source": [
    "### Role of gradient descent in linear regression\n",
    "\n",
    "- In simple linear regression with one feature, the hypothesis is often written as $ \\hat{y} = w_0 + w_1 x $, and the goal is to choose $w_0, w_1$ to minimize the average squared difference between predictions and true values (the cost).  \n",
    "\n",
    "- Gradient descent does this by starting from an initial guess for the weights and iteratively updating them in the negative gradient direction of the cost, so the loss decreases step by step until it converges to (or near) a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd3658",
   "metadata": {},
   "source": [
    "### Role of the normal equation\n",
    "\n",
    "- The normal equation provides a closed‑form solution for the weights that minimize mean squared error, typically written in matrix form as $ w = (X^\\top X)^{-1} X^\\top y $ when the inverse exists.  \n",
    "\n",
    "- Unlike gradient descent, the normal equation does not require choosing a learning rate or running an iterative loop; instead, it directly computes the optimal parameters in a single algebraic step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f99b0",
   "metadata": {},
   "source": [
    "### When to use which\n",
    "\n",
    "- Gradient descent scales better to large datasets and high‑dimensional problems because it avoids forming and inverting $X^\\top X$, and it can be applied to models and loss functions where no closed‑form solution exists.  \n",
    "\n",
    "- The normal equation is convenient for smaller problems with relatively few features, where computing $(X^\\top X)^{-1}$ is feasible and provides an exact minimizer without tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b447760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
