{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41345960",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, intuitive supervised learning algorithm that classifies new data points by finding the *k* most similar (nearest) examples from the training data and using majority voting among their labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c1dd4",
   "metadata": {},
   "source": [
    "#### How KNN Works (Step by Step)\n",
    "\n",
    "KNN follows a straightforward \"lazy learning\" process—no complex model training, just storing the data and computing on the fly during prediction.\n",
    "\n",
    "##### 1. Training Phase (Store Data)\n",
    "- Simply save all labeled training examples (features + class labels).\n",
    "- No fitting or parameter learning happens here—KNN is \"lazy.\"\n",
    "\n",
    "##### 2. Prediction Phase (For a New Point)\n",
    "- **Calculate distances**: Measure distance from the new point to *every* training point (Euclidean distance is common: straight-line distance in feature space).\n",
    "- **Find k nearest**: Sort distances and pick the *k* smallest (your k neighbors).\n",
    "- **Vote or average**:\n",
    "  - **Classification**: Majority class among the k neighbors wins (e.g., if 3/5 say \"spam,\" predict \"spam\").\n",
    "  - **Regression**: Average the values of the k neighbors.\n",
    "\n",
    "**Analogy**: Imagine classifying a fruit by size and color. Compare it to known fruits in a basket, grab the 3 closest ones, and go with whatever type most of them are (e.g., 2 apples → predict apple)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e754c46",
   "metadata": {},
   "source": [
    "#### Key Parameter: Choosing k\n",
    "\n",
    "k controls the \"neighborhood size\" and directly impacts model behavior:\n",
    "\n",
    "- **Small k** (e.g., k=1): Fits training data too closely → overfitting (sensitive to noise).\n",
    "- **Large k**: Smooths too much → underfitting (ignores local patterns, high computation).\n",
    "- **Best practice**: Test odd values (avoids ties in binary classification), use cross-validation or elbow plots to find optimal k (often 3-10).\n",
    "\n",
    "No universal \"best\" k—it depends on your data. Start small, tune via validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd67c1e",
   "metadata": {},
   "source": [
    "#### Strengths and Limitations\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|---------|\n",
    "| **Pros** | Simple to understand/implement; no assumptions about data distribution; works for classification *and* regression; handles multi-class naturally. |\n",
    "| **Cons** | Slow on large datasets (computes all distances); sensitive to feature scaling (scale features first!); struggles in high dimensions (\"curse of dimensionality\"); stores all data (memory-heavy). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d492f99",
   "metadata": {},
   "source": [
    "#### Practical Tips (Beginner-Friendly)\n",
    "- **Scale features**: Use StandardScaler—distances matter, unscaled features dominate.\n",
    "- **Distance metric**: Euclidean for most cases; try Manhattan or Minkowski for others.\n",
    "- **In scikit-learn** (Python):\n",
    "  ```python\n",
    "  from sklearn.neighbors import KNeighborsClassifier\n",
    "  knn = KNeighborsClassifier(n_neighbors=5)  # k=5\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  ```\n",
    "- Real-world: Great for small/medium datasets, prototyping, or when interpretability matters (e.g., \"these 5 similar patients had this outcome\").\n",
    "\n",
    "This matches your description: KNN identifies similarities via proximity, votes for the dominant class, and tunes k to balance accuracy vs complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe594902",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[1](https://www.pinecone.io/learn/k-nearest-neighbor/)\n",
    "[2](https://www.geeksforgeeks.org/machine-learning/k-nearest-neighbours/)\n",
    "[3](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm)\n",
    "[4](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "[5](https://www.youtube.com/watch?v=v5CcxPiYSlA)\n",
    "[6](https://www.youtube.com/watch?v=zeFt_JCA3b4)\n",
    "[7](https://www.youtube.com/watch?v=b6uHw7QW_n4)\n",
    "[8](https://www.elastic.co/what-is/knn)\n",
    "[9](https://www.youtube.com/watch?v=HVXime0nQeI)\n",
    "[10](https://www.ibm.com/think/topics/knn)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
