{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35e68f5",
   "metadata": {},
   "source": [
    "## Evaluating Classifiers: From Confusion Matrix to F1 Score\n",
    "\n",
    "Classification models are evaluated using metrics derived from the **confusion matrix**, which counts True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).[1][2][3]\n",
    "\n",
    "```\n",
    "Confusion Matrix\n",
    "           Predicted\n",
    "           0                   1\n",
    "Actual  0  TN                  FP(Type I error)\n",
    "        1  FN(Type II error)   TP    \n",
    "```\n",
    "\n",
    "**TP**: Correctly predicted positive  \n",
    "**TN**: Correctly predicted negative  \n",
    "**FP**: Wrongly predicted positive (Type I error)  \n",
    "**FN**: Wrongly predicted negative (Type II error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5b19b",
   "metadata": {},
   "source": [
    "### Core Metrics Explained (With Real Examples)\n",
    "\n",
    "#### Accuracy: \"Overall Correctness\"\n",
    "**Formula**: `(TP + TN) / (TP + TN + FP + FN)`  \n",
    "**Question**: \"What fraction of all predictions were correct?\"\n",
    "\n",
    "**Example**: 9/10 predictions correct → **90% accuracy**  \n",
    "**When to use**: Balanced classes (equal positives/negatives)  \n",
    "**Problem**: Fails with imbalanced data (e.g., 99% healthy patients → 99% accuracy by predicting \"healthy\" always)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3034c5",
   "metadata": {},
   "source": [
    "#### Precision: \"How Trustworthy Are Positives?\"\n",
    "**Formula**: `TP / (TP + FP)`  \n",
    "**Question**: \"Of everything predicted positive, how much was actually positive?\"\n",
    "\n",
    "**Example**: 8/10 cancer predictions correct → **80% precision** (2 healthy flagged as cancer)  \n",
    "**When to use**: False positives costly (spam filter, fraud detection, loan approval)  \n",
    "**Real scenario**: Credit risk—high precision avoids risky loans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846aded",
   "metadata": {},
   "source": [
    "#### Recall (Sensitivity): \"Did We Catch All Positives?\"\n",
    "**Formula**: `TP / (TP + FN)`  \n",
    "**Question**: \"Of all actual positives, how many did we catch?\"\n",
    "\n",
    "**Example**: 7/10 COVID patients detected → **70% recall** (3 missed)  \n",
    "**When to use**: False negatives deadly (disease detection, quality control)  \n",
    "**Real scenario**: Medical diagnosis—high recall catches sick patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e5233",
   "metadata": {},
   "source": [
    "#### Specificity: \"True Negative Rate/Did We Catch All Negatives?\"\n",
    "**Formula**: `TN / (TN + FP)`  \n",
    "**Question**: \"Of all actual negatives, how many were correctly identified?\"\n",
    "\n",
    "**Example**: 85/100 healthy patients correctly identified → **Specificity=0.85**  \n",
    "**When to use**: Costly false positives, balanced negative class focus  \n",
    "**Perfect for**: Medical screening, spam filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dc235",
   "metadata": {},
   "source": [
    "#### F1 Score: \"Balance of Precision + Recall\"\n",
    "**Formula**: `2 × (precision × recall) / (precision + recall)`  \n",
    "**Question**: \"Single number balancing both metrics?\"\n",
    "\n",
    "**Example**: Precision=0.8, Recall=0.7 → **F1=0.74**  \n",
    "**When to use**: Imbalanced classes, need holistic view  \n",
    "**Perfect for**: Fraud detection, rare disease diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf939a",
   "metadata": {},
   "source": [
    "### When to Use Each Metric (Quick Guide)\n",
    "\n",
    "| Metric | Best For | Example Scenario | Watch Out For |\n",
    "|--------|----------|------------------|---------------|\n",
    "| **Accuracy** | Balanced classes | Equal spam/ham emails | Imbalanced data |\n",
    "| **Precision** | Costly false positives | Fraud alerts, spam filter | Misses true positives |\n",
    "| **Recall** | Costly false negatives | Cancer screening | Too many false positives |\n",
    "| **F1** | Imbalanced + need balance | Rare disease detection | None—most versatile |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28d1f9",
   "metadata": {},
   "source": [
    "### Precision vs Recall Trade-off (Business Decisions)\n",
    "\n",
    "**Key insight**: You can't maximize both—raising precision often lowers recall (and vice versa).\n",
    "\n",
    "| Scenario | Prioritize Precision | Prioritize Recall |\n",
    "|----------|---------------------|-------------------|\n",
    "| **Medical Diagnosis** | Avoid unnecessary treatments | Catch all sick patients |\n",
    "| **Fraud Detection** | Avoid bothering honest customers | Catch all fraudsters |\n",
    "| **Quality Control** | Avoid scrapping good products | Catch all defects |\n",
    "| **Spam Filter** | Don't block important emails | Block all spam |\n",
    "\n",
    "**Balance both** → Use F1 score or tune decision threshold via ROC curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfac7f6",
   "metadata": {},
   "source": [
    "### Python Quick Check (scikit-learn)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# After predictions\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "**Sample output**:\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "       Paid       0.85      0.92      0.88        50\n",
    "Did not pay       0.90      0.82      0.86        50\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f117f",
   "metadata": {},
   "source": [
    "### ROC Curve (Bonus Visual Metric)\n",
    "\n",
    "**ROC AUC**: Single number (0-1) measuring \"how well model separates classes\" across all thresholds.  \n",
    "- AUC=1.0: Perfect separator  \n",
    "- AUC=0.5: Random guessing  \n",
    "**Use when**: Comparing models or tuning thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05023dce",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[1](https://www.geeksforgeeks.org/machine-learning/sklearn-classification-metrics/)\n",
    "[2](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall)\n",
    "[3](https://www.geeksforgeeks.org/machine-learning/how-to-produce-a-confusion-matrix-and-find-the-misclassification-rate-of-the-naive-bayes-classifier-in-r/)\n",
    "[4](https://www.statology.org/misclassification-rate/)\n",
    "[5](https://www.geeksforgeeks.org/machine-learning/metrics-for-machine-learning-model/)\n",
    "[6](https://www.geeksforgeeks.org/machine-learning/evaluation-metrics-for-classification-model-in-python/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
