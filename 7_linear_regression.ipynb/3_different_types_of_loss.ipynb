{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e562b666",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "Loss functions are essential tools to measure how well a regression model predicts outcomes. Each loss function quantifies the error, with different emphases and mathematical characteristics, helping guide model development for various data distributions and objectives. Here’s an in-depth, beginner-friendly explanation of each major loss function for regression, including formulas and practical considerations.\n",
    "\n",
    "- Loss is a numerical metric that tells us how far off our predictions are from the actual observed data.​\n",
    "\n",
    "- Loss functions are minimized during model training to improve prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253779ac",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "- **Definition**: MSE calculates the average squared difference between predicted ($\\hat{y}_i$) and actual ($y_i$) values.\n",
    "- **Formula**: \n",
    "  $$\n",
    "  \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "- **Advantages**: Efficient convergence for small errors, widely used, sensitive to larger mistakes, smooth for optimization.\n",
    "- **Drawbacks**: Extremely sensitive to outliers – large errors have disproportionate influence, which can destabilize optimization and bias the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714f49a",
   "metadata": {},
   "source": [
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "- **Definition**: RMSE is the square root of MSE and shares its properties, but critically, it is expressed in the same units as the target variable.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "  $$\n",
    "- **Advantages**: Easy to interpret, penalizes larger errors, useful for applications sensitive to large deviations, directly measures average magnitude of errors.\n",
    "- **Drawbacks**: Sensitive to outliers, less robust on non-normal error distributions, scale-variant—hard to compare across datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4b94a",
   "metadata": {},
   "source": [
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "- **Definition**: MAE computes the average absolute difference between predicted and actual values.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "  $$\n",
    "- **Advantages**: Simple, computationally efficient, less sensitive to outliers than MSE.\n",
    "- **Drawbacks**: All errors are weighted equally, not differentiable at zero, which can affect optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ed2df",
   "metadata": {},
   "source": [
    "\n",
    "### Huber Loss (Smooth MAE)\n",
    "\n",
    "- **Definition**: Huber loss blends MSE (for small errors, for smooth optimization) and MAE (for large errors, which reduces sensitivity to outliers).\n",
    "- **Formula** (with threshold $\\delta$):\n",
    "  $$\n",
    "  \\begin{cases}\n",
    "      \\frac{1}{2}(y_i - \\hat{y}_i)^2 & \\text{for } |y_i - \\hat{y}_i| \\leq \\delta \\\\\n",
    "      \\delta|y_i - \\hat{y}_i| - \\frac{1}{2}\\delta^2 & \\text{for } |y_i - \\hat{y}_i| > \\delta\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- **Advantages**: Robust to outliers, continuously differentiable, stable for optimization.\n",
    "- **Drawbacks**: Requires tuning the threshold parameter $\\delta$ for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd8dc4",
   "metadata": {},
   "source": [
    "\n",
    "### Mean Squared Logarithmic Error (MSLE)\n",
    "\n",
    "- **Definition**: MSLE measures the ratio between true and predicted values and is useful when penalizing large differences less aggressively, particularly when values are on vastly different scales.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\text{MSLE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\log(y_i + 1) - \\log(\\hat{y}_i + 1)\\right)^2\n",
    "  $$\n",
    "- **Advantages**: Treats small and large value differences comparably, less severe penalties for huge differences.\n",
    "- **Drawbacks**: Penalizes underprediction more than overprediction, can be biased in some regression cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bee1b0",
   "metadata": {},
   "source": [
    "\n",
    "### Mean Bias Error (MBE)\n",
    "\n",
    "- **Definition**: MBE calculates the average signed error between prediction and observation (not absolute), highlighting model bias.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\text{MBE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)\n",
    "  $$\n",
    "- **Advantages**: Useful for identifying whether the model generally overestimates or underestimates.\n",
    "- **Drawbacks**: Positive and negative errors cancel out, possibly hiding large individual mistakes. Not robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d353fff",
   "metadata": {},
   "source": [
    "\n",
    "### Log-Cosh Loss\n",
    "\n",
    "- **Definition**: This function uses $\\log(\\cosh(x))$ to achieve a blend of smoothness (like MSE) and outlier robustness (like MAE).\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\text{Log-cosh} = \\frac{1}{n}\\sum_{i=1}^n \\log(\\cosh(y_i - \\hat{y}_i))\n",
    "  $$\n",
    "- **Advantages**: Smooth gradient, symmetric, robust to outliers, good generalization.\n",
    "- **Drawbacks**: Computationally more complex than MSE or MAE, lacks tunability for outlier sensitivity, sometimes hard to interpret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828222e3",
   "metadata": {},
   "source": [
    "\n",
    "### Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "- **Definition**: Expresses prediction error as a percentage; good for reporting error relative to actual value magnitude.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\text{MAPE} = \\frac{100\\%}{n}\\sum_{i=1}^n \\frac{|y_i - \\hat{y}_i|}{|y_i|}\n",
    "  $$\n",
    "- **Advantages**: Scale-free, interpretable as percent error.\n",
    "- **Drawbacks**: Can be unstable when true values are near zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f9653",
   "metadata": {},
   "source": [
    "\n",
    "### Comparison Table: Regression Loss Functions\n",
    "\n",
    "| Loss Function      | Formula (LaTeX)                          | Key Traits/When To Use                                                                      | Drawbacks                        |\n",
    "|--------------------|------------------------------------------|---------------------------------------------------------------------------------------------|-----------------------------------|\n",
    "| MSE                | $$\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2$$         | Standard for regression, penalizes large errors, smooth optimization                        | Sensitive to outliers            |\n",
    "| RMSE               | $$\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}$$  | Interpretable units, penalizes large errors, preferred for applications needing magnitude    | Sensitive to outliers            |\n",
    "| MAE                | $$\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{y}_i|$$         | Simple, robust to outliers, easy to compute                                                 | Not differentiable at zero       |\n",
    "| Huber Loss         | See above (piecewise)                     | Robust, combines smoothness and outlier resistance, stable optimization                     | Requires threshold tuning        |\n",
    "| MSLE               | $$\\frac{1}{n}\\sum_{i=1}^n (\\log(y_i+1)-\\log(\\hat{y}_i+1))^2$$ | Use when ratios matter, robust to huge differences                                          | Underprediction penalty          |\n",
    "| MBE                | $$\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)$$           | Model bias measurement, check under/overestimation bias                                     | Errors cancel; can mislead       |\n",
    "| Log-Cosh           | $$\\frac{1}{n}\\sum_{i=1}^n \\log(\\cosh(y_i-\\hat{y}_i))$$| Smooth, robust, symmetric, good for generalization                                          | Hard to interpret, slow compute  |\n",
    "| MAPE               | $$\\frac{100\\%}{n}\\sum_{i=1}^n \\frac{|y_i-\\hat{y}_i|}{|y_i|}$$ | Scale independent, percent error reporting                                                  | Unstable near zero true values   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e3a22",
   "metadata": {},
   "source": [
    "### Practical Notes\n",
    "\n",
    "- **Choosing your loss function depends on your data and the problem context.** Use MSE or RMSE when large errors are unacceptable; MAE or Huber when outliers are present; MSLE for ratio-based regression; and MBE to diagnose bias.\n",
    "- Loss functions guide a model to learn and improve by minimizing error during training.\n",
    "- Understanding their nuances and selecting appropriately improves model reliability for all regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07faa26",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "[1](https://arize.com/blog-course/root-mean-square-error-rmse-what-you-need-to-know/)\n",
    "[2](https://www.geeksforgeeks.org/deep-learning/loss-functions-in-deep-learning/)\n",
    "[3](https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/)\n",
    "[4](https://www.datacamp.com/tutorial/loss-function-in-machine-learning)\n",
    "[5](https://www.datacamp.com/tutorial/rmse)\n",
    "[6](https://www.datarobot.com/blog/introduction-to-loss-functions/)\n",
    "[7](https://statisticsbyjim.com/regression/root-mean-square-error-rmse/)\n",
    "[8](https://developers.google.com/machine-learning/crash-course/linear-regression/loss)\n",
    "[9](https://en.wikipedia.org/wiki/Root_mean_square_deviation)\n",
    "[10](https://www.youtube.com/watch?v=RNUaQciYPzs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
