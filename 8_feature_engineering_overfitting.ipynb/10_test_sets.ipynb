{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa94d0e7",
   "metadata": {},
   "source": [
    "Below is a complete, beginner-friendly explanation of the *Rules of Training, Validation, and Test Sets* with a clear breakdown of the transcript content, expanded theory, real-world context, and Python implementations using `scikit-learn`.\n",
    "\n",
    "***\n",
    "\n",
    "## Understanding Data Splits in Machine Learning\n",
    "\n",
    "### What Are Training, Validation, and Test Sets?\n",
    "\n",
    "In machine learning, we train models on existing data to make predictions about unseen data. The entire dataset is divided into subsets to serve different roles during this process:\n",
    "\n",
    "- **Training Set** – Data used by the model to learn patterns (features → target relationships).  \n",
    "- **Validation Set** – Data used to fine-tune model choices (model type, hyperparameters) and avoid overfitting.  \n",
    "- **Test Set** – Data used at the *end* to measure how well the final model generalizes to unseen data.\n",
    "\n",
    "Typical splits:\n",
    "- 60% train / 20% validation / 20% test  \n",
    "- 70% train / 15% validation / 15% test  \n",
    "- 80% train / 10% validation / 10% test  \n",
    "\n",
    "The choice depends on dataset size, model complexity, and business constraints.\n",
    "\n",
    "***\n",
    "\n",
    "## Why Split Data?\n",
    "\n",
    "If you use all data to train, the model may *memorize* rather than *learn general patterns*.  \n",
    "If you use all data to test, the model never learns.\n",
    "\n",
    "The idea is to simulate real-world deployment where the model must handle *unseen data* reliably.\n",
    "\n",
    "***\n",
    "\n",
    "## Basic Workflow of Model Training\n",
    "\n",
    "1. Divide data into **training**, **validation**, and **test** subsets.  \n",
    "2. Choose a training algorithm (e.g., linear regression, random forest).  \n",
    "3. Train the model using the training set.  \n",
    "4. Evaluate on the validation set to tune hyperparameters or select the best model.  \n",
    "5. Retrain the final model on both training and validation data.  \n",
    "6. Evaluate on the test set once to get an *unbiased performance estimate*.\n",
    "\n",
    "***\n",
    "\n",
    "## From the Transcript: Diamond Price Prediction Example\n",
    "\n",
    "Imagine you want to predict **diamond sale price** based on features like size, shape, and opacity.\n",
    "\n",
    "Two candidate models:\n",
    "- **Linear Regression**\n",
    "- **Deep Learning (Neural Network)**\n",
    "\n",
    "After training:\n",
    "- The Linear Regression model gives a smaller *training error* (mean squared error, MSE).\n",
    "- The Deep Learning model gives a smaller *validation error*.\n",
    "\n",
    "Which is better to deploy?\n",
    "\n",
    "**Answer:** The Deep Learning model, since the validation error (performance on unseen data) is a more reliable gauge of generalization than training error.\n",
    "\n",
    "***\n",
    "\n",
    "## Intuition: Bias in Model Selection\n",
    "\n",
    "When you test many models and pick the best based on validation error, that choice is *slightly biased* — the reported performance is somewhat optimistic.  \n",
    "To correct this bias, we use a **third dataset (test set)** for a final, unbiased evaluation.\n",
    "\n",
    "Analogy from the transcript:\n",
    "> The test set is like having the golfer (model) take a new batch of swings after the competition to measure their true skill, not the luckiest hit from practice.\n",
    "\n",
    "***\n",
    "\n",
    "## Implementing Data Splitting in Python\n",
    "\n",
    "Here’s how to split your dataset (for instance, 2000 diamond samples):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example: diamond dataset\n",
    "# df has columns: ['size', 'shape', 'opacity', 'price']\n",
    "# For simplicity, we’ll just simulate dummy data.\n",
    "\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'size': np.random.rand(2000) * 10,\n",
    "    'shape': np.random.randint(1, 5, 2000),\n",
    "    'opacity': np.random.rand(2000),\n",
    "    'price': np.random.rand(2000) * 5000\n",
    "})\n",
    "\n",
    "# Step 1 — Split into TRAIN + TEMP (temp will be split later)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Step 2 — Split TEMP into VALIDATION and TEST\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))  # ~1500 train, 250 val, 250 test\n",
    "\n",
    "# Step 3 — Train a simple model\n",
    "features = ['size', 'shape', 'opacity']\n",
    "target = 'price'\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_val, y_val = val_df[features], val_df[target]\n",
    "X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4 — Validation performance\n",
    "val_predictions = model.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "\n",
    "# Step 5 — Retrain (if needed) using train+val\n",
    "X_final = pd.concat([X_train, X_val])\n",
    "y_final = pd.concat([y_train, y_val])\n",
    "model.fit(X_final, y_final)\n",
    "\n",
    "# Step 6 — Test performance\n",
    "test_predictions = model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "print(\"Validation MSE:\", val_mse)\n",
    "print(\"Final Test MSE:\", test_mse)\n",
    "```\n",
    "\n",
    "**Explanation of this code:**\n",
    "- `train_test_split` randomly divides the dataset.\n",
    "- The `train` set (roughly 1500 rows) is used to fit the model.\n",
    "- The `val` set (roughly 300 rows) is used to compare model hyperparameters.\n",
    "- The `test` set (roughly 200 rows) is used only once at the very end.\n",
    "\n",
    "***\n",
    "\n",
    "## Practical Guidelines and Key Rules\n",
    "\n",
    "- Never use the test set while developing the model or tuning hyperparameters.\n",
    "- Always shuffle before splitting to ensure randomness.\n",
    "- Validation and test sets should come from the same **underlying distribution**.\n",
    "- The test MSE (mean squared error) should be reported once — it represents how your model would perform on *new, unseen data*.\n",
    "\n",
    "***\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "- **Data leakage:** accidentally mixing training and test samples (e.g., normalizing all data before splitting).  \n",
    "- **Overfitting validation data:** trying too many combinations of hyperparameters on the same validation set.  \n",
    "- **Reusing the test set:** once used for model selection, it ceases to be a test set.  \n",
    "\n",
    "If you need repeated validation, use **cross-validation** instead of touching the test data.\n",
    "\n",
    "***\n",
    "\n",
    "## Key Takeaways from the Transcript\n",
    "\n",
    "- Training set teaches the model.\n",
    "- Validation (development) set tunes and selects models.\n",
    "- Test set fairly measures how the chosen model performs in the real world.\n",
    "- Always evaluate the test set *only once* to ensure honest assessment.\n",
    "- A smaller validation loss than training loss often indicates regularization or model variability.\n",
    "- Both validation and test error curves should behave similarly if data distributions match.\n",
    "\n",
    "***\n",
    "\n",
    "Would you like me to extend this into a visual illustration (e.g., Python plots showing overfitting vs. generalization curves)?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
