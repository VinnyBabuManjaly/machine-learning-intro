{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ad4e91",
   "metadata": {},
   "source": [
    "A **kernel** lets you solve nonlinear problems using methods that were originally designed for linear models, by working with a similarity function between points instead of explicit coordinates in a feature space. \n",
    "\n",
    "The idea is developed first for regularized linear regression, then generalized toward the “kernel trick” used in SVMs and other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ec2f6",
   "metadata": {},
   "source": [
    "### 1. Intuitive picture: why kernels?\n",
    "\n",
    "#### 1.1 Linear vs nonlinear separation\n",
    "\n",
    "- A linear model (like plain linear regression or logistic regression) uses a prediction of the form  \n",
    "  $\\hat{y}(x) = \\beta^\\top \\phi(x)$,  \n",
    "  where $\\phi(x)$ is a vector of features (maybe just the original inputs, maybe some nonlinear transforms).\n",
    "- In a 2D plot, such a model produces **straight** decision boundaries (lines).  \n",
    "- Many real datasets are **not** linearly separable in the original space: you might need a curve or complex boundary.\n",
    "\n",
    "A common trick is to **map the data into a higher-dimensional feature space** using nonlinear features—e.g., quadratics, cubics, trigonometric functions—where a straight boundary becomes possible.\n",
    "\n",
    "Example (conceptual):\n",
    "\n",
    "- In 2D, points of one class form a ring around the origin; the other class is in the center.\n",
    "- No straight line can separate them in 2D.\n",
    "- If you add the feature $r^2 = x_1^2 + x_2^2$, in the new space $(x_1, x_2, r^2)$ you can separate them using a hyperplane.\n",
    "\n",
    "Kernels formalize and generalize this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a7743",
   "metadata": {},
   "source": [
    "#### 1.2 What is a kernel?\n",
    "\n",
    "A **kernel** is a function $k(x, x')$ that takes two data vectors and returns a single number measuring their **similarity** in some (possibly high-dimensional) feature space.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- You can think of a kernel as computing  \n",
    "  $k(x, x') = \\phi(x)^\\top \\phi(x')$  \n",
    "  for some feature map $\\phi$, even if you never explicitly compute $\\phi(x)$.\n",
    "- Different kernels correspond to different choices of $\\phi$ and, therefore, different kinds of nonlinear structure the model can capture.\n",
    "\n",
    "Common kernels (for SVM, etc.):\n",
    "\n",
    "- Linear: $k(x, x') = x^\\top x'$ (no extra features)\n",
    "- Polynomial: $k(x, x') = (\\gamma x^\\top x' + r)^d$\n",
    "- RBF/Gaussian: $k(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$\n",
    "\n",
    "The **kernel trick**: if an algorithm can be written purely in terms of inner products $x_i^\\top x_j$, you can replace those inner products with a kernel $k(x_i, x_j)$. That implicitly moves you to a high-dimensional feature space without ever computing coordinates there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7e588",
   "metadata": {},
   "source": [
    "### 2. Linear regression with nonlinear features (primal view)\n",
    "\n",
    "Start with **regularized linear regression** with nonlinear features, because the math is clean and the structure generalizes to other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493b1f1",
   "metadata": {},
   "source": [
    "#### 2.1 Model and notation\n",
    "\n",
    "We have data:\n",
    "\n",
    "- Inputs: $x_1, \\dots, x_N$\n",
    "- Outputs: $y_1, \\dots, y_N$\n",
    "\n",
    "We define a feature map $\\phi(x)$ (vector of $M$ features):\n",
    "\n",
    "- The 0th feature is always $1$ (bias).\n",
    "- The remaining $M - 1$ features are nonlinear transforms of $x$ (e.g., polynomials, etc.).\n",
    "\n",
    "We collect:\n",
    "\n",
    "- Parameter vector $\\beta \\in \\mathbb{R}^M$: $\\beta = (\\beta_0, \\dots, \\beta_{M-1})^\\top$\n",
    "- Feature vector for sample $i$: $\\phi(x_i) \\in \\mathbb{R}^M$\n",
    "\n",
    "The model is:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_i) = \\beta^\\top \\phi(x_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99266dc",
   "metadata": {},
   "source": [
    "#### 2.2 Regularized least squares loss\n",
    "\n",
    "We want to find $\\beta$ that minimizes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta)\n",
    "= \\sum_{i=1}^N (\\beta^\\top \\phi(x_i) - y_i)^2\n",
    "+ \\frac{\\lambda}{2} \\|\\beta\\|^2,\n",
    "$$\n",
    "\n",
    "where $\\lambda > 0$ is a regularization strength:\n",
    "\n",
    "- First term: sum of squared errors.\n",
    "- Second term: L2 penalty (ridge) on coefficients, discouraging very large $\\beta$.\n",
    "\n",
    "The factor $\\frac{\\lambda}{2}$ is just for algebraic convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fb5b7",
   "metadata": {},
   "source": [
    "#### 2.3 Matrix form (primal / β-formulation)\n",
    "\n",
    "Define:\n",
    "\n",
    "- Feature matrix $\\Phi \\in \\mathbb{R}^{N \\times M}$: each row is $\\phi(x_i)^\\top$.\n",
    "- Target vector $Y \\in \\mathbb{R}^N$: entries $y_i$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta)\n",
    "= \\|\\Phi \\beta - Y\\|^2 + \\frac{\\lambda}{2} \\|\\beta\\|^2.\n",
    "$$\n",
    "\n",
    "This is a convex quadratic in $\\beta$. The minimizer is found by setting the derivative to zero.\n",
    "\n",
    "Compute gradient and set to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\beta}\n",
    "= 2\\Phi^\\top (\\Phi\\beta - Y) + \\lambda \\beta = 0.\n",
    "$$\n",
    "\n",
    "Rearrange:\n",
    "\n",
    "$$\n",
    "(\\Phi^\\top \\Phi + \\lambda I)\\beta = \\Phi^\\top Y.\n",
    "$$\n",
    "\n",
    "So the **closed-form solution** (primal solution) is:\n",
    "\n",
    "$$\n",
    "\\beta^\\ = (\\Phi^\\top \\Phi + \\lambda I)^{-1} \\Phi^\\top Y.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $\\Phi^\\top \\Phi$ is an $M \\times M$ matrix.\n",
    "- You invert an $M \\times M$ matrix: complexity ~ $O(M^3)$.\n",
    "\n",
    "Prediction for a new point $x_{\\text{new}}$:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_{\\text{new}}) = \\beta^{*\\top} \\phi(x_{\\text{new}}).\n",
    "$$\n",
    "\n",
    "Notice:\n",
    "\n",
    "- After computing $\\beta^{*\\top}$, you can forget the training data and just keep $\\beta^{*\\top}$.\n",
    "- This is the **primal**, or **β-based**, viewpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18885d",
   "metadata": {},
   "source": [
    "### 3. Alternative formulation: α-parameters (dual-like view)\n",
    "\n",
    "An alternative set of parameters $\\alpha_i$, one per data point, to expose where kernels will appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7416cc",
   "metadata": {},
   "source": [
    "#### 3.1 Define α in terms of errors\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "\\alpha_i = -\\frac{\\beta^\\top \\phi(x_i) - y_i}{\\lambda}.\n",
    "$$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{1}{\\lambda} (y_i - \\beta^\\top \\phi(x_i)).\n",
    "$$\n",
    "\n",
    "So $\\alpha_i$ is (up to scaling) the negative prediction error for point $i$.\n",
    "\n",
    "We can write:\n",
    "\n",
    "- Vector $\\alpha \\in \\mathbb{R}^N$: entries $\\alpha_i$.\n",
    "\n",
    "From the gradient condition:\n",
    "\n",
    "$$\n",
    "2\\Phi^\\top (\\Phi\\beta - Y) + \\lambda \\beta = 0\n",
    "$$\n",
    "\n",
    "Divide by 2 for simplicity:\n",
    "\n",
    "$$\n",
    "\\Phi^\\top (\\Phi\\beta - Y) + \\frac{\\lambda}{2}\\beta = 0\n",
    "$$\n",
    "\n",
    "But it’s simpler to view the derivation in the way presented in the transcript:\n",
    "\n",
    "Using the definition of $\\alpha_i$ and some algebra, you can show:\n",
    "\n",
    "$$\n",
    "\\beta = \\Phi^\\top \\alpha.\n",
    "$$\n",
    "\n",
    "This says: **β is a linear combination of the feature vectors of the training points**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4ee34",
   "metadata": {},
   "source": [
    "#### 3.2 Matrix equations for α\n",
    "\n",
    "We also plug the definition of $\\alpha$ into matrix form:\n",
    "\n",
    "$$\n",
    "- \\lambda \\alpha = \\Phi \\beta - Y.\n",
    "$$\n",
    "\n",
    "Call that equation (4) in the transcript. Combine:\n",
    "\n",
    "- $\\beta = \\Phi^\\top \\alpha$\n",
    "- $-\\lambda \\alpha = \\Phi\\beta - Y$\n",
    "\n",
    "Substitute $\\beta = \\Phi^\\top \\alpha$ into the second:\n",
    "\n",
    "$$\n",
    "- \\lambda \\alpha = \\Phi (\\Phi^\\top \\alpha) - Y = (\\Phi\\Phi^\\top)\\alpha - Y.\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "(\\Phi\\Phi^\\top + \\lambda I)\\alpha = Y.\n",
    "$$\n",
    "\n",
    "Therefore, the **α-solution** is:\n",
    "\n",
    "$$\n",
    "\\alpha^* = (\\Phi\\Phi^\\top + \\lambda I)^{-1} Y.\n",
    "$$\n",
    "\n",
    "Now:\n",
    "\n",
    "- $\\Phi\\Phi^\\top$ is an $N \\times N$ matrix.\n",
    "- You invert an $N \\times N$ matrix: complexity ~ $O(N^3)$.\n",
    "\n",
    "This looks **worse** than the β-formulation if $N \\gg M$ (many more data points than features), which is typical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5c470",
   "metadata": {},
   "source": [
    "#### 3.3 Predictions in terms of α\n",
    "\n",
    "We still know:\n",
    "\n",
    "$$\n",
    "\\beta^* = \\Phi^\\top \\alpha^*.\n",
    "$$\n",
    "\n",
    "So for a new point $x_{\\text{new}}$:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_{\\text{new}}) \n",
    "= \\beta^{*\\top} \\phi(x_{\\text{new}})\n",
    "= \\alpha^{*\\top} \\Phi \\phi(x_{\\text{new}}).\n",
    "$$\n",
    "\n",
    "Expand $\\Phi \\phi(x_{\\text{new}})$:\n",
    "\n",
    "- The $i$-th entry is $\\phi(x_i)^\\top \\phi(x_{\\text{new}})$.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_{\\text{new}}) = \\sum_{i=1}^N \\alpha_i^* \\phi(x_i)^\\top \\phi(x_{\\text{new}}).\n",
    "$$\n",
    "\n",
    "Key observation:\n",
    "\n",
    "- With β-formulation, predictions only need $\\beta$.\n",
    "- With α-formulation, predictions require **all training points** (through the sums).\n",
    "- Even without kernels, this seems computationally and memory-wise worse.\n",
    "\n",
    "So why is this useful? Because of what $\\phi(x_i)^\\top \\phi(x_j)$ really is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffad8c9",
   "metadata": {},
   "source": [
    "### 4. The kernel matrix and kernel function\n",
    "\n",
    "Look at $\\Phi\\Phi^\\top$:\n",
    "\n",
    "- Entry $(i, j)$ is $\\phi(x_i)^\\top \\phi(x_j)$.\n",
    "- This is the **dot product** of feature vectors for points $i$ and $j$.\n",
    "- It measures **similarity** between the two data points **in feature space**.\n",
    "\n",
    "We call:\n",
    "\n",
    "- $K_{ij} = \\phi(x_i)^\\top \\phi(x_j)$  \n",
    "  the **kernel matrix** $K$.\n",
    "- The function  \n",
    "  $k(x, x') = \\phi(x)^\\top \\phi(x')$  \n",
    "  the **kernel function**.\n",
    "\n",
    "In the α-formulation:\n",
    "\n",
    "- Training uses $\\Phi\\Phi^\\top$, i.e., **only dot products** of the form $\\phi(x_i)^\\top \\phi(x_j)$.\n",
    "- Prediction uses sums of $\\phi(x_i)^\\top \\phi(x_{\\text{new}})$, again **only dot products**.\n",
    "\n",
    "This reveals the main insight:\n",
    "\n",
    "> If an algorithm (in this alternative form) only needs dot products of feature vectors $\\phi(x)$, then you never need to know $\\phi(x)$ explicitly—only a function that gives you these dot products.\n",
    "\n",
    "That function is the kernel $k(x, x')$.\n",
    "\n",
    "So we can:\n",
    "\n",
    "- **Replace** $\\phi(x_i)^\\top \\phi(x_j)$ with **any** valid kernel $k(x_i, x_j)$.\n",
    "- This lets us work in a **potentially infinite-dimensional feature space** implicitly.\n",
    "- We avoid ever computing explicit coordinates in that space.\n",
    "\n",
    "This is the **kernel trick**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abe81b",
   "metadata": {},
   "source": [
    "### 5. Why the kernel trick matters\n",
    "\n",
    "#### 5.1 Computation and representation\n",
    "\n",
    "Instead of:\n",
    "\n",
    "- Choosing a finite set of nonlinear features and building $\\phi(x)$ explicitly.\n",
    "- Constructing a potentially huge $\\Phi$ (with possibly thousands or millions of features).\n",
    "- Inverting an $M \\times M$ matrix.\n",
    "\n",
    "We can:\n",
    "\n",
    "- Choose a kernel $k(x, x')$ that corresponds to an **implicit** feature map.\n",
    "- Work directly with the kernel matrix $K$ of size $N \\times N$.\n",
    "- Solve for α and make predictions using only kernel evaluations.\n",
    "\n",
    "Even if the implicit feature space is very high-dimensional (or infinite-dimensional, as with the RBF kernel), computations are done in terms of $N$ and kernel calls, not $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c435b88",
   "metadata": {},
   "source": [
    "#### 5.2 Applicability to many algorithms\n",
    "\n",
    "Kernels are not just for SVMs. Any algorithm that can be written in terms of inner products can be “kernelized,” including:\n",
    "\n",
    "- Linear regression (as shown).\n",
    "- Logistic regression.\n",
    "- Principal Component Analysis (PCA) → kernel PCA.\n",
    "- Others that rely on distances/inner products.\n",
    "\n",
    "The general pattern:\n",
    "\n",
    "1. Rewrite the algorithm in terms of dot products $\\phi(x_i)^\\top \\phi(x_j)$.\n",
    "2. Replace those with $k(x_i, x_j)$.\n",
    "3. Choose a kernel appropriate to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727f371",
   "metadata": {},
   "source": [
    "### 7. Connecting to SVMs and beyond\n",
    "\n",
    "In SVMs (which the module will focus on next):\n",
    "\n",
    "- The optimization problem can be written in “dual” form using only dot products of inputs.\n",
    "- Replacing dot products with kernels turns a **linear SVM** into a **nonlinear SVM**.\n",
    "- The resulting decision boundary in the original space can be dramatically nonlinear, while the algorithm itself uses only a kernel function and constraints on α-like coefficients.\n",
    "\n",
    "Similarly, kernel versions exist for:\n",
    "\n",
    "- Logistic regression (kernel logistic regression).\n",
    "- PCA (kernel PCA).\n",
    "- Other algorithms that rely fundamentally on inner products.\n",
    "\n",
    "The big picture:\n",
    "\n",
    "- Many linear algorithms **depend only on geometry** (relative positions, angles, distances).\n",
    "- Geometry can often be captured fully by inner products.\n",
    "- Kernels let us **redefine geometry** in a richer feature space without ever working in that space explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0da09",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 8. Connecting back to the overview and “higher-dimensional space”\n",
    "\n",
    "The overview says:\n",
    "\n",
    "- A kernel maps nonlinear data into a higher-dimensional space where it becomes linearly separable.\n",
    "- The kernel trick then lets you **use the original feature space coordinates**, without actually computing the high-dimensional ones.\n",
    "\n",
    "Here’s how this ties in:\n",
    "\n",
    "1. When you choose a feature map $\\phi(x)$ that includes nonlinear functions (e.g., all polynomials up to high degree), you are conceptually mapping your data into a **higher-dimensional space**.\n",
    "2. In that space, a linear model might separate the data nicely.\n",
    "3. But explicitly computing $\\phi(x)$ in a very high dimension can be expensive or impossible.\n",
    "4. Fortunately, the model only needs **inner products** $\\phi(x_i)^\\top \\phi(x_j)$ and $\\phi(x_i)^\\top \\phi(x_{\\text{new}})$.\n",
    "5. A kernel $k(x, x')$ gives you those inner products **directly from the original data**.\n",
    "\n",
    "So you get the benefits of a high-dimensional feature space (powerful nonlinear modeling) while computing in the original space using only kernel evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21136fe6",
   "metadata": {},
   "source": [
    "### 9. Why this matters for SVMs and other methods\n",
    "\n",
    "We now know we can do **regularized linear regression** purely with a kernel function instead of explicit features, both in training and prediction.\n",
    "\n",
    "This same idea applies to:\n",
    "\n",
    "- **SVMs**: their “dual” optimization problem is written entirely in terms of inner products between data points. Replacing them with a kernel gives you a **kernel SVM**, which learns nonlinear decision boundaries.\n",
    "- **Logistic regression**: switching to a kernelized formulation gives **kernel logistic regression**.\n",
    "- **PCA**: using kernels yields **kernel PCA**, which finds principal components in a nonlinear feature space.\n",
    "\n",
    "The key conditions:\n",
    "\n",
    "- The algorithm can be rewritten so that it depends on data **only through dot products**.\n",
    "- Once that’s done, swapping in a kernel function gives you a nonlinear, kernel-based version of the algorithm.\n",
    "\n",
    "This is why kernels, and the kernel trick, are such a central idea in classical machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
