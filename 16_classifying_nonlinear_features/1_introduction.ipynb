{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b3e5e3",
   "metadata": {},
   "source": [
    "### What are nonlinear features?\n",
    "\n",
    "Nonlinear features are input variables whose relationship with the target cannot be captured well by a straight line (or linear decision boundary). Instead, they form curves, thresholds, interactions, or more complex patterns in the feature space.\n",
    "\n",
    "Examples include:\n",
    "\n",
    "- Target changes only after a feature crosses a certain threshold.\n",
    "- The effect of a feature increases or decreases at different rates (e.g., quadratic or exponential growth).\n",
    "- The outcome depends on combinations or interactions of features (e.g., XOR patterns).\n",
    "\n",
    "Recognizing such **nonlinear** structure is important because purely linear models will underfit in these settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3995b40",
   "metadata": {},
   "source": [
    "### Why they matter for classification\n",
    "\n",
    "When features and labels are related nonlinearly, linear classifiers (like plain logistic regression or linear SVM) cannot find a single hyperplane that separates the classes well. To model these patterns, we either:\n",
    "\n",
    "- Transform the features into a richer, nonlinear feature space and apply a linear model there.\n",
    "- Use inherently nonlinear models that can learn curved and piecewise-constant decision boundaries directly.\n",
    "\n",
    "This choice strongly affects predictive performance, interpretability, and computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4115f",
   "metadata": {},
   "source": [
    "### Common nonlinear classification algorithms\n",
    "\n",
    "Several supervised learning methods are well suited to nonlinear feature–label relationships:\n",
    "\n",
    "- Decision trees  \n",
    "  - Recursively split the feature space into regions using axis-aligned thresholds.  \n",
    "  - Naturally capture stepwise, threshold-based, and interaction effects.\n",
    "\n",
    "- Random forests  \n",
    "  - Ensembles of many decision trees trained on bootstrapped samples and feature subsets.  \n",
    "  - Average many nonlinear trees to reduce variance and improve generalization.\n",
    "\n",
    "- Gradient boosting machines (GBMs, e.g., XGBoost, LightGBM, CatBoost)  \n",
    "  - Build trees sequentially, where each new tree corrects the residual errors of the previous ones.  \n",
    "  - Very flexible, often state-of-the-art for tabular nonlinear data.\n",
    "\n",
    "- Support Vector Machines (SVMs) with nonlinear kernels  \n",
    "  - Use a kernel function to implicitly map inputs into a high-dimensional feature space.  \n",
    "  - Learn a linear separator in that space, which corresponds to a nonlinear boundary in the original space.\n",
    "\n",
    "- Neural networks (e.g., multilayer perceptrons, deep networks)  \n",
    "  - Stack layers of linear transformations and nonlinear activations.  \n",
    "  - Can approximate highly complex nonlinear functions given enough data, capacity, and regularization.\n",
    "\n",
    "- Gaussian processes (GPs)  \n",
    "  - Define a distribution over functions, specified by a mean and covariance (kernel) function.  \n",
    "  - With appropriate kernels, can flexibly model smooth nonlinear relationships and provide uncertainty estimates.\n",
    "\n",
    "These methods differ in their bias–variance characteristics, scalability, interpretability, and typical application domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e55e5",
   "metadata": {},
   "source": [
    "### Role of feature engineering and regularization\n",
    "\n",
    "Even nonlinear models benefit from careful feature design:\n",
    "\n",
    "- Feature engineering  \n",
    "  - Create polynomial terms, interaction terms, or domain-specific transformations (e.g., log, ratios, cyclic encodings).  \n",
    "  - Use learned representations (e.g., embeddings) for complex inputs like text or categorical variables.\n",
    "\n",
    "- Regularization  \n",
    "  - Penalize model complexity (e.g., weight decay, max depth, dropout, early stopping) to combat overfitting.  \n",
    "  - Choose kernel parameters, tree depth, learning rates, and network width via cross-validation.\n",
    "\n",
    "Balancing expressive power with appropriate regularization is central to successful nonlinear classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c73572",
   "metadata": {},
   "source": [
    "### Emphasis of this module: SVMs with nonlinear kernels\n",
    "\n",
    "This module will focus on using SVMs to handle nonlinear features through kernels. The key ideas are:\n",
    "\n",
    "- Map inputs into a high-dimensional feature space implicitly via a **kernel function** $k(x, x')$.\n",
    "- Learn a maximum-margin hyperplane in that feature space.\n",
    "- Interpret the resulting classifier as a nonlinear decision boundary in the original input space.\n",
    "\n",
    "Typical kernels to be covered:\n",
    "\n",
    "- Polynomial kernel: captures interaction and polynomial effects up to a chosen degree.\n",
    "- Radial Basis Function (RBF) or Gaussian kernel: captures smooth, localized nonlinear patterns.\n",
    "- Other kernels (e.g., sigmoid) as special-purpose choices.\n",
    "\n",
    "We will see how kernel choice and hyperparameters (such as degree, gamma, and regularization parameter control the complexity of the learned nonlinear boundary, and how to tune them in practice using cross-validation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
