{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c07450",
   "metadata": {},
   "source": [
    "Here’s a beginner‑friendly walkthrough of everything in that “Examples of the Kernel Trick” video, step by step, including what the different kernels mean and what the code is doing conceptually.\n",
    "\n",
    "***\n",
    "\n",
    "## 1. Two ways to add nonlinearity (recap)\n",
    "\n",
    "So far you have seen **two** ways to make a *linear regression* model handle nonlinear patterns:\n",
    "\n",
    "1. **Feature‑based approach (φ)**  \n",
    "   - You explicitly build new features \\(\\phi(x)\\) (e.g., \\(x, x^2, x^3, \\sin x\\), etc.).  \n",
    "   - You put these as columns in a table (DataFrame), with the target \\(Y\\) as the last column.  \n",
    "   - You run ordinary linear regression on this feature table and get **β‑coefficients**: \\(\\beta_0, \\dots, \\beta_{M-1}\\).  \n",
    "   - Prediction: \\(\\hat{y}(x) = \\beta^\\top \\phi(x)\\).\n",
    "\n",
    "2. **Kernel‑based approach (K)**  \n",
    "   - Instead of building features, you build a **kernel matrix** \\(K\\), an \\(N \\times N\\) table of pairwise similarities between training points.  \n",
    "   - You run linear regression on this kernel matrix (treating its columns as features) with target \\(Y\\) and obtain **α‑coefficients**: \\(\\alpha_1, \\dots, \\alpha_N\\).  \n",
    "   - Prediction uses a weighted sum of kernel similarities to training points.\n",
    "\n",
    "So:\n",
    "\n",
    "- Feature‑based model: **M parameters** (β’s), one per feature.\n",
    "- Kernel‑based model: **N parameters** (α’s), one per training example.\n",
    "\n",
    "The video’s goal here is to show concrete kernel choices (linear, quadratic, polynomial, Gaussian) and how, in code, you swap them in and see their effect.\n",
    "\n",
    "***\n",
    "\n",
    "## 2. General coding pattern for kernel regression\n",
    "\n",
    "Conceptually, the code for the kernel approach follows the same pattern for any kernel:\n",
    "\n",
    "1. **Define a kernel function** `kfunc(x, z)`  \n",
    "   - Given two vectors (rows) \\(x\\) and \\(z\\), returns a **similarity** number \\(k(x, z)\\).\n",
    "\n",
    "2. **Build the kernel matrix** \\(K\\) for the training data  \n",
    "   - You have training inputs \\(X\\) with shape \\((N, d)\\).  \n",
    "   - The kernel matrix \\(K\\) has shape \\((N, N)\\).  \n",
    "   - Entry \\(K_{ij} = k(x^{(i)}, x^{(j)})\\).\n",
    "\n",
    "3. **Add regularization on the diagonal**  \n",
    "   - They add something like \\(0.1 \\times I_N\\) to \\(K\\).  \n",
    "   - This is the \\(\\lambda I\\) term from ridge regression, ensuring the matrix is invertible and controlling overfitting.\n",
    "\n",
    "4. **Fit a linear regression model using K as the input**  \n",
    "   - Treat the **rows of K** as the “features” and \\(Y\\) as the targets.  \n",
    "   - When you call `fit(K, Y)` on scikit‑learn’s linear regression, it learns coefficients that correspond to **α’s** (one weight per training point).\n",
    "\n",
    "5. **Predict on new data**  \n",
    "   - To predict on test points \\(X_{\\text{test}}\\), construct a **cross‑kernel matrix** \\(K_{\\text{test}}\\) with shape \\((N_{\\text{test}}, N)\\):  \n",
    "     - Entry \\((i, j)\\) is \\(k(x^{(\\text{test})}_i, x^{(\\text{train})}_j)\\).  \n",
    "   - Call `predict(K_test)` on the fitted model to get predictions.  \n",
    "   - Under the hood, this is computing a weighted sum of similarities to all training points.\n",
    "\n",
    "The important conceptual trick:\n",
    "\n",
    "> You never hand‑construct high‑dimensional features. You only compute pairwise kernel values, and standard linear regression (or other models) sits on top of that.\n",
    "\n",
    "***\n",
    "\n",
    "## 3. Linear kernel: kernel version of “no extra features”\n",
    "\n",
    "### 3.1 Linear kernel definition\n",
    "\n",
    "The **linear kernel** between two vectors \\(x\\) and \\(z\\) is:\n",
    "\n",
    "\\[\n",
    "k_{\\text{linear}}(x, z) = x^\\top z.\n",
    "\\]\n",
    "\n",
    "That is just the usual dot product.\n",
    "\n",
    "In NumPy, this is implemented using `.dot` or `@`.\n",
    "\n",
    "- Kernel matrix:\n",
    "  - \\(K_{ij} = x^{(i)\\top} x^{(j)}\\).\n",
    "\n",
    "### 3.2 What the code conceptually does\n",
    "\n",
    "- Compute `K_train`:\n",
    "  - Loop over all pairs of training examples \\((i, j)\\).\n",
    "  - Compute `k_linear(x_i, x_j)` for each.\n",
    "  - Add \\(0.1 \\times I\\) (or similar) to the diagonal for regularization.\n",
    "\n",
    "- Fit linear regression:\n",
    "  - `linreg.fit(K_train, y)`.\n",
    "  - The learned coefficients correspond to α’s.\n",
    "\n",
    "- Build `K_test`:\n",
    "  - For each test point and each training point, compute `k_linear(x_test_i, x_train_j)`.\n",
    "\n",
    "- Predict:\n",
    "  - `y_pred = linreg.predict(K_test)`.\n",
    "\n",
    "### 3.3 Interpretation\n",
    "\n",
    "With a linear kernel and a linear regression on top, you are basically reconstructing **ordinary linear regression in the original features**. The video notes:\n",
    "\n",
    "> “The result is as expected. The linear kernel function is equivalent to just using the original linear features.”\n",
    "\n",
    "So linear kernel = no extra nonlinearity; it mainly serves as a sanity check that the kernel framework matches the plain case.\n",
    "\n",
    "***\n",
    "\n",
    "## 4. Quadratic kernel: implicit second‑degree features\n",
    "\n",
    "### 4.1 Quadratic kernel formula\n",
    "\n",
    "The **quadratic kernel** is a special case of the polynomial kernel with degree 2:\n",
    "\n",
    "\\[\n",
    "k_{\\text{quad}}(x, z) = (x^\\top z + 1)^2.\n",
    "\\]\n",
    "\n",
    "If \\(x\\) and \\(z\\) are 2‑dimensional:\n",
    "\n",
    "- \\(x = (x_1, x_2)\\)\n",
    "- \\(z = (z_1, z_2)\\)\n",
    "\n",
    "then\n",
    "\n",
    "\\[\n",
    "k_{\\text{quad}}(x, z)\n",
    "= (x_1 z_1 + x_2 z_2 + 1)^2.\n",
    "\\]\n",
    "\n",
    "If you **expand this square** (the instructor says “we won’t do it here”), you can rewrite it as:\n",
    "\n",
    "\\[\n",
    "k_{\\text{quad}}(x, z) = \\phi(x)^\\top \\phi(z)\n",
    "\\]\n",
    "\n",
    "for a feature vector\n",
    "\n",
    "\\[\n",
    "\\phi(x) = \\big[1,\\, x_1,\\, x_2,\\, x_1 x_2,\\, x_1^2,\\, x_2^2\\big].\n",
    "\\]\n",
    "\n",
    "So:\n",
    "\n",
    "- The quadratic kernel corresponds to a feature map that includes:\n",
    "  - Constant term: \\(1\\)\n",
    "  - Linear terms: \\(x_1, x_2\\)\n",
    "  - Interaction: \\(x_1 x_2\\)\n",
    "  - Squared terms: \\(x_1^2, x_2^2\\)\n",
    "\n",
    "These are **all monomials up to order 2** in 2 variables.\n",
    "\n",
    "### 4.2 Why this is useful\n",
    "\n",
    "Instead of explicitly creating these 6 features and running regression on them, you:\n",
    "\n",
    "- Use the quadratic kernel to fill the kernel matrix.\n",
    "- Train linear regression on that kernel matrix.\n",
    "\n",
    "Result:\n",
    "\n",
    "> “It is the same as you would get if you included all of the quadratic monomials as features. But a lot easier to do.”\n",
    "\n",
    "The kernel hides the feature expansion; the math is the same as using φ, but the code is cleaner, and it scales better to higher dimensions and degrees.\n",
    "\n",
    "### 4.3 Code‑wise\n",
    "\n",
    "Conceptually, the quadratic kernel function might look like:\n",
    "\n",
    "```python\n",
    "def quadratic_kernel(x, z):\n",
    "    return (x @ z + 1.0) ** 2\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "- Use `quadratic_kernel` instead of `linear_kernel` when forming `K_train` and `K_test`.\n",
    "- Everything else (fit linear regression, predict) stays the same.\n",
    "\n",
    "***\n",
    "\n",
    "## 5. Polynomial kernel: generalizing degree\n",
    "\n",
    "### 5.1 Polynomial kernel definition\n",
    "\n",
    "The **polynomial kernel** generalizes the quadratic kernel by allowing arbitrary degree \\(d\\):\n",
    "\n",
    "\\[\n",
    "k_{\\text{poly}}(x, z) = (x^\\top z + 1)^d,\n",
    "\\]\n",
    "\n",
    "where \\(d\\) is a positive integer (2, 3, 4, …).\n",
    "\n",
    "### 5.2 Corresponding features\n",
    "\n",
    "The polynomial kernel corresponds to a feature vector with **all monomials of the components of \\(x\\) up to degree \\(d\\)**.\n",
    "\n",
    "- If \\(M\\) is the number of original dimensions of \\(x\\), then the number of monomials up to degree \\(d\\) is:\n",
    "\n",
    "\\[\n",
    "\\binom{M + d}{d} = \\frac{(M + d)!}{d! \\, M!}.\n",
    "\\]\n",
    "\n",
    "The video gives examples:\n",
    "\n",
    "- \\(M = 2\\), \\(d = 2\\):  \n",
    "  - \\(\\binom{2 + 2}{2} = \\binom{4}{2} = 6\\) features (matches the 6 monomials above).\n",
    "- \\(M = 10\\), \\(d = 10\\):  \n",
    "  - \\(\\binom{10 + 10}{10} = \\binom{20}{10}\\), which is over 184,000 features.\n",
    "\n",
    "So explicit feature construction quickly becomes huge in dimension.\n",
    "\n",
    "### 5.3 Why the kernel is a win here\n",
    "\n",
    "With a polynomial kernel:\n",
    "\n",
    "- You conceptually get a model that uses all polynomial terms up to degree \\(d\\).\n",
    "- You never explicitly construct those 184k features.\n",
    "- You only compute kernel values \\(k(x_i, x_j)\\), which is just:\n",
    "  - Compute dot product \\(x_i^\\top x_j\\),\n",
    "  - Add 1,\n",
    "  - Raise to power \\(d\\).\n",
    "\n",
    "Thus:\n",
    "\n",
    "> “This is where the kernel method really begins to shine.”\n",
    "\n",
    "You can use extremely rich feature spaces without the combinatorial explosion of actual feature columns.\n",
    "\n",
    "### 5.4 Using scikit‑learn’s built‑in polynomial kernel\n",
    "\n",
    "The video mentions:\n",
    "\n",
    "- Using scikit‑learn’s `polynomial_kernel` from `sklearn.metrics.pairwise`.\n",
    "- For example, with degree 3:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "\n",
    "K_train = polynomial_kernel(X_train, X_train, degree=3)\n",
    "# Add regularization term on the diagonal\n",
    "# Fit linear regression on K_train, target Y\n",
    "```\n",
    "\n",
    "That kernel matrix \\(K\\) is equivalent (in terms of what the model can represent) to using all monomials of degree up to 3 as explicit features.\n",
    "\n",
    "***\n",
    "\n",
    "## 6. Gaussian (RBF) kernel: infinite‑dimensional features\n",
    "\n",
    "### 6.1 Gaussian (RBF) kernel formula\n",
    "\n",
    "The **Gaussian kernel**, also known as the **RBF (radial basis function) kernel**, has the form:\n",
    "\n",
    "\\[\n",
    "k_{\\text{RBF}}(x, z) = \\exp\\big(-\\gamma \\|x - z\\|^2\\big),\n",
    "\\]\n",
    "\n",
    "where:\n",
    "\n",
    "- \\(\\|x - z\\|^2\\) is the squared Euclidean distance.\n",
    "- \\(\\gamma > 0\\) is a hyperparameter that controls how quickly similarity decays with distance.\n",
    "\n",
    "Interpretation as a similarity measure:\n",
    "\n",
    "- If \\(x\\) and \\(z\\) are very close, \\(\\|x - z\\|^2 \\approx 0\\), so \\(k \\approx 1\\).\n",
    "- If they are far apart, \\(\\|x - z\\|^2\\) is large, so \\(k \\approx 0\\).\n",
    "- So this kernel says: “points are similar if they are close in Euclidean space.”\n",
    "\n",
    "### 6.2 Infinite‑dimensional feature space\n",
    "\n",
    "The video notes:\n",
    "\n",
    "- It is **not obvious** how to write this as a dot product of two finite feature vectors.\n",
    "- The corresponding feature map \\(\\phi(x)\\) actually has **infinitely many components**.\n",
    "- You can think of it as taking all monomials of all orders with certain weights; as you append more and more terms, the dot product of those infinite features approaches the Gaussian kernel.\n",
    "\n",
    "This is impossible to implement by explicit features, but:\n",
    "\n",
    "- With the kernel trick, you don’t need to know the infinite feature vector.\n",
    "- You just compute \\(k(x, z)\\) directly.\n",
    "\n",
    "So the Gaussian kernel is a vivid example of the power of kernels: you get a model equivalent to a linear model in an infinite‑dimensional space, but your code stays finite and simple.\n",
    "\n",
    "### 6.3 Using Gaussian kernel in code\n",
    "\n",
    "The video says:\n",
    "\n",
    "- Replace the polynomial kernel with `rbf_kernel` from scikit‑learn’s pairwise metrics.\n",
    "- For example:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "gamma = 0.5  # example value\n",
    "K_train = rbf_kernel(X_train, X_train, gamma=gamma)\n",
    "# Add regularization (e.g., 0.1 * I) to K_train\n",
    "# Fit linear regression on K_train, target Y\n",
    "\n",
    "K_test = rbf_kernel(X_test, X_train, gamma=gamma)\n",
    "# Predict using the kernel-based regression model\n",
    "```\n",
    "\n",
    "Empirically:\n",
    "\n",
    "> “The Gaussian kernel often produces pretty good results, as we can see here.”\n",
    "\n",
    "In practice, RBF kernels are very popular in SVMs, kernel ridge regression, etc., because they are flexible, smooth, and often work well with sensible choices of \\(\\gamma\\).\n",
    "\n",
    "***\n",
    "\n",
    "## 7. Big picture: what the examples show\n",
    "\n",
    "The video’s examples make these points concrete:\n",
    "\n",
    "1. **Feature‑based vs kernel‑based**  \n",
    "   - Feature‑based: explicitly build columns for all monomials (quadratic, cubic, etc.).  \n",
    "   - Kernel‑based: define one function `k(x, z)`, build an \\(N \\times N\\) kernel matrix, and let linear regression fit **α’s**.\n",
    "\n",
    "2. **Linear kernel**  \n",
    "   - Equivalent to plain linear regression with original features (no new nonlinearity).\n",
    "   - Serves as a baseline check.\n",
    "\n",
    "3. **Quadratic and polynomial kernels**  \n",
    "   - Implicitly create all monomials up to degree \\(d\\).\n",
    "   - Avoid explicit explosion of feature columns.\n",
    "\n",
    "4. **Gaussian/RBF kernel**  \n",
    "   - Implicitly uses an infinite number of features.\n",
    "   - Captures very flexible, smooth nonlinear relationships based solely on pairwise distances.\n",
    "\n",
    "5. **Same code pattern, just swap kernels**  \n",
    "   - The training/prediction logic is the same; you only change which kernel function is used to build \\(K\\) and \\(K_{\\text{test}}\\).\n",
    "\n",
    "Finally, the video closes by emphasizing:\n",
    "\n",
    "- The **kernel trick** is not limited to linear regression.  \n",
    "- It can be applied to **PCA**, **logistic regression**, and many other algorithms that can be expressed in terms of inner products.  \n",
    "- In the **next video**, this will be especially useful for the **maximum margin classifier**, i.e., support vector machines (SVMs), where kernels are most famous.\n",
    "\n",
    "If you’d like, I can next write out a concrete toy Python example (with NumPy + scikit‑learn) that shows:\n",
    "\n",
    "- feature‑based polynomial regression vs\n",
    "- kernel‑based polynomial regression vs\n",
    "- kernel‑based Gaussian regression,\n",
    "\n",
    "on the same 1D dataset so you can see the behavior numerically and visually."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
