{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42bce15",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) combine two ideas you’ve already seen: **maximum‑margin classifiers** and the **kernel trick**. \n",
    "\n",
    "They find a separating hyperplane that is as far as possible from the closest training points, and (optionally) use kernels to make that hyperplane nonlinear in the original input space. [heartbeat.comet](https://heartbeat.comet.ml/understanding-the-mathematics-behind-support-vector-machines-5e20243d64d5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647e6f3",
   "metadata": {},
   "source": [
    "## 1. Core SVM concepts in plain language\n",
    "\n",
    "### 1.1 Hyperplane and linear separation\n",
    "\n",
    "- In SVMs, a **hyperplane** is the decision boundary: a line in 2D, a plane in 3D, and in general something of the form  \n",
    "  $w^\\top x + b = 0$. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
    "  \n",
    "- If the data is linearly separable, we can draw such a hyperplane so that:\n",
    "  - All points of class +1 are on one side.\n",
    "  - All points of class −1 are on the other. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
    "\n",
    "Among all possible separating hyperplanes, SVM chooses the one with the **maximum margin**. [heartbeat.comet](https://heartbeat.comet.ml/understanding-the-mathematics-behind-support-vector-machines-5e20243d64d5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c78be6",
   "metadata": {},
   "source": [
    "### 1.2 Margin and support vectors\n",
    "\n",
    "- The **margin** is the distance from the hyperplane to the **nearest data points** on each side. [en.wikipedia](https://en.wikipedia.org/wiki/Margin_(machine_learning))\n",
    "\n",
    "- The points that lie exactly on the margin boundaries are called **support vectors**. [pages.hmc](https://pages.hmc.edu/ruye/MachineLearning/lectures/ch9/node6.html)\n",
    "\n",
    "  - If you removed or moved them, the optimal hyperplane would shift.\n",
    "  - If you move any non‑support point slightly (and it stays outside the margin), the boundary stays the same. [quantstart](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners/)\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- Larger margin → more “breathing room” around the boundary → the classifier is more robust to noise and small shifts in data. [alan-turing-institute.github](https://alan-turing-institute.github.io/Intro-to-transparent-ML-course/08-glm-svm/support-vec-classifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72edc0",
   "metadata": {},
   "source": [
    "## 2. From maximum‑margin classifier to SVM\n",
    "\n",
    "### 2.1 Maximum margin classifier (hard margin)\n",
    "\n",
    "In the ideal separable case:\n",
    "\n",
    "- You find $w$ and $b$ that solve:\n",
    "\n",
    "$$\n",
    "\\text{Minimize } \\|w\\|^2\n",
    "\\quad\\text{subject to } y_i (w^\\top x_i + b) \\ge 1,\\ \\forall i,\n",
    "$$\n",
    "\n",
    "where $y_i \\in \\{+1,-1\\}$. [cs.cornell](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html)\n",
    "\n",
    "- The constraints enforce correct classification with margin at least 1.\n",
    "- Minimizing $\\|w\\|^2$ maximizes the margin (margin is $1/\\|w\\|$). [cs.cornell](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html)\n",
    "\n",
    "This is the **maximum margin classifier** (hard‑margin SVM). [web.stanford](https://web.stanford.edu/class/stats202/notes/Support-vector-machines/Maximal-margin-classifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44f5ba",
   "metadata": {},
   "source": [
    "### 2.2 Soft margin: handling overlap and noise\n",
    "\n",
    "Real data often overlaps or has outliers, so those constraints can’t all be satisfied. [pub.aimind](https://pub.aimind.so/soft-margin-svm-exploring-slack-variables-the-c-parameter-and-flexibility-1555f4834ecc)\n",
    "\n",
    "To fix this, introduce **slack variables** $\\xi_i \\ge 0$ (called $C_i$ or $\\delta_i$ in your transcript):\n",
    "\n",
    "- Relax the constraints to:\n",
    "\n",
    "$$\n",
    "y_i (w^\\top x_i + b) \\ge 1 - \\xi_i,\\quad \\xi_i \\ge 0.\n",
    "$$\n",
    "\n",
    "- If $\\xi_i = 0$, point is correctly classified and outside/on the margin.\n",
    "- If $0 < \\xi_i < 1$, point is inside the margin but correctly classified.\n",
    "- If $\\xi_i > 1$, point is misclassified. [eitca](https://eitca.org/artificial-intelligence/eitc-ai-mlp-machine-learning-with-python/support-vector-machine/soft-margin-svm/examination-review-soft-margin-svm/what-is-the-role-of-slack-variables-in-soft-margin-svm/)\n",
    "\n",
    "The **soft‑margin SVM** optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\text{Minimize } \\frac{1}{2}\\|w\\|^2 + C \\sum_{i=1}^N \\xi_i\n",
    "$$\n",
    "\n",
    "subject to the constraints above. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/using-a-hard-margin-vs-soft-margin-in-svm/)\n",
    "\n",
    "- $C > 0$ is a hyperparameter:\n",
    "  - Larger $C$ → strongly penalize violations → narrower margin, lower training error, risk of overfitting. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)\n",
    "  - Smaller $C$ → allow more violations → wider margin, more regularization, risk of underfitting. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)\n",
    "\n",
    "This is still a **convex** optimization, so it can be solved efficiently and has a unique optimum. [courses.grainger.illinois](https://courses.grainger.illinois.edu/cs446/sp2015/Slides/Lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b259711",
   "metadata": {},
   "source": [
    "### 2.3 Which points matter?\n",
    "\n",
    "In soft‑margin SVM:\n",
    "\n",
    "- Points with $\\xi_i > 0$ or lying exactly on the margin boundary become **support vectors**. [eitca](https://eitca.org/artificial-intelligence/eitc-ai-mlp-machine-learning-with-python/support-vector-machine/soft-margin-svm/examination-review-soft-margin-svm/what-is-the-role-of-slack-variables-in-soft-margin-svm/)\n",
    "- All other points have $\\xi_i = 0$ and lie outside the margin; they don’t affect the final boundary.\n",
    "\n",
    "This is exactly what your 1D picture described:\n",
    "\n",
    "- Red dot: decision boundary.\n",
    "- One green point misclassified (slack $>1$).\n",
    "- One green point inside the margin but correctly classified (slack between 0 and 1).\n",
    "- All others with slack 0. [eitca](https://eitca.org/artificial-intelligence/eitc-ai-mlp-machine-learning-with-python/support-vector-machine/soft-margin-svm/examination-review-soft-margin-svm/what-is-the-role-of-slack-variables-in-soft-margin-svm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd83b7c",
   "metadata": {},
   "source": [
    "## 3. Adding kernels: from Support Vector Classifier to full SVM\n",
    "\n",
    "The **support vector classifier (SVC)** is the maximum‑margin classifier (soft or hard margin) in **original feature space**. [bioinformatics-training.github](https://bioinformatics-training.github.io/intro-machine-learning-2017/svm.html)\n",
    "\n",
    "The **support vector machine** is that same classifier, but using the **kernel trick** to operate in an implicit high‑dimensional feature space:\n",
    "\n",
    "- Replace dot products $x_i^\\top x_j$ with a kernel function $k(x_i, x_j)$. [quantstart](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners/)\n",
    "- Common kernels:\n",
    "  - Linear: $k(x,z) = x^\\top z$.\n",
    "  - Polynomial: $k(x,z) = (\\gamma x^\\top z + r)^{d}$.\n",
    "  - RBF / Gaussian: $k(x,z) = \\exp(-\\gamma \\|x - z\\|^2)$. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/major-kernel-functions-in-support-vector-machine-svm/)\n",
    "\n",
    "So in practice:\n",
    "\n",
    "- **Linear hyperplane** in original space → often called a **support vector classifier** (linear SVM).\n",
    "- **Nonlinear hyperplane** via kernels → typically called **support vector machine**. [quantstart](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners/)\n",
    "\n",
    "This is exactly what the transcript means when it says:\n",
    "\n",
    "> If the hyperplane is linear then it is called SVC. If the hyperplane is non‑linear then it is SVM which uses the kernel trick.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- You keep the **sparsity** of support vectors (only some points matter).\n",
    "- You get the **flexibility** of kernel methods (complex nonlinear boundaries). [heartbeat.comet](https://heartbeat.comet.ml/understanding-the-mathematics-behind-support-vector-machines-5e20243d64d5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f741660",
   "metadata": {},
   "source": [
    "## 4. Advantages and disadvantages of SVMs\n",
    "\n",
    "These match standard lists in the literature. [pwskills](https://pwskills.com/blog/svm-in-machine-learning/)\n",
    "\n",
    "### 4.1 Advantages\n",
    "\n",
    "- **Works well when classes are well separated.**  \n",
    "  - A clear margin between classes fits SVM’s maximum‑margin assumption; performance is typically strong. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
    "- **Effective in high‑dimensional spaces.**  \n",
    "  - SVMs handle many features well, because the margin maximization acts as regularization. [pwskills](https://pwskills.com/blog/svm-in-machine-learning/)\n",
    "- **Good when number of dimensions > number of samples.**  \n",
    "  - You can still find a separating hyperplane with strong regularization even when features outnumber examples. [pwskills](https://pwskills.com/blog/svm-in-machine-learning/)\n",
    "- **Memory efficient.**  \n",
    "  - Only support vectors are needed at prediction time, not the entire dataset. [quantstart](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners/)\n",
    "\n",
    "### 4.2 Disadvantages\n",
    "\n",
    "- **Not ideal for very large datasets.**  \n",
    "  - Training time scales at least quadratically in the number of samples for standard SVM solvers; can be slow on millions of points. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
    "- **Sensitive to noise and overlapping classes.**  \n",
    "  - Overlapping class distributions mean many points inside the margin or misclassified, making SVMs less effective unless hyperparameters are tuned carefully. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/using-a-hard-margin-vs-soft-margin-in-svm/)\n",
    "- **No direct probabilistic interpretation.**  \n",
    "  - SVM outputs signed distances to the hyperplane, not probabilities. Probability estimates require extra calibration (e.g., Platt scaling). [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
    "- **Kernel and hyperparameter tuning can be complex.**  \n",
    "  - Choosing kernel type, $C$, $\\gamma$, degree, and other parameters typically needs cross‑validation and can be time‑consuming. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b369f8c",
   "metadata": {},
   "source": [
    "## 5. Using SVMs in scikit‑learn \n",
    "\n",
    "Using scikit‑learn’s `SVC` on synthetic and real data. [jaquesgrobler.github](http://jaquesgrobler.github.io/online-sklearn-build/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc421b8",
   "metadata": {},
   "source": [
    "### 5.1 Basic SVC usage\n",
    "\n",
    "Core steps:\n",
    "\n",
    "1. Import and create the model:\n",
    "\n",
    "   ```python\n",
    "   from sklearn.svm import SVC\n",
    "\n",
    "   clf = SVC(kernel=\"linear\", C=1.0)\n",
    "   ```\n",
    "\n",
    "2. Fit on training data:\n",
    "\n",
    "   ```python\n",
    "   clf.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "3. Predict on new data:\n",
    "\n",
    "   ```python\n",
    "   y_pred = clf.predict(X_test)\n",
    "   ```\n",
    "\n",
    "Here:\n",
    "\n",
    "- `kernel=\"linear\"`: use a linear decision boundary. [jaquesgrobler.github](http://jaquesgrobler.github.io/online-sklearn-build/modules/generated/sklearn.svm.SVC.html)\n",
    "- `C`: soft‑margin parameter; controls margin vs misclassification trade‑off. [jaquesgrobler.github](http://jaquesgrobler.github.io/online-sklearn-build/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "This is a **support vector classifier** with a linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032408f7",
   "metadata": {},
   "source": [
    "### 5.2 Polynomial kernel in scikit‑learn\n",
    "\n",
    "To use a **polynomial kernel**, you specify kernel and its hyperparameters:\n",
    "\n",
    "```python\n",
    "clf_poly = SVC(\n",
    "    kernel=\"poly\",\n",
    "    degree=8,      # d: degree of the polynomial\n",
    "    gamma=\"scale\", # kernel coefficient; or set to a positive float\n",
    "    coef0=1.0,     # r: independent term in the kernel\n",
    "    C=1.0\n",
    ")\n",
    "clf_poly.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Hyperparameters: [stackoverflow](https://stackoverflow.com/questions/56072682/does-the-param-coef0-mean-a-specific-coefficient-in-sklearn-svm-svc-method)\n",
    "\n",
    "- `degree`:\n",
    "  - Degree $d$ of the polynomial; controls complexity (higher = more flexible boundary). [jaquesgrobler.github](http://jaquesgrobler.github.io/online-sklearn-build/modules/generated/sklearn.svm.SVC.html)\n",
    "- `gamma`:\n",
    "  - Kernel coefficient; affects how strongly each training point influences the decision boundary. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)\n",
    "  - `\"scale\"` or `\"auto\"` lets scikit‑learn choose a reasonable default. [jaquesgrobler.github](http://jaquesgrobler.github.io/online-sklearn-build/modules/generated/sklearn.svm.SVC.html)\n",
    "- `coef0`:\n",
    "  - Offset term $r$ inside the polynomial kernel; important for `poly` and `sigmoid` kernels. [stackoverflow](https://stackoverflow.com/questions/56072682/does-the-param-coef0-mean-a-specific-coefficient-in-sklearn-svm-svc-method)\n",
    "  - The video warns that scikit‑learn defaults `coef0` to 0 unless you set it, which may not match the +1 offset used in theory.\n",
    "\n",
    "Polynomial kernel behavior:\n",
    "\n",
    "- Generates nonlinear decision boundaries that can be quite complex for high degrees (like 8).\n",
    "- Implicitly corresponds to including all monomials up to degree $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89bf1cb",
   "metadata": {},
   "source": [
    "\n",
    "### 5.3 RBF (Gaussian) kernel in scikit‑learn\n",
    "\n",
    "To use the **Gaussian (RBF) kernel**:\n",
    "\n",
    "```python\n",
    "clf_rbf = SVC(\n",
    "    kernel=\"rbf\",\n",
    "    gamma=10.0,  # example value\n",
    "    C=1.0\n",
    ")\n",
    "clf_rbf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "- `kernel=\"rbf\"`: use the radial basis function kernel  \n",
    "  $k(x,z) = \\exp(-\\gamma \\|x - z\\|^2)$. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
    "- `gamma`:\n",
    "  - Controls how quickly similarity decays with distance. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)\n",
    "  - Larger `gamma` → more local, wiggly boundaries, higher risk of overfitting.\n",
    "  - Smaller `gamma` → smoother, more global boundary, higher risk of underfitting. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)\n",
    "\n",
    "The video shows:\n",
    "\n",
    "- On a double‑spiral dataset, RBF SVM can learn very curved decision boundaries that separate the intertwined arms well.\n",
    "- Hyperparameters (`gamma`, and often `C`) can be tuned with cross‑validation:\n",
    "  - Try a grid of values, pick the one with highest validation accuracy. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33f37b",
   "metadata": {},
   "source": [
    "### 5.4 Applied to the wine dataset (2D projection)\n",
    "\n",
    "Back to the wine example:\n",
    "\n",
    "- Use only two features: `total_phenols` and `color_intensity` (as in the earlier modules).\n",
    "- Fit an SVM with a suitable kernel (often RBF or low‑degree polynomial).\n",
    "- The resulting decision boundaries:\n",
    "  - Capture most of the class structure.\n",
    "  - Are **less wiggly** than k‑NN but more flexible than plain linear logistic regression.\n",
    "  - Achieve around **89% cross‑validated accuracy** in the 2D setting described, comparable to carefully engineered nonlinear logistic regression features but with far less manual work.\n",
    "\n",
    "Key benefit:\n",
    "\n",
    "- You don’t manually build quadratic, cubic, exponential, or Fourier features.\n",
    "- You **set the kernel** and let the SVM handle the nonlinear feature mapping implicitly.\n",
    "- This simplicity and strong performance are why SVMs became one of the most popular classical ML methods for classification and regression. [quantstart](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
