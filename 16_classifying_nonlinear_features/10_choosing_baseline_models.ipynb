{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d250e4",
   "metadata": {},
   "source": [
    "The key idea is that a **baseline model** is a simple reference point, and SVMs (via SVC) are then compared against that baseline and against each other under different kernels and hyperparameters. [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ae96e",
   "metadata": {},
   "source": [
    "## 1. Baseline model: DummyClassifier\n",
    "\n",
    "A **baseline classifier** is a very simple model you use to answer: *“Is my real model actually learning anything?”* [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/ml-dummy-classifiers-using-sklearn/)\n",
    "\n",
    "- In scikit‑learn, `DummyClassifier` ignores the input features and predicts using trivial rules like:\n",
    "  - Always predict the most frequent class (`strategy=\"most_frequent\"`).\n",
    "  - Predict classes randomly according to their observed frequencies (`\"stratified\"`).\n",
    "  - Predict a constant label (`\"constant\"`). [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)\n",
    "- Its **baseline score** is just the accuracy of this dummy model on the test set. [towardsdatascience](https://towardsdatascience.com/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e/)\n",
    "\n",
    "If your real model can’t beat the dummy, it’s not useful.\n",
    "\n",
    "Example intuition:\n",
    "\n",
    "- If 80% of wines are class 0, `DummyClassifier(strategy=\"most_frequent\")` achieves 80% accuracy by always predicting 0.  \n",
    "- Any real classifier should aim for **better than 80%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90754e20",
   "metadata": {},
   "source": [
    "## 2. Support Vector Classifier (SVC) and kernels\n",
    "\n",
    "An **SVC** is scikit‑learn’s implementation of a support vector machine for classification. [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "- It can model:\n",
    "  - **Linear** decision boundaries (`kernel=\"linear\"`).\n",
    "  - **Nonlinear** boundaries via kernels (`\"poly\"`, `\"rbf\"`, `\"sigmoid\"`). [scikit-learn](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html)\n",
    "- In the notebook, the **wine dataset** is used with SVC:\n",
    "  - Train SVC on two or more wine features.\n",
    "  - Compare performance under different kernels (linear, polynomial, RBF, sigmoidal).\n",
    "  - See how kernel choice and hyperparameters change accuracy and boundary shape. [scikit-learn](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html)\n",
    "\n",
    "Conceptually:\n",
    "\n",
    "- SVC uses the **maximum margin classifier** idea and the **kernel trick**:\n",
    "  - Finds a hyperplane in feature space that maximizes the margin.\n",
    "  - Kernels implicitly define the feature space without manual feature engineering. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/major-kernel-functions-in-support-vector-machine-svm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0c727",
   "metadata": {},
   "source": [
    "## 3. Choosing a kernel: practical guidelines\n",
    "\n",
    "The **kernel function** $k(x,z)$ determines the geometry of the feature space and the shape of the decision boundary. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/major-kernel-functions-in-support-vector-machine-svm/)\n",
    "\n",
    "Common kernels in SVC: [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "- **Linear** (`kernel=\"linear\"`):  \n",
    "  - $k(x,z) = x^\\top z$.  \n",
    "  - Boundary is a straight hyperplane in the original space.\n",
    "- **Polynomial** (`\"poly\"`):  \n",
    "  - $k(x,z) = (\\gamma x^\\top z + r)^d$.  \n",
    "  - Captures polynomial interactions and mild curvature.\n",
    "- **RBF / Gaussian** (`\"rbf\"`):  \n",
    "  - $k(x,z) = \\exp(-\\gamma \\|x - z\\|^2)$.  \n",
    "  - Very flexible; good for complex, local patterns.\n",
    "- **Sigmoid** (`\"sigmoid\"`):  \n",
    "  - $k(x,z) = \\tanh(\\gamma x^\\top z + r)$.  \n",
    "  - Related to neural network activation functions; less commonly used. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
    "\n",
    "Guidelines (as in the mini‑lesson and standard practice): [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
    "\n",
    "- **Linear separability**:\n",
    "  - If data looks roughly linearly separable (classes separated by a “wide” straight boundary), start with a **linear kernel**.\n",
    "- **Problem complexity**:\n",
    "  - If boundaries appear curved or complex (e.g., spiral, circles), use a nonlinear kernel like **RBF** or **polynomial**. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/major-kernel-functions-in-support-vector-machine-svm/)\n",
    "- **Computational budget**:\n",
    "  - Linear SVC scales better on very large datasets.\n",
    "  - Polynomial and RBF kernels can be more computationally expensive, especially if you tune many hyperparameters. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
    "- **Empirical testing**:\n",
    "  - It’s hard to know the best kernel a prior; you typically **try several** and compare with cross‑validation. [scikit-learn](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89786631",
   "metadata": {},
   "source": [
    "## 4. Tuning kernel hyperparameters (grid search & CV)\n",
    "\n",
    "You usually tune kernel hyperparameters using **grid search** plus **cross‑validation**: [youtube](https://www.youtube.com/watch?v=ZobQggQtRt8)\n",
    "\n",
    "1. Choose a set of candidate kernels and parameter grids, e.g.:\n",
    "   - Linear: just vary $C$.\n",
    "   - Polynomial: vary `degree`, `gamma`, `coef0`, `C`.\n",
    "   - RBF: vary `gamma`, `C`. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)\n",
    "2. For each combination, run cross‑validation and compute metrics (accuracy, F1, etc.).\n",
    "3. Pick the combination with the best cross‑validation performance, considering:\n",
    "   - Predictive performance.\n",
    "   - Computational cost.\n",
    "   - Interpretability (linear models are easier to explain). [scikit-learn](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ee38a",
   "metadata": {},
   "source": [
    "## 5. Understanding gamma (γ) in kernels like RBF and poly\n",
    "How `gamma` is treated in scikit‑learn: [stackoverflow](https://stackoverflow.com/questions/59594653/what-does-the-gamma-parameter-in-svm-svc-actually-do)\n",
    "\n",
    "- `gamma` is the **kernel coefficient** for `rbf`, `poly`, and `sigmoid` kernels. [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "- Intuition in RBF:\n",
    "\n",
    "  - $k(x,z) = \\exp(-\\gamma \\|x - z\\|^2)$. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)\n",
    "  - **High gamma**:\n",
    "    - Distance matters a lot; each training point influences only a tiny neighborhood.\n",
    "    - Decision boundary becomes more complex and wiggly around training points.\n",
    "    - Higher risk of **overfitting**—fits training data very closely but may generalize poorly. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)\n",
    "  - **Low gamma**:\n",
    "    - Distance matters less; points have broader influence.\n",
    "    - Boundary is smoother and simpler.\n",
    "    - Higher risk of **underfitting**—boundary may be too coarse to capture real structure. [geeksforgeeks](https://www.geeksforgeeks.org/python/rbf-svm-parameters-in-scikit-learn/)\n",
    "\n",
    "- You choose the **best gamma** via cross‑validation:\n",
    "  - The gamma that gives the highest cross‑validated score is preferred.\n",
    "  - It indicates a good balance between bias and variance (good generalization). [youtube](https://www.youtube.com/watch?v=ZobQggQtRt8)\n",
    "\n",
    "The same idea applies (with some nuance) to `gamma` in polynomial and sigmoid kernels, though RBF is the most common case where gamma tuning is critical. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/major-kernel-functions-in-support-vector-machine-svm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d75553",
   "metadata": {},
   "source": [
    "## 6. Summary of how to use this in practice\n",
    "\n",
    "1. **Start with a baseline**:\n",
    "   - Use `DummyClassifier` to get a trivial benchmark accuracy. [towardsdatascience](https://towardsdatascience.com/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e/)\n",
    "2. **Train an SVC with a simple kernel**:\n",
    "   - Try `kernel=\"linear\"` on your dataset.\n",
    "3. **Explore more complex kernels if needed**:\n",
    "   - Try `poly`, `rbf`, and `sigmoid`, especially when the data is obviously non‑linear. [scikit-learn](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html)\n",
    "4. **Use cross‑validation and grid search**:\n",
    "   - Tune `C`, `gamma`, and `degree` (for poly), and pick the configuration with the best cross‑val metrics. [youtube](https://www.youtube.com/watch?v=ZobQggQtRt8)\n",
    "5. **Check for over/underfitting**:\n",
    "   - Very high gamma or very high degree → watch for overfitting.\n",
    "   - Very low gamma or too simple kernel → watch for underfitting.\n",
    "\n",
    "Define a simple baseline, then compare SVC variants with different kernels and gamma values, using cross‑validation to justify your choices."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
