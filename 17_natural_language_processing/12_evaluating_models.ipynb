{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b16e643",
   "metadata": {},
   "source": [
    "## Evaluating Sentiment Analysis Models (Tweet Data)\n",
    "\n",
    "Model evaluation answers one question:\n",
    "\n",
    "> **‚ÄúHow well will my model perform on unseen, real-world data?‚Äù**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df546f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Prepare Your Data\n",
    "\n",
    "### Dataset Splitting\n",
    "\n",
    "Typical split:\n",
    "\n",
    "* **70% Training** ‚Üí learn parameters\n",
    "* **15% Validation** ‚Üí tune hyperparameters\n",
    "* **15% Test** ‚Üí final, unbiased evaluation\n",
    "\n",
    "üîë **Rule**: *Never* tune using the test set.\n",
    "\n",
    "### Labeling\n",
    "\n",
    "* Sentiments: **positive / negative / neutral**\n",
    "* Labels must be:\n",
    "\n",
    "  * Correct\n",
    "  * Consistent\n",
    "  * Representative of real usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a0a63",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Evaluation Metrics (Very Important)\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{\\text{Total}}\n",
    "$$\n",
    "\n",
    "* ‚úÖ Good for **balanced datasets**\n",
    "* ‚ùå Misleading for **imbalanced data**\n",
    "\n",
    "\n",
    "\n",
    "### Precision\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "**Use when:** false positives are costly  \n",
    "\n",
    "üìå Example: labeling neutral tweets as *negative*\n",
    "\n",
    "\n",
    "\n",
    "### Recall (Sensitivity)\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "**Use when:** false negatives are costly   \n",
    "\n",
    "üìå Example: missing negative tweets about a product\n",
    "\n",
    "\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision √ó Recall}}{\\text{Precision + Recall}}\n",
    "$$\n",
    "\n",
    "* Best **single metric** for imbalanced sentiment data    \n",
    "* Balances precision and recall\n",
    "\n",
    "\n",
    "\n",
    "### AUC‚ÄìROC\n",
    "\n",
    "* Measures **class separability**\n",
    "* Useful for **binary classification**\n",
    "* Robust to **class imbalance**\n",
    "\n",
    "\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Shows:\n",
    "\n",
    "* True Positives (TP)\n",
    "* True Negatives (TN)\n",
    "* False Positives (FP)\n",
    "* False Negatives (FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6d621",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Perform Evaluation\n",
    "\n",
    "### Confusion Matrix Analysis\n",
    "\n",
    "Ask:\n",
    "\n",
    "* Which sentiment is misclassified most?\n",
    "* Are errors systematic or random?\n",
    "\n",
    "\n",
    "\n",
    "### Cross-Validation (k-fold)\n",
    "\n",
    "* Train/test on multiple splits\n",
    "* Ensures **stability and consistency**\n",
    "\n",
    "\n",
    "\n",
    "### Baseline Comparison\n",
    "\n",
    "Always compare against:\n",
    "\n",
    "* Majority-class predictor\n",
    "* Simple rule-based sentiment model\n",
    "\n",
    "üîë If you can‚Äôt beat the baseline ‚Üí model is useless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928731f9",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Assess Real-World Performance\n",
    "\n",
    "### Manual Review\n",
    "\n",
    "* Sample predictions\n",
    "* Compare with human judgment\n",
    "\n",
    "\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Look for:\n",
    "\n",
    "* Sarcasm\n",
    "* Slang\n",
    "* Emojis\n",
    "* Negation (‚Äúnot good‚Äù)\n",
    "\n",
    "This guides feature/model improvements.\n",
    "\n",
    "\n",
    "\n",
    "### Domain-Specific Testing\n",
    "\n",
    "* News tweets\n",
    "* Product reviews\n",
    "* Political tweets\n",
    "\n",
    "Checks **generalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d7514b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Check for Bias\n",
    "\n",
    "### Bias Analysis\n",
    "\n",
    "Evaluate:\n",
    "\n",
    "* Performance skew across topics\n",
    "* Language style sensitivity\n",
    "* Demographic language patterns\n",
    "\n",
    "‚ö†Ô∏è Critical for fairness and ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b2f52",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ External Benchmarks\n",
    "\n",
    "* Compare against:\n",
    "\n",
    "  * Published models\n",
    "  * Pretrained sentiment analyzers\n",
    "* Gives **context**, not just raw numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69128b0",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Continuous Monitoring (Production)\n",
    "\n",
    "### Real-Time Monitoring\n",
    "\n",
    "* Track accuracy drift\n",
    "* Watch for language evolution\n",
    "\n",
    "### Periodic Retraining\n",
    "\n",
    "* Slang changes\n",
    "* New hashtags\n",
    "* New sentiment patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ac21d",
   "metadata": {},
   "source": [
    "## Summary ‚≠ê\n",
    "\n",
    "* Accuracy ‚â† always good\n",
    "* F1 is best for imbalanced sentiment data\n",
    "* Confusion matrix explains *why* accuracy is low\n",
    "* Always compare to a baseline\n",
    "* Manual + automated evaluation both matter\n",
    "* Models degrade over time ‚Üí retraining required"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
