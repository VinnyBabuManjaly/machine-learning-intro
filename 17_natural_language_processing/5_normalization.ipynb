{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233fdfdf",
   "metadata": {},
   "source": [
    "Normalization standardizes messy tokens from tokenization, making text consistent for ML models by reducing variations like case or word forms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c55927",
   "metadata": {},
   "source": [
    "## Why Normalize?\n",
    "Raw tokens have noise (e.g., \"Home Run\" vs \"home run\"). Normalization cuts variations, shrinks data size, boosts model efficiency, and handles human quirks like spelling errors. Key goal: Turn diverse text into a \"standard\" form before feature extraction (tokens → numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952c3af",
   "metadata": {},
   "source": [
    "## Overview: Normalization Techniques\n",
    "\n",
    "Common steps (apply after tokenization):\n",
    "- **Case folding:** All lowercase (HOME → home).\n",
    "- **Numbers:** Convert to words (\"5\" → \"five\") or remove.\n",
    "- **Punctuation/special chars:** Remove $, @, #, accents.\n",
    "- **Whitespace:** Trim extras.\n",
    "- **Abbreviations:** Expand (\"can't\" → \"cannot\").\n",
    "- **Stop words:** Drop uninformative ones (\"the\", \"is\", \"and\").\n",
    "- **Canonicalizing:** Standardize dates, etc.\n",
    "- **Stemming/Lemmatization:** Reduce to root forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf1703b",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Applies operations sequentially.\n",
    "\n",
    "### 1. POS Tagging (`pos_tag()`)\n",
    "Part-of-speech (POS) tagging involves assigning labels to each word in a sentence based on its role in the sentence (noun, verb, adjective, etc). This helps in understanding the grammatical structure and meaning of the text.\n",
    "Identifies grammar: \"The\" = determiner (DT), \"movie\" = noun (NN), \"was\" = verb (VBD).  \n",
    "Names like \"Tom Cruise\" = proper nouns (NNP). Check tags: `nltk.help.upenn_tagset()`. PRP$ = possessive pronoun.\n",
    "\n",
    "### 2. Named Entity Recognition (NER) (`ne_chunk()`)\n",
    "Named entity recognition involves identifying and classifying entities in text into predefined categories (names, organizations, locations, dates, etc)\n",
    "Tags real-world items: PERSON (Tom Cruise), ORGANIZATION.  \n",
    "Use: Remove entities for privacy (loses specifics but keeps sentiment).\n",
    "\n",
    "### 3. Lowercase + Remove Stop Words\n",
    "- Lower: \"I enjoyed 'minority report'...\"\n",
    "- Stop words (`nltk.corpus.stopwords.words('english')`): Drops \"I\", \"was\", \"it\" → 36 → 19 tokens (50% smaller).  \n",
    "(telegram-style, sentiment intact).\n",
    "\n",
    "### 4. Stemming (`PorterStemmer`)\n",
    "Stemming is the process of reducing a word to its base or root form, often by removing suffixes or prefixes.\n",
    "Rule-based: Chops suffixes → stems (may not be real words).  \n",
    "- joy/joyful/joyfully → joy  \n",
    "- joyous → \"joyou\", geese → \"gees\" (OK for ML; models learn patterns).  \n",
    "Review shrinks more; good for sentiment analysis.\n",
    "\n",
    "### 5. Lemmatization (`WordNetLemmatizer`)\n",
    "Lemmatization is the process of reducing words to their base or dictionary form, considering the word’s context and its part of speech. Unlike stemming, lemmatization uses vocabulary and morphological analysis to return the base or canonical form of a word.\n",
    "Dictionary-based: Real words only, needs POS context. Uses WordNet database.  \n",
    "- geese → goose (correct)  \n",
    "- joy/joyful/joyfully/joyous → themselves (no change here).  \n",
    "Better for grammar-heavy tasks like summarization.\n",
    "\n",
    "**Stem vs Lemma:** Stem = faster/cruder; Lemma = accurate/slower.\n",
    "\n",
    "> Normalization cleans tokens perfectly for modeling!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
