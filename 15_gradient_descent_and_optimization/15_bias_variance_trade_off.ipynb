{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d83e5d",
   "metadata": {},
   "source": [
    "The bias–variance tradeoff explains how model complexity affects two different sources of error—bias and variance—and how, in “modern” over‑parameterized models trained with stochastic gradient descent, this classical picture can break down. [telnyx](https://telnyx.com/learn-ai/bias-variance-tradeoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5974166",
   "metadata": {},
   "source": [
    "### Bias and variance: intuitive meanings\n",
    "\n",
    "- **Bias**: how fundamentally *wrong* the model’s assumptions are, even with the best possible parameters.  \n",
    "  - High bias = model is too simple to capture the true pattern (underfitting). [bmc](https://www.bmc.com/blogs/bias-variance-machine-learning/)\n",
    "  - Example from the video: a model with **only an intercept** for miles‑per‑gallon (MPG) assumes “all cars have the same fuel efficiency.” Even with the best intercept, it can’t capture the relationship between MPG and weight/horsepower, so its bias is high.\n",
    "\n",
    "- **Variance**: how *sensitive* the model is to the specific training data it sees.  \n",
    "  - High variance = model changes a lot when the data changes a little (overfitting). [telnyx](https://telnyx.com/learn-ai/bias-variance-tradeoff)\n",
    "  - In the MPG example, a very wiggly high‑degree model will move dramatically if a single point is nudged, so it has high variance.\n",
    "\n",
    "You can think:\n",
    "\n",
    "- Low‑complexity models: **high bias, low variance**.  \n",
    "- High‑complexity models: **low bias, high variance**. [bmc](https://www.bmc.com/blogs/bias-variance-machine-learning/)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148a9b82",
   "metadata": {},
   "source": [
    "### MPG example: bias decreasing, variance increasing\n",
    "\n",
    "The video walks through increasing model complexity on an MPG‑style dataset:\n",
    "\n",
    "1. **Intercept‑only model**  \n",
    "   - Predicts the same MPG for every vehicle.  \n",
    "   - High bias: cannot express the clear dependency on the input variable.  \n",
    "   - Low variance: tiny changes in data barely change the constant estimate.\n",
    "\n",
    "2. **Linear model (intercept + slope)**  \n",
    "   - A straight regression line through the data.  \n",
    "   - Bias is **lower**: captures a general trend (e.g., MPG decreases as weight increases).  \n",
    "   - Variance still modest: different samples will shift the line, but not wildly.\n",
    "\n",
    "3. **Quadratic model (degree‑2 polynomial)**  \n",
    "   - Can capture a **roughly parabolic** shape in MPG vs input.  \n",
    "   - Bias decreases again: fits the visible curvature better.  \n",
    "   - Variance increases: more flexible, more sensitive to small data changes.\n",
    "\n",
    "4. **Very high‑degree polynomial (e.g., degree 25 with 25 points)**  \n",
    "   - Can fit every training point exactly → **zero training error** → essentially zero bias on the training set.  \n",
    "   - But extremely high variance: tiny changes in one point can produce a totally different, wildly oscillating curve.  \n",
    "   - This is classic overfitting: the model is “too free” and learns noise.\n",
    "\n",
    "So, as complexity increases:\n",
    "\n",
    "- **Bias** tends to go down.  \n",
    "- **Variance** tends to go up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a8aae",
   "metadata": {},
   "source": [
    "### Formal relationship: error = bias² + variance + noise\n",
    "\n",
    "If you define bias and variance mathematically, you can show:\n",
    "\n",
    "$$\n",
    "\\text{Expected loss (risk)} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible error}\n",
    "$$\n",
    "\n",
    "- **Irreducible error** is noise in the data you can’t get rid of (e.g., measurement noise). [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/ml-bias-variance-trade-off/)\n",
    "- Since all terms are non‑negative, the irreducible error is a lower bound on what any model can achieve. [en.wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n",
    "\n",
    "The **bias–variance tradeoff** is the fact that:\n",
    "\n",
    "- Making the model more complex usually **reduces bias** but **increases variance**.  \n",
    "- Making it simpler usually **increases bias** but **reduces variance**. [mlu-explain.github](https://mlu-explain.github.io/bias-variance/)\n",
    "\n",
    "Classically, if you plot:\n",
    "\n",
    "- Total error  \n",
    "- Bias²  \n",
    "- Variance  \n",
    "\n",
    "against model complexity, you get:\n",
    "\n",
    "- Bias²: decreases as complexity increases.  \n",
    "- Variance: increases as complexity increases.  \n",
    "- Total error: U-shaped, with a **sweet spot** at intermediate complexity where bias² + variance is minimized. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/ml-bias-variance-trade-off/)\n",
    "\n",
    "That sweet spot is the “just right” model in the classical regime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf6848",
   "metadata": {},
   "source": [
    "### Modern interpolating regime and double descent\n",
    "\n",
    "The video then connects this to the **modern interpolating regime** and **double descent** seen in large models:\n",
    "\n",
    "- In many neural networks and over‑parameterized models, **test error vs. complexity** does *not* simply follow the classical U‑shape.  \n",
    "- As you increase complexity:\n",
    "  1. Test error decreases (classical first descent).  \n",
    "  2. Near the point where training error hits **zero**, test error can **spike up** (overfitting).  \n",
    "  3. As you increase complexity further, test error can **drop again** and sometimes reach **even lower** values than at the classical sweet spot.  \n",
    "\n",
    "This is **double descent**: two descents in test error separated by a peak around the interpolation threshold.\n",
    "\n",
    "The cited work (Belkin et al. and Yang et al.) and the video’s discussion emphasize:\n",
    "\n",
    "- When you move far into the over‑parameterized, interpolating regime, bias can continue to **decrease**.  \n",
    "- Surprisingly, **variance can start to decrease again**, rather than just increasing forever.  \n",
    "- In some experiments, the **optimal** model (lowest test risk) is not at the classical sweet spot, but way out in the high‑capacity region, where the model is enormous.\n",
    "\n",
    "In the neural network experiment mentioned:\n",
    "\n",
    "- Bias² decreases with complexity.  \n",
    "- Variance first increases, then **drops again** as the model grows huge.  \n",
    "- Expected loss (risk) reaches its minimum in this far‑right regime, where both bias and variance are relatively small.\n",
    "\n",
    "In that setting, it almost looks like there is **no tradeoff**: bigger models can achieve **lower bias and lower variance** simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc5979",
   "metadata": {},
   "source": [
    "### Why can bigger models have low variance? Role of SGD\n",
    "\n",
    "The video links this to **stochastic gradient descent** and **implicit regularization** (discussed in earlier videos):\n",
    "\n",
    "- In over‑parameterized models (more parameters than data), there are **infinitely many** parameter settings that perfectly fit the training data (zero training error).  \n",
    "- However, training with SGD does **not** pick an arbitrary interpolating solution; it picks a particular one, influenced by:\n",
    "  - Initialization.  \n",
    "  - Learning rate schedule.  \n",
    "  - Noise from mini‑batches.  \n",
    "  - Properties of the architecture and optimization path.\n",
    "\n",
    "Empirically and in some theoretical settings, the solutions SGD gravitates toward are:\n",
    "\n",
    "- **Less wiggly** or smoother in function space.  \n",
    "- “Smaller” in some norm or capacity measure.  \n",
    "- More **implicitly regularized**, even though you did not explicitly add a penalty.\n",
    "\n",
    "As the model size grows:\n",
    "\n",
    "- The space of interpolating solutions gets larger.  \n",
    "- The **implicit bias** of SGD becomes more influential in picking a “nice” solution among many bad ones.  \n",
    "- This can **reduce variance** again, because the chosen solutions are “tamer” than what raw capacity might suggest.\n",
    "\n",
    "That is part of why:\n",
    "\n",
    "- Extremely large neural networks—trained with SGD and related methods—can generalize well, even though parameter count far exceeds number of data points.  \n",
    "- The best‑performing models in practice today are often **huge**, with hundreds of billions of parameters, not small, carefully “bias–variance balanced” models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250d787",
   "metadata": {},
   "source": [
    "### Practical takeaways\n",
    "\n",
    "From the video’s perspective:\n",
    "\n",
    "- **Classical regime (smaller models)**  \n",
    "  - Bias–variance tradeoff works as usual: pick a moderate complexity to balance underfitting and overfitting.  \n",
    "  - The sweet spot is where bias is fairly low and variance hasn’t exploded.\n",
    "\n",
    "- **Modern interpolating regime (very large models)**  \n",
    "  - Test error and even variance can improve again as you go to larger models.  \n",
    "  - In some contexts, the best model is **very large**, far beyond the interpolation threshold.  \n",
    "  - The way SGD selects one solution among many zero‑training‑error solutions is crucial to this behavior.\n",
    "\n",
    "- **In practice**  \n",
    "  - Modern ML often pushes model sizes very high, especially in neural networks.  \n",
    "  - Classical bias–variance intuition still helps, but you must be aware of double descent and implicit regularization when reasoning about very large models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
