{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba5314c",
   "metadata": {},
   "source": [
    "Stochastic gradient descent (SGD) is a way to make gradient descent practical and fast on large datasets by using only *parts* of the data to approximate the true gradient at each step, instead of the full dataset every time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f313190",
   "metadata": {},
   "source": [
    "### Why “plain” gradient descent is too slow\n",
    "\n",
    "In ordinary (batch) gradient descent for linear regression:\n",
    "\n",
    "- The loss (e.g., mean squared error) depends on **all** training examples.  \n",
    "- The gradient at each step is computed by:\n",
    "  - Making predictions for every row in the dataset.  \n",
    "  - Comparing them with the true labels.  \n",
    "  - Averaging the contribution of every point.\n",
    "\n",
    "This is fine for 244 rows (tips dataset), but:\n",
    "\n",
    "- Real problems (e.g., image classification) may have millions or billions of examples.  \n",
    "- Computing the full gradient at every step becomes extremely expensive in time and memory.\n",
    "\n",
    "So we want a cheaper way to get a *good enough* direction to move in, without scanning the whole dataset every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a628e",
   "metadata": {},
   "source": [
    "### Key idea: approximate the gradient using a subset of data\n",
    "\n",
    "Instead of computing the gradient using **all** data points, SGD uses only a **subset**:\n",
    "\n",
    "- Take a small set of indices (a *batch* or *mini‑batch*).  \n",
    "- Pretend, for this step, that only those examples exist.  \n",
    "- Compute the gradient of the loss using only those examples.  \n",
    "- Use that approximate gradient to update θ.\n",
    "\n",
    "Because you use fewer examples, each update is much cheaper, and you can update parameters more frequently.\n",
    "\n",
    "Intuitively:\n",
    "\n",
    "- Full gradient = “exact” slope, expensive.  \n",
    "- Batch or mini-batch gradient = “noisy” slope, cheap.\n",
    "\n",
    "That “noise” is actually a feature, not just a bug—it can help the algorithm escape shallow areas or local irregularities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8283a9",
   "metadata": {},
   "source": [
    "## From full-batch gradient to batch-only gradient\n",
    "\n",
    "Recall the batch gradient for 2D linear regression (θ₀, θ₁):\n",
    "\n",
    "- Model: $\\hat{y}_i = \\theta_0 x_{0,i} + \\theta_1 x_{1,i}$.  \n",
    "- Residual: $e_i = y_i - \\hat{y}_i$.  \n",
    "- Full-dataset gradient (mean over all i):\n",
    "\n",
    "  - $\\frac{\\partial L}{\\partial \\theta_0} = -\\frac{2}{n} \\sum_i e_i x_{0,i}$  \n",
    "  - $\\frac{\\partial L}{\\partial \\theta_1} = -\\frac{2}{n} \\sum_i e_i x_{1,i}$\n",
    "\n",
    "In code for the full dataset, you conceptually do:\n",
    "\n",
    "- `x0 = X[:, 0]`  \n",
    "- `x1 = X[:, 1]`  \n",
    "- `errors = y_obs - (theta0 * x0 + theta1 * x1)`  \n",
    "- compute mean over **all** rows.\n",
    "\n",
    "To get a **batch-only** gradient, you:\n",
    "\n",
    "1. Accept a list/array of row indices, e.g. `[5]` or `[0,1,2,3]`.  \n",
    "2. Use only those rows of X and y.  \n",
    "3. Compute the same formula, but averaged over this subset only.\n",
    "\n",
    "So instead of:\n",
    "\n",
    "- `x0 = X[:, 0]` (all rows)\n",
    "\n",
    "you do:\n",
    "\n",
    "- `x0 = X[indices, 0]` (selected rows only)  \n",
    "- `x1 = X[indices, 1]`  \n",
    "- `y_batch = y_obs[indices]`  \n",
    "\n",
    "Then compute the residuals and mean exactly as before, but just on that slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9235cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sympy as sy\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading tips dataset\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# Add bias column\n",
    "tips[\"bias\"] = 1.0\n",
    "\n",
    "# Feature matrix X and target y\n",
    "X = tips[[\"bias\", \"total_bill\"]]  \n",
    "y = tips[\"tip\"]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b0c9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient_batch_only(theta, batch_indices, X, y_obs):\n",
    "    \"\"\"Returns the gradient of the MSE on only the given data (via batch_indices) \n",
    "       for the given theta\"\"\"    \n",
    "    x0 = X.iloc[batch_indices, 0]\n",
    "    x1 = X.iloc[batch_indices, 1]\n",
    "    dth0 = np.mean(-2 * (y_obs[batch_indices] - theta[0] * x0 - theta[1] * x1) * x0)\n",
    "    dth1 = np.mean(-2 * (y_obs[batch_indices] - theta[0] * x0 - theta[1] * x1) * x1)\n",
    "    return np.array([dth0, dth1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3aaca81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -5.99655738, -135.22631803])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If `batch_indices` = all rows (e.g., `0` to `len(X)-1`), you recover the **full** gradient\n",
    "mse_gradient_batch_only(np.array([0, 0]), np.arange(0, len(X)), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816a449c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -9.42  , -238.2318])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If `batch_indices` = `[5]`, you get the gradient contributed by **only** row 5\n",
    "mse_gradient_batch_only(np.array([0, 0]),[5], X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14993c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.74   , -93.12005])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If `batch_indices` = `[0,1,2,3]`, you get the average gradient over just those 4 examples\n",
    "mse_gradient_batch_only(np.array([0, 0]), np.arange(0, 4), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ae7da",
   "metadata": {},
   "source": [
    "This is the building block for SGD: a gradient function that can work on arbitrary subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ebb63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient_batch_only_two_arg(theta, batch_indices):\n",
    "    \"\"\"Returns the gradient of the MSE on only the given data (via batch_indices) \n",
    "       for the given theta\"\"\"    \n",
    "    X = tips[[\"bias\", \"total_bill\"]]\n",
    "    y_obs = tips[\"tip\"]\n",
    "    return mse_gradient_batch_only(theta, batch_indices, X, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff4b6872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -6.23666667, -126.59116667])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_gradient_batch_only_two_arg(np.array([0, 0]), [5, 6, 7, 8, 15, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f1d361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3]), array([4, 5, 6]), array([7, 8, 9])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20b360",
   "metadata": {},
   "source": [
    "Randomly split indices into equal-sized batches using a “split” function:\n",
    "\n",
    "1. Start with an array of indices (e.g., for 12 points: `[0,1,2,3,4,5,6,7,8,9,10,11]`).  \n",
    "2. Randomly permute it to shuffle the order.  \n",
    "3. Split this shuffled list into a fixed number of equal parts (batches).\n",
    "\n",
    "Example with 12 data points and 3 batches:\n",
    "\n",
    "- Original indices: `[0,1,2,3,4,5,6,7,8,9,10,11]`  \n",
    "- After random permutation: `[11, 8, 10, 1, 6, 0, 5, 7, 9, 2, 4, 3]`  \n",
    "- After splitting into 3 parts:  \n",
    "  - Batch 1: `[11, 8, 10, 1]`  \n",
    "  - Batch 2: `[6, 0, 5, 7]`  \n",
    "  - Batch 3: `[9, 2, 4, 3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cadd396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2, 1, 5]), array([8, 0, 3]), array([10,  9,  6]), array([11,  7,  4])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(np.random.permutation(np.arange(12)), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc8046",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent algorithm (procedural view)\n",
    "\n",
    "Now the video ties everything together into a stochastic gradient descent loop. The high-level idea:\n",
    "\n",
    "1. Choose:\n",
    "   - Initial parameters θ (e.g., `[0, 0]`).  \n",
    "   - Learning rate α (e.g., 0.001).  \n",
    "   - Number of overall steps or epochs.  \n",
    "   - Number of batches per epoch (e.g., 4).\n",
    "\n",
    "2. For each outer iteration:\n",
    "   - Generate a **random permutation** of all data indices.  \n",
    "   - Split that permutation into `num_batches` equal parts (mini-batches).\n",
    "\n",
    "3. For each batch in this split:\n",
    "   - Compute the gradient using only this batch (using `mse_gradient_batch_only`).  \n",
    "   - Update θ using that batch gradient.  \n",
    "   - Record the new θ and current loss (using, for example, the full-data loss to monitor progress).\n",
    "\n",
    "Because every epoch re‑shuffles the order, each example ends up in different batches over time, and the individual gradient estimates vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78eae584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(theta, X, y_obs):\n",
    "    y_hat = theta[0] * X.iloc[:, 0] + theta[1] * X.iloc[:, 1]\n",
    "    return np.mean((y_hat - y_obs) ** 2)    \n",
    "\n",
    "def mse_loss_single_arg(theta):  \n",
    "    X = tips[[\"bias\", \"total_bill\"]]\n",
    "    y_obs = tips[\"tip\"]\n",
    "    return mse_loss(theta, X, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a23d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(df, initial_guess, alpha, n, num_dps, number_of_batches):\n",
    "    guesses = [initial_guess]\n",
    "    guess = initial_guess\n",
    "    losses = [mse_loss_single_arg(guess)]\n",
    "    while len(guesses) < n:\n",
    "        dp_indices = np.random.permutation(np.arange(num_dps))\n",
    "        for batch_indices in np.split(dp_indices, number_of_batches):            \n",
    "            guess = guess - alpha * df(guess, batch_indices)\n",
    "            guesses.append(guess)\n",
    "            losses.append(mse_loss_single_arg(guess))\n",
    "    return np.array(guesses), np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "564adcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses, losses = stochastic_gradient_descent(mse_gradient_batch_only_two_arg, np.array([0, 0]), 0.001, 10000, len(tips), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "704caead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.00592197, 0.13048851],\n",
       "       [0.00670676, 0.14123385],\n",
       "       ...,\n",
       "       [0.88962466, 0.10052997],\n",
       "       [0.88978149, 0.10763332],\n",
       "       [0.89001254, 0.10959086]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesses[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6320a",
   "metadata": {},
   "source": [
    "In the tips dataset:\n",
    "\n",
    "- There are 244 data points.  \n",
    "- If we pick “4 batches”, each batch has 244 / 4 = 61 points.  \n",
    "- So each pass (or “epoch”) consists of 4 gradient updates, one per batch of 61 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935fab1",
   "metadata": {},
   "source": [
    "### Behaviour of SGD in the tips example\n",
    "\n",
    "When the instructor runs this on the tips dataset:\n",
    "\n",
    "- Start: θ₀ = 0, θ₁ = 0, with a high loss.  \n",
    "- As the algorithm runs:\n",
    "  - θ₀ and θ₁ change with each batch.  \n",
    "  - Loss generally decreases over time but with fluctuations.\n",
    "\n",
    "The **patterns** they emphasize:\n",
    "\n",
    "1. **Compared to full-batch gradient descent:**\n",
    "   - Full-batch: one update per pass, smooth monotonic loss decrease (for convex problems and good α).  \n",
    "   - SGD: many small updates per pass, loss curve is jagged/noisy because each step uses only a subset and may “miss” some difficult points (e.g., outliers).\n",
    "\n",
    "2. Despite the noise:\n",
    "   - Over many steps, θ₀ and θ₁ converge close to the “true” optimum found by scikit-learn or by batch gradient descent (e.g., ≈ 0.92 offset and ≈ 10–10.5% tip).  \n",
    "   - The final values in the experiment are something like 0.89 (offset) and 0.10 (tip rate), very near the earlier results.\n",
    "\n",
    "3. Each batch gradient is an **approximation** of the true gradient:\n",
    "   - Some batches may not contain rare outliers.  \n",
    "   - This can make certain steps look “too optimistic” or “too pessimistic” about the global loss.  \n",
    "   - Over many batches and epochs, the estimates average out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a400a",
   "metadata": {},
   "source": [
    "## Why SGD is useful (and tricky)\n",
    "\n",
    "Key advantages (as hinted in the video):\n",
    "\n",
    "- **Efficiency:**  \n",
    "  - Each gradient computation uses only a small subset.  \n",
    "  - On massive datasets (e.g., millions of images), this makes training feasible.  \n",
    "- **Frequent updates:**  \n",
    "  - Parameters are updated many times per pass through the data, which can speed up practical convergence.  \n",
    "- **Escaping shallow regions:**  \n",
    "  - The noise in gradient estimates can help avoid getting “stuck” in flattish or awkward regions.\n",
    "\n",
    "But it is also:\n",
    "\n",
    "- **Noisier:**  \n",
    "  - Loss does not decrease smoothly; it bounces around.  \n",
    "- **Sensitive to hyperparameters:**  \n",
    "  - Learning rate α and batch size matter a lot.  \n",
    "  - Poor choices can cause divergence, very slow progress, or oscillations.  \n",
    "- **Harder to reason about:**  \n",
    "  - The path of θ across the loss surface isn’t as nicely behaved as full-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc94596",
   "metadata": {},
   "source": [
    "## Big-picture intuition\n",
    "\n",
    "Putting all the small details together:\n",
    "\n",
    "- **Batch gradient descent**:  \n",
    "  - Uses the exact gradient from all data.  \n",
    "  - Fewer, more expensive, smoother updates.\n",
    "\n",
    "- **Stochastic / mini-batch gradient descent** (as coded in the video):  \n",
    "  - Uses approximate gradients from *random subsets*.  \n",
    "  - More, cheaper updates; noisy but efficient.  \n",
    "  - Over time, still steers θ towards the minimum of the loss.\n",
    "\n",
    "Even though each individual step is based on an incomplete (and noisy) picture of the loss surface, the repeated use of many batches and repeated passes over randomly shuffled data makes the overall process converge in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
