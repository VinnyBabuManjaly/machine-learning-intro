{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820d18db",
   "metadata": {},
   "source": [
    "This video connects three ideas: classical bias–variance, the modern “double descent” phenomenon, and the surprising way stochastic gradient descent (SGD) behaves like a built‑in regularizer (“implicit regularization”), especially in over‑parameterized models.\n",
    "\n",
    "***\n",
    "\n",
    "## Classical view: complexity vs train/test error\n",
    "\n",
    "In the classical bias–variance picture:\n",
    "\n",
    "- As **model complexity** goes up (more parameters, more flexible shape), **training error** typically goes down steadily.  \n",
    "- **Test/validation error** usually:\n",
    "  - Decreases at first (less bias, better fit).  \n",
    "  - Reaches a “sweet spot.”  \n",
    "  - Then **increases** as the model starts to overfit (capturing noise).\n",
    "\n",
    "The Belkin et al. 2019 paper reframes this in terms of:\n",
    "\n",
    "- **Risk** (expected error) instead of test error.  \n",
    "- **Capacity** instead of complexity.\n",
    "\n",
    "Conceptually it is the same shape: a U‑shaped test error curve with a single optimal complexity.\n",
    "\n",
    "***\n",
    "\n",
    "## Modern phenomenon: double descent\n",
    "\n",
    "In the late 2010s, people noticed that in many real models (especially large neural networks), the test error vs. complexity story is more complicated:\n",
    "\n",
    "- If you keep increasing model capacity far beyond the point where training error first hits **zero**, something surprising can happen.  \n",
    "- Near the point of exactly fitting the training data (interpolation), the test error often **spikes** upward (overfitting).  \n",
    "- But if you keep increasing capacity further:\n",
    "  - Test error can **decrease again**, sometimes to a level even **lower** than the original “sweet spot.”  \n",
    "  - This gives a **double descent** shape:\n",
    "    - First descent (classical regime).  \n",
    "    - Spike near interpolation (zero training error).  \n",
    "    - Second descent in an over‑parameterized “modern” regime.\n",
    "\n",
    "Key terms:\n",
    "\n",
    "- **Interpolating regime**: the part of the curve where the model achieves **zero training error** (fits every training point exactly).  \n",
    "- **Modern interpolating regime**: extremely over‑parameterized models (like big neural nets) that interpolate but still generalize well.\n",
    "\n",
    "So the simple “more complexity ⇒ worse generalization after a point” story is not always correct in practice.\n",
    "\n",
    "***\n",
    "\n",
    "## Witten’s spline example: 4, 6, 20, and 36 degrees\n",
    "\n",
    "The video uses an example from Daniela Witten (via a series of tweets) to make double descent concrete in a simpler setting (spline regression, conceptually similar to high‑degree polynomial regression) on 20 data points:\n",
    "\n",
    "1. **Degree‑4 model**  \n",
    "   - Blue curve fits the data reasonably well.  \n",
    "   - Still some bias, but it captures the broad shape of the underlying “true” black curve.\n",
    "\n",
    "2. **Degree‑6 model**  \n",
    "   - Blue curve fits a bit better (more flexibility).  \n",
    "   - Training error decreases; test error also often improves compared to degree 4.\n",
    "\n",
    "3. **Degree‑20 model**  \n",
    "   - Now the model has enough flexibility to achieve **zero training error** on the 20 points.  \n",
    "   - The curve wiggles wildly between points, “chasing” noise.  \n",
    "   - Training error is zero, but test/generalization is clearly bad (overfitting).  \n",
    "   - This matches the classical picture near the interpolation point.\n",
    "\n",
    "4. **Degree‑36 model**  \n",
    "   - There are now more parameters (36) than data points (20).  \n",
    "   - In linear algebra terms:\n",
    "     - 20 data constraints.  \n",
    "     - 36 parameters.  \n",
    "     - There are **infinitely many** parameter settings that perfectly fit the training data (zero training error).  \n",
    "   - Among these infinitely many interpolating solutions, the particular one found by the training algorithm is actually **less wiggly** and somewhat better in test behavior than the degree‑20 solution.  \n",
    "   - This illustrates the **second descent**: moving deeper into the over‑parameterized regime reduces test error again.\n",
    "\n",
    "Plotting test and training error vs. degree:\n",
    "\n",
    "- Training error decreases to zero and stays there.  \n",
    "- Test error:\n",
    "  - Decreases at first (degrees 4–6).  \n",
    "  - Rises up near degree 20 (classic overfit).  \n",
    "  - Then decreases again at degree 36 (double descent).  \n",
    "\n",
    "In this specific toy example, test error eventually rises again for even higher degrees; in many neural network cases, the second descent can continue without a visible “second spike.”\n",
    "\n",
    "***\n",
    "\n",
    "## Over‑parameterization and infinite interpolating solutions\n",
    "\n",
    "Once a model has **more parameters than data points**:\n",
    "\n",
    "- It can fit the training data in many different ways.  \n",
    "- There are **infinitely many** parameter vectors that all give **zero training error**.  \n",
    "\n",
    "For example:\n",
    "\n",
    "- With \\(n\\) points and \\(n\\) parameters, there is usually one unique interpolating solution.  \n",
    "- With \\(n\\) points and \\(p > n\\) parameters, there is an entire family (a high‑dimensional set) of interpolating solutions.\n",
    "\n",
    "So if we just say, “train until training error is zero,” that does not tell us:\n",
    "\n",
    "- Which particular interpolating model we will end up with.  \n",
    "- How “wiggly” or “smooth” that model will be.  \n",
    "- How well it will generalize.\n",
    "\n",
    "The key question becomes:\n",
    "\n",
    "> Among all zero‑training‑error models, **which one** does the training procedure (like SGD) actually pick?\n",
    "\n",
    "***\n",
    "\n",
    "## Where stochastic gradient descent comes in\n",
    "\n",
    "Stochastic gradient descent (and mini‑batch versions) is not just “any” optimizer: it has its own built‑in preferences.\n",
    "\n",
    "- SGD is a **specific procedure** for walking through parameter space.  \n",
    "- In over‑parameterized settings with infinitely many perfect fits, SGD tends to select **particular** kinds of solutions.  \n",
    "- Empirically and theoretically (in certain settings), these solutions often behave as if they are **regularized**, even if you did not add any explicit regularization term (like L2, dropout, etc.) in the loss function.\n",
    "\n",
    "This effect is called **implicit regularization** (or **implicit bias**) of SGD.\n",
    "\n",
    "***\n",
    "\n",
    "## Implicit regularization: “less wiggly” solutions without explicit penalties\n",
    "\n",
    "Explicit regularization is something you put directly into the objective, e.g.:\n",
    "\n",
    "- L2 penalty: minimize \\( \\text{loss}(\\theta) + \\lambda \\|\\theta\\|^2 \\).  \n",
    "- Smoothing penalties in splines or polynomials.  \n",
    "\n",
    "Implicit regularization means:\n",
    "\n",
    "- There is **no explicit penalty term** in the loss.  \n",
    "- Yet, the **training algorithm** (SGD) itself tends to favor solutions that look like they had been regularized.\n",
    "\n",
    "In the Witten example:\n",
    "\n",
    "- Both degree‑20 and degree‑36 models can achieve zero training error.  \n",
    "- But when trained with SGD (or similar gradient methods), the degree‑36 model that emerges from the optimization is **less wiggly** and behaves more smoothly around the data.  \n",
    "- Intuitively: even though it has higher nominal complexity, the *particular* solution SGD lands on is effectively more constrained and smoother—“as if” we had applied a stronger regularization.\n",
    "\n",
    "The lecturer summarizes this as:\n",
    "\n",
    "- “Stochastic gradient descent results in a model which is implicitly regularized, yielding less wiggly behavior.”  \n",
    "- “Counterintuitively, because the 36‑degree model has a greater implicit regularization strength, it is actually less free than a 20‑degree model.”\n",
    "\n",
    "That is, the training dynamics under SGD push the very high‑capacity model into a special subset of all possible interpolating solutions—those that are smoother, simpler in some hidden sense, or of smaller “norm,” depending on the setting.\n",
    "\n",
    "***\n",
    "\n",
    "## Why implicit regularization leads to double descent\n",
    "\n",
    "Putting the pieces together:\n",
    "\n",
    "1. **Moderate complexity**  \n",
    "   - Classic bias–variance regime.  \n",
    "   - Increasing capacity reduces bias, then overfitting starts, test error goes up.\n",
    "\n",
    "2. **Approaching interpolation (training error → 0)**  \n",
    "   - Test error spikes because the model is just flexible enough to chase noise.  \n",
    "   - There is essentially only one interpolating solution; it is typically quite wiggly.\n",
    "\n",
    "3. **Further over‑parameterization (many more parameters than data points)**  \n",
    "   - There are infinitely many interpolating solutions.  \n",
    "   - SGD doesn’t explore all of them; it has a built‑in bias toward certain smoother, more regularized solutions.  \n",
    "   - As a result, the chosen zero‑training‑error model can generalize **better** than the unique interpolator at the interpolation threshold.  \n",
    "   - Test error can drop again → **double descent**.\n",
    "\n",
    "As model capacity grows even more:\n",
    "\n",
    "- The strength or effect of this implicit regularization can increase.  \n",
    "- In some contexts (e.g., deep nets), test error after the second descent is **lower** than anywhere in the classical regime, which explains why huge models can perform very well in practice despite being capable of massive overfitting.\n",
    "\n",
    "***\n",
    "\n",
    "## Active research and pointers\n",
    "\n",
    "The instructor stresses that:\n",
    "\n",
    "- The detailed theory of implicit regularization and double descent is still an **active research area**.  \n",
    "- Many of the deeper mathematical explanations are beyond the course (and in some cases still not fully understood by researchers themselves).  \n",
    "- They mention:\n",
    "  - Belkin et al. 2019, *“Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off”*.  \n",
    "  - Barrett 2021, *“Implicit Gradient Regularization”*, which studies how gradient-based methods (like SGD) themselves act as a kind of regularizer.\n",
    "\n",
    "The main takeaway for you at this stage is **intuition**, not proofs:\n",
    "\n",
    "- Over‑parameterized models have many perfect‑fit solutions.  \n",
    "- SGD tends to pick particular, implicitly regularized ones that can generalize surprisingly well.  \n",
    "- This selection mechanism is a key piece of why we see double descent and why very large models often perform better than smaller ones, contrary to the classical picture.\n",
    "\n",
    "If you’d like, next I can give a small toy linear‑regression example (no code, just concept) to illustrate how “implicit bias toward minimum‑norm solutions” can arise from gradient descent in a simple over‑parameterized linear system."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
