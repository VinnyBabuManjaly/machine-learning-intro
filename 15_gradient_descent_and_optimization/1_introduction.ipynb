{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afda47ae",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative optimization algorithm that moves parameters in the direction of the negative gradient of a loss function to find (typically local) minima, and in ML it is used to learn model weights that minimize a chosen cost. Batch, stochastic, and mini-batch gradient descent differ mainly in how much data they use per parameter update, which affects speed, noise, and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d993643",
   "metadata": {},
   "source": [
    "### Core idea of gradient descent\n",
    "\n",
    "- Gradient descent updates parameters $\\theta$ by $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta)$, where $\\eta$ is the learning rate and $J$ is the cost function.\n",
    "- Following the negative gradient corresponds to moving in the direction of steepest decrease of the cost, so repeated updates aim to reach a minimum of $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6dd59c",
   "metadata": {},
   "source": [
    "### Batch (vanilla) gradient descent\n",
    "\n",
    "- Batch gradient descent computes the gradient using the entire training dataset before performing a single update, and one full pass over the data is an epoch.\n",
    "- This yields a stable, low-variance gradient estimate but can be slow and memory‑intensive for large datasets because each update waits for all examples to be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b42e42",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent (SGD)\n",
    "\n",
    "- SGD updates parameters after each individual training example, computing the gradient using only that sample.\n",
    "- This makes updates very frequent and often faster per update than batch descent, but the gradient estimate is noisy, causing the loss to fluctuate instead of decreasing smoothly and sometimes helping escape shallow local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c9999",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "- Mini-batch gradient descent splits the training data into small batches (e.g., 32, 64, 128 examples) and updates parameters for each batch.\n",
    "- It balances efficiency and noise: gradients are less noisy than pure SGD and updates are more frequent and memory‑efficient than full‑batch, which is why this is the default choice in most modern deep learning frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8190d1",
   "metadata": {},
   "source": [
    "### Practical implications and usage\n",
    "\n",
    "- Mini-batch methods exploit hardware like GPUs efficiently through vectorized operations over batches, significantly speeding up training.\n",
    "- In practice, gradient descent is often combined with enhancements such as momentum, learning-rate schedules, and adaptive methods (Adam, RMSProp) to improve convergence behavior and robustness."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
