{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbcf4af",
   "metadata": {},
   "source": [
    "An activity that uses `SGDRegressor` on a built‑in dataset and highlights why **scaling your features is crucial** for gradient‑descent‑based methods. [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35579106",
   "metadata": {},
   "source": [
    "### Goal of the activity\n",
    "\n",
    "- Use `SGDRegressor` to model a real regression problem.  \n",
    "- Compare performance **with and without** feature scaling.  \n",
    "- Observe how scaling affects convergence and model quality.\n",
    "\n",
    "We’ll use the **Diabetes** regression dataset built into scikit‑learn. [codecademy](https://www.codecademy.com/article/linear-regression-with-scikit-learn-a-step-by-step-guide-using-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf99812",
   "metadata": {},
   "source": [
    "### Step 1: Load the dataset and do a basic train/test split\n",
    "\n",
    "Note: `load_diabetes()` returns features that are already scaled in a particular way, but for this activity you still see the *effect* of proper standardization because `SGDRegressor` is very sensitive to scale. [scikit-learn](https://scikit-learn.org/stable/modules/sgd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e69708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "# Load diabetes dataset as numpy arrays\n",
    "diabetes = load_diabetes()           # built-in dataset\n",
    "X = diabetes.data                    # shape (442, 10), already standardized-ish but we will treat it as raw\n",
    "y = diabetes.target                  # disease progression measure\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9710919a",
   "metadata": {},
   "source": [
    "## Step 2: Fit `SGDRegressor` **without** additional scaling\n",
    "\n",
    "What to look for:\n",
    "\n",
    "- There might be **unstable or mediocre performance** (e.g., poor R², relatively large MSE).  \n",
    "- Coefficients may be quite noisy or show signs the optimizer struggled to converge nicely, depending on random state and learning rate. [sdsawtelle.github](https://sdsawtelle.github.io/blog/output/week2-andrew-ng-machine-learning-with-python.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0444daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without extra scaling:\n",
      "  MSE: 2867.9671711478036\n",
      "  R^2: 0.4586853477817451\n",
      "  Coefficients: [  48.54964032 -154.7957172   447.94627464  295.81644927  -41.55911569\n",
      "  -87.92352684 -204.32972962  145.29889217  337.15113445  135.06416597]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "sgd_plain = SGDRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    max_iter=5000,\n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "sgd_plain.fit(X_train, y_train)\n",
    "\n",
    "y_pred_plain = sgd_plain.predict(X_test)\n",
    "mse_plain = mean_squared_error(y_test, y_pred_plain)\n",
    "r2_plain = r2_score(y_test, y_pred_plain)\n",
    "\n",
    "print(\"Without extra scaling:\")\n",
    "print(\"  MSE:\", mse_plain)\n",
    "print(\"  R^2:\", r2_plain)\n",
    "print(\"  Coefficients:\", sgd_plain.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3600bc",
   "metadata": {},
   "source": [
    "### Step 3: Fit `SGDRegressor` **with** proper scaling via a pipeline\n",
    "\n",
    "The scikit‑learn docs emphasize: “Always scale the input. The most convenient way is to use a pipeline.” [scikit-learn](https://scikit-learn.org/1.0/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
    "\n",
    "What to look for:\n",
    "- **Better R²** and generally **lower MSE** compared with the unscaled version.  \n",
    "- Much more stable training across different runs and hyperparameters. [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "823947ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With StandardScaler in pipeline:\n",
      "  MSE: 2883.720046438128\n",
      "  R^2: 0.4557120702996995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sgd_scaled = make_pipeline(\n",
    "    StandardScaler(),              # standardize features: mean 0, variance 1\n",
    "    SGDRegressor(\n",
    "        loss=\"squared_error\",\n",
    "        max_iter=5000,\n",
    "        tol=1e-3,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "sgd_scaled.fit(X_train, y_train)\n",
    "\n",
    "y_pred_scaled = sgd_scaled.predict(X_test)\n",
    "mse_scaled = mean_squared_error(y_test, y_pred_scaled)\n",
    "r2_scaled = r2_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(\"\\nWith StandardScaler in pipeline:\")\n",
    "print(\"  MSE:\", mse_scaled)\n",
    "print(\"  R^2:\", r2_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3939031",
   "metadata": {},
   "source": [
    "### Step 4: Discussion prompts for learning\n",
    "\n",
    "Use these questions to reflect (or as written questions in a worksheet):\n",
    "\n",
    "1. **Why does scaling matter for SGD?**  \n",
    "   - In gradient descent, the step size in each direction depends on both the gradient and the learning rate. When features have very different scales, one feature can dominate the gradient, making it hard to choose a single learning rate that works well for all parameters. Scaling puts features on similar numerical ranges, so the optimizer progresses more evenly in every dimension. [scikit-learn](https://scikit-learn.org/stable/modules/sgd.html)\n",
    "\n",
    "2. **What did you observe about performance before and after scaling?**  \n",
    "   - Compare the MSE and R² values. Did scaling improve test performance? Did it affect convergence warnings or stability?\n",
    "\n",
    "3. **Why use a `Pipeline` rather than calling `StandardScaler` manually?**  \n",
    "   - A pipeline ensures that exactly the **same scaling learned from the training data** is applied to the test data, preventing data leakage and making your workflow more robust and concise. [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
    "\n",
    "4. **How is this tied to stochastic gradient descent concepts?**  \n",
    "   - `SGDRegressor` is a practical implementation of SGD for linear regression. \n",
    "     - Uses gradient steps on mini‑batches or single samples.  \n",
    "     - Is sensitive to the scale of features.  \n",
    "     - Benefits strongly from preprocessing like standardization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
