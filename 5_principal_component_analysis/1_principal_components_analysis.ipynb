{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "689450fd",
   "metadata": {},
   "source": [
    "## Introduction to Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a foundational technique in data analysis and machine learning for reducing the number of variables (features) in a dataset while retaining most of its essential information. It transforms the original data into new uncorrelated variables called principal components. These components represent the directions in which the data varies the most, allowing you to simplify high-dimensional data for easier analysis and modeling.\n",
    "\n",
    "PCA is especially valuable when you're dealing with datasets with many features (columns), making it easier to visualize, analyze, and input into machine learning models—saving time and reducing the risk of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc09107",
   "metadata": {},
   "source": [
    "### Why Use PCA?\n",
    "\n",
    "- Dimensionality Reduction: Lessens the number of input variables without losing much information.\n",
    "- Pattern Discovery: Reveals the main patterns and relationships in the data.\n",
    "- Noise Reduction: Helps remove less important variations (noise).\n",
    "- Preprocessing: Often used before clustering or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85907a",
   "metadata": {},
   "source": [
    "### How PCA Works (Step-by-Step)\n",
    "Let's break down the main steps so you see how PCA transforms your data:\n",
    "\n",
    "#### Step 1: Standardize Your Data\n",
    "Variables may have different units or scales. For fairness, all features are converted to have **mean 0** and **standard deviation 1** (called standardizing or z-scoring):\n",
    "\n",
    "$$Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Where $X$ is a value, $\\mu$ the mean, and $\\sigma$ the standard deviation of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd213789",
   "metadata": {},
   "source": [
    "### Step 2: Compute the Covariance Matrix\n",
    "This matrix shows how much different features vary together (covariance). It's the mathematical foundation for finding principal components. Strong covariances point to redundant information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561d2bf",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Calculate Eigenvectors and Eigenvalues\n",
    "* **Eigenvectors** of the covariance matrix show the directions (principal axes) where variance is maximized (think of finding new axes to “see” most spread).\n",
    "* **Eigenvalues** show how much variance (information) each direction captures. The higher the value, the more important that direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b29e7",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: Sort and Select Top Components\n",
    "1.  Sort principal components by eigenvalues, descending.\n",
    "2.  Decide how many components to keep (the first 2, 3, etc.), based on how much total variance you want to retain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945edbc8",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Project Data onto Principal Components\n",
    "Your data is now represented in fewer dimensions by projecting it onto the new axes (principal components) you selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ee890",
   "metadata": {},
   "source": [
    "\n",
    "Mnemonic: **“Standardize → Covariance → Eigenvectors/values → Sort → Project”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bce9e21",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "SVD is a mathematical method to break a data matrix $A$ into three matrices:\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "* $U$: Describes relationships among data points (rows)\n",
    "* $\\Sigma$: Diagonal matrix with singular values (importance)\n",
    "* $V^T$: Describes relationships among features (columns)\n",
    "\n",
    "SVD is used for data compression, noise reduction, and is a powerful way to mathematically implement PCA. In fact, most PCA implementations use SVD “under the hood!”\n",
    "\n",
    "SVD can be seen as the machinery enabling PCA. The principal components that PCA discovers are essentially derived from SVD of your standardized data matrix.\n",
    "\n",
    "When to Use PCA or SVD Directly?\n",
    "* For numeric stability, SVD is used to compute PCA in practice.\n",
    "* If you center the data (subtract mean) before SVD, the principal directions obtained by SVD and PCA match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7605f81",
   "metadata": {},
   "source": [
    "### Geometric and Practical Intuition\n",
    "* Visualize PCA as finding new, rotated axes that capture the largest spread (variation) of your data.\n",
    "* The first principal component is the line where projections of points are most spread out (direction of greatest variance).\n",
    "* Each next component is perpendicular to the previous, capturing the next largest variance.\n",
    "* Dimensionality reduction means keeping only the top $k$ principal components.\n",
    "\n",
    "#### Example: Why Standardization and PCA Matter\n",
    "Suppose you have people’s age (0–100) and income (in thousands, 0–200). If you skipped standardization, income’s wide range would dominate principal components, hiding important age patterns. After scaling, both features contribute fairly to PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffd628",
   "metadata": {},
   "source": [
    "### Curse of dimensionality\n",
    "The curse of dimensionality is a key challenge in machine learning and data analysis that arises when working with datasets containing a large number of features (dimensions).\n",
    "\n",
    "The curse of dimensionality is why PCA and other dimensionality reduction approaches are so important: they help simplify complex data and make analysis and modeling more practical and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ff03d",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "* PCA is a method to reduce variables and find patterns in complex data.\n",
    "* It works by discovering new axes (principal components) aligned to the strongest patterns in the data.\n",
    "* Standardization is often required to ensure each feature contributes fairly.\n",
    "* SVD is the main math engine for PCA in real-world applications.\n",
    "* Choosing how many components to keep is a balance between simplicity and information retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources: \n",
    "# [1](https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-pca/)\n",
    "# [2](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)\n",
    "# [3](https://www.ibm.com/think/topics/principal-component-analysis)\n",
    "# [4](https://stackoverflow.com/questions/9590114/importance-of-pca-or-svd-in-machine-learning)\n",
    "# [5](https://www.geeksforgeeks.org/machine-learning/singular-value-decomposition-svd/)\n",
    "# [6](https://www.reddit.com/r/learnmachinelearning/comments/s66d63/what_is_singular_value_decomposition_svd_a/)\n",
    "# [7](https://towardsdatascience.com/singular-value-decomposition-vs-eigendecomposition-for-dimensionality-reduction-fc0d9ac24a8e/)\n",
    "# [8](https://www.reddit.com/r/MachineLearning/comments/4dkxm3/what_is_better_pca_or_svd/)\n",
    "# [9](https://www.youtube.com/watch?v=gXbThCXjZFM)\n",
    "# [10](https://www.reddit.com/r/statistics/comments/2yp3tl/can_someone_please_explain_principal_component/)\n",
    "# [11](https://zilliz.com/glossary/curse-of-dimensionality-in-machine-learning)\n",
    "# [12](https://telnyx.com/learn-ai/curse-of-dimensionality)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
