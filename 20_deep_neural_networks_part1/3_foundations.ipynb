{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b4f0ffc",
   "metadata": {},
   "source": [
    "### Starting Point: Linear Regression\n",
    "\n",
    "Think back to linear regression, the simplest predictive model.\n",
    "\n",
    "- You have inputs (features) like $x_1, x_2, ..., x_D$.  \n",
    "- You have an output (target) $y$.  \n",
    "- The goal is to learn a function $h(x)$ that predicts $y$ for new input values.  \n",
    "\n",
    "In math form, linear regression says:  \n",
    "\n",
    "$$\n",
    "h(x) = A^T \\phi(x) + b\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $A$ = vector of weights (slopes)  \n",
    "- $b$ = bias (intercept)  \n",
    "- $\\phi(x)$ = feature transformations of the inputs (for instance, squaring or taking logs to bend a straight line into a curve)\n",
    "\n",
    "The computer’s job during training is to find $A$ and $b$ that make predictions closest to the real data by minimizing a loss function, usually the sum of squared errors between predictions and true values.  \n",
    "This setup is a convex problem - imagine a smooth bowl-shaped surface; gradient descent slides down to the bottom easily.\n",
    "\n",
    "\n",
    "\n",
    "### Turning a Linear Model into a Neural Network\n",
    "\n",
    "A neural network is just a generalization of linear regression with a few smart upgrades.\n",
    "\n",
    "#### 1. Using Activation Functions (Nonlinearities)\n",
    "\n",
    "Each “feature function” $\\phi$ can be replaced by an activation function, which introduces non‑linearity.  \n",
    "That’s what allows neural networks to model complex, curved, and layered relationships.\n",
    "\n",
    "Common examples:\n",
    "- Sigmoid: squashes input into values between 0 and 1 - like a smooth on/off switch.  \n",
    "- Tanh: similar but outputs between −1 and 1, helpful for centered data.  \n",
    "- ReLU (Rectified Linear Unit): passes positive values unchanged and zeros out negatives - simple, fast, and the most widely used today.\n",
    "\n",
    "Activations are like gates that decide whether certain signals pass through based on their strength.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Adding Weights Everywhere\n",
    "\n",
    "In linear regression, weights only appear on the output side.  \n",
    "In neural networks, every connection between layers has its own set of weights ($A_1, A_2, \\dots, A_L$) and biases ($b_1, b_2, \\dots, b_L$).  \n",
    "\n",
    "This makes the model far more flexible - each layer can transform the data in a unique way.\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Stacking Layers (Depth)\n",
    "\n",
    "Instead of one big transformation, neural networks apply many transformations in sequence - layer by layer.\n",
    "\n",
    "- Each layer takes the outputs of the previous layer and applies its weights, bias, and activation function.  \n",
    "- The outputs of one layer become the inputs to the next.\n",
    "\n",
    "If we have $L$ layers:\n",
    "- $c_1$ = output of layer 1  \n",
    "- $c_2$ = output of layer 2 = activation of ($A_2 c_1 + b_2$)  \n",
    "- … and so on, until  \n",
    "- the final output layer produces $h(x)$, the model’s prediction.\n",
    "\n",
    "Layers in the middle are called hidden layers because we don’t directly observe what they compute - they capture internal representations.\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Flexible Output Layers\n",
    "\n",
    "The last layer determines what kind of problem you’re solving:\n",
    "- Regression: no activation on the output (continuous values).  \n",
    "- Binary classification: add a sigmoid to output probabilities between 0 and 1.  \n",
    "- Multiclass classification: use a softmax function so outputs sum to 1 across classes.\n",
    "\n",
    "So by swapping the output layer, the same architecture can handle very different tasks.\n",
    "\n",
    "\n",
    "\n",
    "### Training the Network\n",
    "\n",
    "Just like in linear regression, training means finding all the weights and biases ($A$’s and $b$’s) that minimize the loss.  \n",
    "The big difference is that now the model is nonlinear and nested - one layer’s output feeds into another - so the loss surface is no longer a simple bowl.  \n",
    "\n",
    "Gradient descent (specifically, backpropagation) still works, but training is trickier:\n",
    "- Many local minima, not just one global bottom.\n",
    "- More parameters mean more data and computation are needed.\n",
    "\n",
    "Still, this complexity is what makes neural networks capable of modeling images, language, and other rich data - far beyond what simple regression can do.\n",
    "\n",
    "\n",
    "\n",
    "Analogy:  \n",
    "If linear regression is like fitting one straight beam between two points, a neural network is like building an entire bridge of interlinked beams - each section correcting and shaping the last - allowing it to adapt to terrains (data) of any shape.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
