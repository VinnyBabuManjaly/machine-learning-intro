{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e08d093",
   "metadata": {},
   "source": [
    "Keras is a high-level interface for building and training neural networks in Python, built on top of TensorFlow. It lets you define complex models with concise, readable code.\n",
    "\n",
    "\n",
    "\n",
    "## What Keras Provides\n",
    "\n",
    "Keras offers ready‑made components you can mix and match to build neural networks:\n",
    "\n",
    "- Layers: Dense (fully connected), convolutional, recurrent, etc.  \n",
    "\n",
    "- Activation functions: ReLU, sigmoid, tanh, softmax, and more.  \n",
    "\n",
    "- Optimizers: Different variants of stochastic gradient descent such as RMSprop, Adam, SGD, etc.  \n",
    "\n",
    "- Loss (cost) functions: Binary cross‑entropy, categorical cross‑entropy, mean squared error, etc.  \n",
    "\n",
    "- Regularization and initialization: Ways to control overfitting and initialize weights.\n",
    "\n",
    "All of these are modular pieces: you choose which layers, activations, optimizer, and loss you want, and combine them in Python code to define a model.\n",
    "\n",
    "\n",
    "\n",
    "## Defining a Neural Network in Keras\n",
    "\n",
    "To build a neural network, you typically:\n",
    "\n",
    "1. Specify the architecture (layers):  \n",
    "   For example, a model with three hidden layers using fully connected (Dense) layers:\n",
    "   - First hidden layer: 3 neurons  \n",
    "   - Second hidden layer: 4 neurons  \n",
    "   - Third hidden layer: 2 neurons  \n",
    "   - Output layer: 1 neuron (for a binary probability)\n",
    "\n",
    "   “Dense” means every neuron in one layer connects to every neuron in the next layer.\n",
    "\n",
    "2. Choose activation functions:\n",
    "   - Hidden layers often use ReLU because it trains efficiently and works well with gradient descent.  \n",
    "   - The output layer for a binary classification uses sigmoid so the output is a probability between 0 and 1.\n",
    "\n",
    "At this point the model structure is defined, but the weights are not yet trained.\n",
    "\n",
    "\n",
    "\n",
    "## Compiling the Model\n",
    "\n",
    "Before training, you “compile” the model by specifying how it should learn:\n",
    "\n",
    "- Optimizer: e.g., RMSprop, a popular variant of stochastic gradient descent. It controls how weights are updated using gradients.  \n",
    "\n",
    "- Loss function: For binary classification, binary cross‑entropy is used; it measures how far predicted probabilities are from the true 0/1 labels.  \n",
    "\n",
    "- Metrics: Commonly accuracy, so you can track how often predictions match labels during training.\n",
    "\n",
    "Compiling sets up the training configuration but does not yet adjust any weights.\n",
    "\n",
    "\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "Training is done by calling a fit‑like method with:\n",
    "\n",
    "- Input data (x) and labels (y).  \n",
    "- Epochs: How many passes over the entire dataset (e.g., 5 epochs).  \n",
    "- Batch size: How many samples to use per weight‑update step (e.g., 8).\n",
    "\n",
    "During training:\n",
    "\n",
    "- The optimizer performs repeated stochastic gradient descent steps.  \n",
    "- The loss typically decreases over epochs, indicating the model is fitting the data better.  \n",
    "- Accuracy often improves, though it can bounce around even as loss goes down.\n",
    "\n",
    "More complex models (more layers/neurons) often need more epochs to converge.\n",
    "\n",
    "\n",
    "\n",
    "## Model Complexity and Decision Boundaries\n",
    "\n",
    "Using a small model (few neurons per layer) can lead to:\n",
    "\n",
    "- Simpler decision boundaries.  \n",
    "- Limited ability to fit complex patterns, resulting in moderate accuracy.\n",
    "\n",
    "Using a larger model (more neurons per layer):\n",
    "\n",
    "- Allows more complex decision boundaries that better fit intricate data.  \n",
    "- Can achieve very high or even 100% accuracy on suitable tasks.  \n",
    "- Shows loss decreasing to very small values, with accuracy approaching 1.0.\n",
    "\n",
    "Keras makes it straightforward to scale from simple to very complex architectures just by changing the layer definitions in code.\n",
    "\n",
    "\n",
    "\n",
    "## Practical Notes Mentioned\n",
    "\n",
    "- When running in an environment like Google Colab, enabling a GPU hardware accelerator can significantly speed up training.  \n",
    "\n",
    "- Keras models defined in Python avoid separate configuration files; the entire architecture and training setup live in code."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
