{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b612a424",
   "metadata": {},
   "source": [
    "## Decision tree hyperparameters\n",
    "\n",
    "Decision tree hyperparameters control how the tree grows and how complex it becomes, which directly impacts overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4ddc1",
   "metadata": {},
   "source": [
    "#### Tree parts\n",
    "\n",
    "- **Root node**: Starting node with all training data.  \n",
    "- **Internal node**: Non-final node that can still split.  \n",
    "- **Leaf node**: Final node; no more splits, outputs prediction.  \n",
    "- **Branch**: Connection between nodes representing an answer (e.g., “yes” / “no”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820ca22",
   "metadata": {},
   "source": [
    "#### Key hyperparameters (simple view)\n",
    "\n",
    "- **criterion**: How to score splits.  \n",
    "  - `\"gini\"` (default) or `\"entropy\"` to measure how mixed labels are in a node.\n",
    "\n",
    "- **splitter**: How to search for the split.  \n",
    "  - `\"best\"` (try all and pick best) or `\"random\"` (consider random candidates).\n",
    "\n",
    "- **max_features**: How many features to consider at each split.  \n",
    "  - Integer → that many features.  \n",
    "  - Float → that fraction of all features.  \n",
    "  - `\"sqrt\"`, `\"log2\"` → functions of number of features.  \n",
    "  - `None` → use all features (default).\n",
    "\n",
    "- **max_depth**: Maximum depth (levels) of the tree.  \n",
    "  - Integer → hard depth limit.  \n",
    "  - `None` → grow until pure or limited by other params (default).  \n",
    "  - Smaller → simpler tree, less overfitting.\n",
    "\n",
    "- **max_leaf_nodes**: Maximum number of leaves.  \n",
    "  - Integer → cap on leaf count.  \n",
    "  - `None` → no cap (default).\n",
    "\n",
    "- **min_samples_split**: Minimum samples in a node to allow a split.  \n",
    "  - Integer → minimum count.  \n",
    "  - Float → minimum fraction of total data.  \n",
    "  - Larger → fewer splits, less overfitting.\n",
    "\n",
    "- **min_samples_leaf**: Minimum samples required in each leaf.  \n",
    "  - Integer → minimum count in every leaf.  \n",
    "  - Float → minimum fraction.  \n",
    "  - Larger → smoother, less overfitted tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72a14d",
   "metadata": {},
   "source": [
    "#### Why these matter\n",
    "\n",
    "- Making the tree **too flexible** (deep, many leaves, tiny leaves) → high risk of overfitting.  \n",
    "- Adding limits (depth, leaf size, min samples) makes the model more **robust**, so it generalizes better to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0331c1",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "[1](https://scikit-learn.org/stable/modules/tree.html)\n",
    "[2](https://inside-machinelearning.com/en/decision-tree-and-hyperparameters/)\n",
    "[3](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html)\n",
    "[4](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "[5](https://inria.github.io/scikit-learn-mooc/python_scripts/trees_hyperparameters.html)\n",
    "[6](https://businessanalyticsinstitute.com/implementing-decision-trees-with-scikit-learn/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
