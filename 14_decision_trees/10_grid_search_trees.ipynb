{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcdca1c",
   "metadata": {},
   "source": [
    "## Grid Search Decision Trees\n",
    "\n",
    "Hyperparameter tuning is critical for decision trees because it controls model complexity, preventing overfitting and underfitting and managing the bias–variance trade‑off. \n",
    "\n",
    "Key hyperparameters include,\n",
    "- maximum depth\n",
    "- minimum samples per leaf/split, \n",
    "- the split criterion (Gini or entropy); \n",
    "\n",
    "these shape how deep the tree grows and how finely it fits the data. \n",
    "\n",
    "To choose good values, different search strategies are used with cross‑validation: \n",
    "\n",
    "- GridSearchCV exhaustively tries all combinations in a predefined grid\n",
    "- RandomizedSearchCV tests a random subset of combinations for efficiency \n",
    "- Bayesian optimization (e.g., with scikit‑optimize) uses a probabilistic model to explore promising regions of the hyperparameter space more intelligently. \n",
    "\n",
    "All three aim to find hyperparameters that maximize a chosen score (such as accuracy or F1) while keeping computation manageable.\n",
    "\n",
    "[https://businessanalyticsinstitute.com/implementing-decision-trees-with-scikit-learn/]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53de55",
   "metadata": {},
   "source": [
    "#### Example using the Iris dataset to compare four tuning strategies for a DecisionTreeClassifier:\n",
    "\n",
    "- GridSearchCV\n",
    "- RandomizedSearchCV\n",
    "- HalvingGridSearchCV\n",
    "- HalvingRandomSearchCV\n",
    "(halving searches use “successive halving”, which evaluates many candidates briefly, then focuses resources on the best ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faf6b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
    "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "156c05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Common parameter space for all searches\n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [3, 5, 7, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Helper to run a search and report time + scores\n",
    "def run_search(name, search_obj):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    start = time.time()\n",
    "    search_obj.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Best params:\", search_obj.best_params_)\n",
    "    print(\"Best CV score:\", search_obj.best_score_)\n",
    "\n",
    "    best_model = search_obj.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test accuracy:\", test_acc)\n",
    "    print(\"Elapsed time (s):\", round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4989bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GridSearchCV ===\n",
      "Best params: {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 2}\n",
      "Best CV score: 0.9523809523809523\n",
      "Test accuracy: 0.9333333333333333\n",
      "Elapsed time (s): 0.07\n"
     ]
    }
   ],
   "source": [
    "# 3. GridSearchCV (exhaustive grid)\n",
    "grid = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=0),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "run_search(\"GridSearchCV\", grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bee371f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomizedSearchCV ===\n",
      "Best params: {'min_samples_split': 10, 'max_depth': None, 'criterion': 'gini'}\n",
      "Best CV score: 0.9523809523809523\n",
      "Test accuracy: 0.9333333333333333\n",
      "Elapsed time (s): 0.04\n"
     ]
    }
   ],
   "source": [
    "# 4. RandomizedSearchCV (random subset of combinations)\n",
    "#    n_iter controls how many random combinations are tried\n",
    "rand = RandomizedSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,        # fewer than total combinations -> faster\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    ")\n",
    "run_search(\"RandomizedSearchCV\", rand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c9ad6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HalvingGridSearchCV ===\n",
      "Best params: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5}\n",
      "Best CV score: 0.9555555555555555\n",
      "Test accuracy: 0.8888888888888888\n",
      "Elapsed time (s): 0.11\n"
     ]
    }
   ],
   "source": [
    "# 5. HalvingGridSearchCV (successive halving over grid)\n",
    "halving_grid = HalvingGridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=0),\n",
    "    param_grid=param_grid,\n",
    "    factor=3,         # how aggressively to cut candidates each round\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "run_search(\"HalvingGridSearchCV\", halving_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f28d1a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HalvingRandomSearchCV ===\n",
      "Best params: {'min_samples_split': 5, 'max_depth': None, 'criterion': 'entropy'}\n",
      "Best CV score: 0.9444444444444443\n",
      "Test accuracy: 0.8888888888888888\n",
      "Elapsed time (s): 0.04\n"
     ]
    }
   ],
   "source": [
    "# 6. HalvingRandomSearchCV (successive halving over random samples)\n",
    "halving_rand = HalvingRandomSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=0),\n",
    "    param_distributions=param_grid,\n",
    "    factor=3,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    ")\n",
    "run_search(\"HalvingRandomSearchCV\", halving_rand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98667c13",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- param_grid/param_distributions use the same ranges for fair comparison.\n",
    "​\n",
    "- GridSearchCV tests all combinations; RandomizedSearchCV tests only n_iter random ones.\n",
    "​\n",
    "- HalvingGridSearchCV and HalvingRandomSearchCV start with many candidates but use fewer data/resources per candidate at first, then keep only the best and allocate more resources in later iterations, often finding good hyperparameters faster.\n",
    "​"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
