{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bd7544",
   "metadata": {},
   "source": [
    "The Gini index (Gini impurity) measures how **mixed** the class labels are at a node in a decision tree, and it is used to choose splits that make nodes as pure (homogeneous) as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36914ae6",
   "metadata": {},
   "source": [
    "\n",
    "### Intuition and probability view\n",
    "\n",
    "- A node is **pure** if all samples belong to the same class; its Gini index is 0 (no impurity).\n",
    "- A node is **impure** if labels are mixed; the closer the class proportions are to equal, the higher the impurity (closer to 1 for many classes, up to 0.5 in a balanced binary case).\n",
    "\n",
    "Probabilistic interpretation:\n",
    "\n",
    "- Imagine picking a random sample from the node.  \n",
    "- Then imagine assigning its label **at random**, but using the observed class probabilities in that node.  \n",
    "- The **Gini impurity** is the probability that this sample would be misclassified in that random process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4b567",
   "metadata": {},
   "source": [
    "\n",
    "Formal definition for a node with $ k $  classes:\n",
    "\n",
    "- Let $ p_i $  be the proportion of samples of class $ i $  in that node.  \n",
    "- The Gini index is  \n",
    "\n",
    "  $$ \n",
    "  G = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "  $$  \n",
    "\n",
    "  where $ \\sum_{i=1}^{k} p_i = 1 $ .\n",
    "\n",
    "Key properties:\n",
    "\n",
    "- If all samples are one class: one $ p_i = 1 $ , others 0 → $ G = 1 - 1^2 = 0 $  (perfectly pure).\n",
    "- For a binary node with $ p $  and $ 1-p $ : $ G = 1 - (p^2 + (1-p)^2) $ , with a maximum of 0.5 at $ p = 0.5 $ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58261125",
   "metadata": {},
   "source": [
    "\n",
    "### Role in decision trees\n",
    "\n",
    "Decision trees use Gini impurity as a **splitting criterion**:\n",
    "\n",
    "- At a node, the algorithm considers possible splits on each feature.  \n",
    "- For each candidate split, it computes the **weighted average** Gini of the child nodes.  \n",
    "- The split that produces the **lowest weighted Gini** (or equivalently, **highest Gini gain**) is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab40b87",
   "metadata": {},
   "source": [
    "\n",
    "Weighted Gini after a split:\n",
    "\n",
    "- Suppose a node with $ N $  samples is split into left and right child nodes with $ N_L $  and $ N_R $  samples.  \n",
    "- Their impurities are $ G_L $  and $ G_R $ .  \n",
    "- The weighted Gini of the split is  \n",
    "  $$ \n",
    "  G_{\\text{split}} = \\frac{N_L}{N} G_L + \\frac{N_R}{N} G_R\n",
    "  $$[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bf426",
   "metadata": {},
   "source": [
    "\n",
    "Gini gain (how much impurity is reduced):\n",
    "\n",
    "- Let $ G_{\\text{parent}} $  be the impurity before splitting.  \n",
    "- Gini gain is  \n",
    "  $$ \n",
    "  \\Delta G = G_{\\text{parent}} - G_{\\text{split}}\n",
    "  $$[1][5]\n",
    "\n",
    "- Higher $ \\Delta G $  means the split is better (it cleans up the node more).  \n",
    "- Practically, libraries either **minimize** $ G_{\\text{split}} $  or **maximize** $ \\Delta G $ ; both are equivalent.\n",
    "\n",
    "The feature (and threshold) that gives the **smallest weighted Gini** after the split becomes the decision at that node (e.g., the root feature for the first split).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4878483",
   "metadata": {},
   "source": [
    "### Gini calculation example\n",
    "\n",
    "Dataset (binary labels “Yes” and “No”):\n",
    "\n",
    "| Instance | Feature 1 | Feature 2 | Label |\n",
    "|----------|-----------|-----------|-------|\n",
    "| 1        | 0         | 1         | Yes   |\n",
    "| 2        | 1         | 0         | No    |\n",
    "| 3        | 0         | 1         | Yes   |\n",
    "| 4        | 1         | 1         | Yes   |\n",
    "| 5        | 0         | 0         | No    |\n",
    "\n",
    "1. **Count labels at the root node**:\n",
    "\n",
    "- Total instances $ N = 5 $ .  \n",
    "- Yes: 3 (instances 1, 3, 4) → $ p_{\\text{Yes}} = 3/5 $ .  \n",
    "- No: 2 (instances 2, 5) → $ p_{\\text{No}} = 2/5 $ .\n",
    "\n",
    "2. **Compute root Gini**:\n",
    "\n",
    "$$ \n",
    "G_{\\text{root}} = 1 - \\left( \\left(\\frac{3}{5}\\right)^2 + \\left(\\frac{2}{5}\\right)^2 \\right)\n",
    "= 1 - \\left( \\frac{9}{25} + \\frac{4}{25} \\right)\n",
    "= 1 - \\frac{13}{25}\n",
    "= \\frac{12}{25} = 0.48\n",
    "$$\n",
    "\n",
    "So the root node impurity is 0.48; the labels are somewhat mixed but not maximally so.\n",
    "\n",
    "\n",
    "#### Gini for Feature 1\n",
    "\n",
    "Feature 1 has values 0 and 1.\n",
    "\n",
    "##### Node: Feature 1 = 0\n",
    "\n",
    "Instances: 1, 3, 5.\n",
    "\n",
    "- Labels: Yes, Yes, No.  \n",
    "- Total: 3.  \n",
    "- $ p_{\\text{Yes}} = 2/3 $ , $ p_{\\text{No}} = 1/3 $ .\n",
    "\n",
    "Gini:\n",
    "\n",
    "$$ \n",
    "G_{F1=0} = 1 - \\left( \\left(\\frac{2}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 \\right)\n",
    "= 1 - \\left( \\frac{4}{9} + \\frac{1}{9} \\right)\n",
    "= 1 - \\frac{5}{9}\n",
    "= \\frac{4}{9} \\approx 0.444\n",
    "$$\n",
    "\n",
    "##### Node: Feature 1 = 1\n",
    "\n",
    "Instances: 2, 4.\n",
    "\n",
    "- Labels: No, Yes.  \n",
    "- Total: 2.  \n",
    "- $ p_{\\text{Yes}} = 1/2 $ , $ p_{\\text{No}} = 1/2 $ .\n",
    "\n",
    "Gini:\n",
    "\n",
    "$$ \n",
    "G_{F1=1} = 1 - \\left( \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 \\right)\n",
    "= 1 - \\left( \\frac{1}{4} + \\frac{1}{4} \\right)\n",
    "= 1 - \\frac{1}{2}\n",
    "= \\frac{1}{2} = 0.5\n",
    "$$\n",
    "\n",
    "##### Weighted Gini after splitting on Feature 1\n",
    "\n",
    "Node counts:\n",
    "\n",
    "- For Feature 1 = 0: 3 samples.  \n",
    "- For Feature 1 = 1: 2 samples.  \n",
    "- Total: 5.\n",
    "\n",
    "Weighted Gini:\n",
    "\n",
    "$$ \n",
    "G_{\\text{split, F1}} = \\frac{3}{5} \\cdot 0.444 + \\frac{2}{5} \\cdot 0.5\n",
    "\\approx 0.2664 + 0.2 = 0.4664\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "- Root node Gini: 0.48.  \n",
    "- After splitting on Feature 1: 0.4664 (slightly cleaner overall).\n",
    "\n",
    "Gini gain for this split:\n",
    "\n",
    "$$ \n",
    "\\Delta G_{\\text{F1}} = 0.48 - 0.4664 = 0.0136\n",
    "$$\n",
    "\n",
    "So Feature 1 slightly improves purity compared to the unsplit node.\n",
    "\n",
    "The same procedure is repeated for **Feature 2**:\n",
    "\n",
    "- Compute Gini for Feature 2 = 0 and Feature 2 = 1 nodes.  \n",
    "- Compute their weighted Gini.  \n",
    "- Compare with 0.4664:  \n",
    "  - If Feature 2’s weighted Gini is **lower**, Feature 2 is better for the root.  \n",
    "  - If it is **higher**, Feature 1 is better.  \n",
    "  - The feature with the **lowest weighted Gini** becomes the root feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5470b",
   "metadata": {},
   "source": [
    "## Python implementation (from basics)\n",
    "\n",
    "Below is a minimal, beginner-friendly implementation of Gini impurity and the weighted Gini calculation for a split, using only core Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f1548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing the toy dataset\n",
    "\n",
    "data = [\n",
    "    {\"instance\": 1, \"feature1\": 0, \"feature2\": 1, \"label\": \"Yes\"},\n",
    "    {\"instance\": 2, \"feature1\": 1, \"feature2\": 0, \"label\": \"No\"},\n",
    "    {\"instance\": 3, \"feature1\": 0, \"feature2\": 1, \"label\": \"Yes\"},\n",
    "    {\"instance\": 4, \"feature1\": 1, \"feature2\": 1, \"label\": \"Yes\"},\n",
    "    {\"instance\": 5, \"feature1\": 0, \"feature2\": 0, \"label\": \"No\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c121923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Gini impurity\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def gini_index(labels):\n",
    "    \"\"\"\n",
    "    labels: list of class labels at a node, e.g. [\"Yes\", \"No\", \"Yes\"]\n",
    "    returns: Gini impurity between 0 and 1\n",
    "    \"\"\"\n",
    "    n = len(labels)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    counts = Counter(labels)\n",
    "    # compute sum of squared probabilities\n",
    "    sum_p2 = 0.0\n",
    "    for count in counts.values():\n",
    "        p = count / n\n",
    "        sum_p2 += p ** 2\n",
    "\n",
    "    return 1.0 - sum_p2\n",
    "\n",
    "# - `Counter` counts the occurrences of each label.  \n",
    "# - Each probability $ p_i $  is `count / n`.  \n",
    "# - It applies the formula $ 1 - \\sum p_i^2 $ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695b3ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Gini: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Root Gini with this function\n",
    "\n",
    "root_labels = [row[\"label\"] for row in data]\n",
    "g_root = gini_index(root_labels)\n",
    "print(\"Root Gini:\", g_root)  # expected 0.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542e3d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini(F1=0): 0.4444444444444444\n",
      "Gini(F1=1): 0.5\n",
      "Weighted Gini after split on F1: 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Splitting on Feature 1 and computing weighted Gini\n",
    "\n",
    "# Split data into two groups based on feature1\n",
    "left = [row for row in data if row[\"feature1\"] == 0]\n",
    "right = [row for row in data if row[\"feature1\"] == 1]\n",
    "\n",
    "left_labels = [row[\"label\"] for row in left]\n",
    "right_labels = [row[\"label\"] for row in right]\n",
    "\n",
    "g_left = gini_index(left_labels)   # expected ≈ 0.444\n",
    "g_right = gini_index(right_labels) # expected = 0.5\n",
    "\n",
    "n_total = len(data)\n",
    "n_left = len(left)\n",
    "n_right = len(right)\n",
    "\n",
    "weighted_gini_f1 = (n_left / n_total) * g_left + (n_right / n_total) * g_right\n",
    "\n",
    "print(\"Gini(F1=0):\", g_left)\n",
    "print(\"Gini(F1=1):\", g_right)\n",
    "print(\"Weighted Gini after split on F1:\", weighted_gini_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d57b45",
   "metadata": {},
   "source": [
    "This code reproduces the calculations:\n",
    "\n",
    "- Gini at root = 0.48.  \n",
    "- Gini for Feature 1 = 0 node ≈ 0.444.  \n",
    "- Gini for Feature 1 = 1 node = 0.5.  \n",
    "- Weighted Gini ≈ 0.4664."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935491fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Gini: 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "# General function to evaluate a binary split\n",
    "\n",
    "def weighted_gini(groups):\n",
    "    \"\"\"\n",
    "    groups: list of groups, where each group is a list of labels.\n",
    "            e.g. groups = [left_labels, right_labels]\n",
    "    returns: weighted Gini of the split\n",
    "    \"\"\"\n",
    "    total = sum(len(g) for g in groups)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "\n",
    "    result = 0.0\n",
    "    for labels in groups:\n",
    "        weight = len(labels) / total\n",
    "        result += weight * gini_index(labels)\n",
    "\n",
    "    return result\n",
    "    \n",
    "groups = [left_labels, right_labels]\n",
    "print(\"Weighted Gini:\", weighted_gini(groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8df2d4",
   "metadata": {},
   "source": [
    "## Summary of what Gini tells you\n",
    "\n",
    "- Gini impurity is a **number between 0 and 1** that quantifies how mixed the labels are at a node.\n",
    "- A node with Gini = 0 is perfectly **pure** (all labels identical).  \n",
    "- Decision trees repeatedly split nodes to **reduce** Gini, aiming for purer child nodes.  \n",
    "- For each feature, a split’s quality is measured via **weighted average Gini** of the resulting branches; the feature with the **lowest weighted Gini** becomes the splitting feature at that node.\n",
    "\n",
    "This connects directly to overfitting: aggressively minimizing impurity with deep trees can yield extremely pure leaves (very low Gini and low training error) while harming generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b888f8b",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "[1](https://victorzhou.com/blog/gini-impurity/)\n",
    "[2](https://www.geeksforgeeks.org/machine-learning/gini-impurity-and-entropy-in-decision-tree-ml/)\n",
    "[3](https://www.baeldung.com/cs/impurity-entropy-gini-index)\n",
    "[4](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "[5](https://www.learndatasci.com/glossary/gini-impurity/)\n",
    "[6](https://courses.cs.washington.edu/courses/cse416/20su/files/section/section04/gini-impurity.pdf)\n",
    "[7](https://blog.quantinsti.com/gini-index/)\n",
    "[8](https://www.youtube.com/watch?v=u4IxOk2ijSs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
