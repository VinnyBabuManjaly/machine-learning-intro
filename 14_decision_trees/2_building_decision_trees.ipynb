{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39da3325",
   "metadata": {},
   "source": [
    "Decision trees organize data classification like a flowchart of yes/no questions, breaking down decisions into simple branches that lead to predictions. They consist of root, internal, and leaf nodes, built step-by-step by selecting splits that reduce data disorder, as outlined in the transcript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d613e7",
   "metadata": {},
   "source": [
    "## Key Components\n",
    "\n",
    "- The root node starts the tree, representing the full dataset before any decisions. \n",
    "\n",
    "- Internal nodes act as question points with branches for different outcomes, like \"Income high or low?\", allowing further splits into child nodes. \n",
    "\n",
    "- Leaf nodes end the paths, providing final predictions such as \"Buy\" or \"Not Buy,\" with no more splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f24165",
   "metadata": {},
   "source": [
    "## Step-by-Step Building\n",
    "\n",
    "- Begin at Step 1 by picking the root attribute's best split for highest information gain or lowest impurity, like choosing \"Income\" for customer buy/no-buy data. \n",
    "\n",
    "- Step 2 computes class entropy as disorder measure: for 80% buy (p+ = 0.8) and 20% not (p- = 0.2), \n",
    "$$\n",
    "E(S) = -0.8 \\log_2(0.8) - 0.2 \\log_2(0.2) \\approx 0.72\n",
    "$$\n",
    "\n",
    "- Steps 3-4 evaluate post-split entropy for each attribute (weighted average) and gain (original entropy minus that average), selecting the top gainer like \"Income.\"\n",
    "\n",
    "- Step 5 splits data into child nodes, such as high/low income groups.\n",
    "\n",
    "- Steps 6-7 repeat entropy/gain calculations recursively—for high-income, maybe split by \"Age > 30\" or \"Education\"—until stopping rules like max depth or min samples per leaf, forming paths like \"High Income → Age > 30 → Buy.\" \n",
    "\n",
    "- Step 8 tests on validation data with accuracy, precision, or recall to ensure reliable predictions on new customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d44d5",
   "metadata": {},
   "source": [
    "## Practical Tips\n",
    "\n",
    "Use criteria like Gini impurity (similar to entropy but simpler: 1 - sum of squared class proportions) alongside entropy for splits in algorithms like CART. \n",
    "\n",
    "Visualize trees to trace decisions, prune to avoid overfitting, and note they handle categorical/numerical data well for intuitive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f61e0",
   "metadata": {},
   "source": [
    "Sources\n",
    "\n",
    "[1](https://dev.to/adityapratapbh1/decision-tree-structure-a-comprehensive-guide-3peb)\n",
    "[2](https://spotintelligence.com/2024/05/22/decision-trees-in-ml/)\n",
    "[3](https://graphite-note.com/a-comprehensive-guide-to-decision-trees-everything-you-need-to-know/)\n",
    "[4](https://en.wikipedia.org/wiki/Decision_tree)\n",
    "[5](https://www.geeksforgeeks.org/machine-learning/decision-tree/)\n",
    "[6](https://www.vationventures.com/glossary/decision-trees-definition-explanation-and-use-cases)\n",
    "[7](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html)\n",
    "[8](https://christophm.github.io/interpretable-ml-book/tree.html)\n",
    "[9](https://www.hypertextbookshop.com/dataminingbook/public_version/contents/chapters/chapter001/section002/green/page001.html)\n",
    "[10](https://www.ibm.com/think/topics/decision-trees)\n",
    "[11](https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
