{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39f3c6f",
   "metadata": {},
   "source": [
    "Training error, test error, and overfitting in decision trees all revolve around one idea: as a tree grows deeper, it fits the training data better, but beyond a point it starts memorizing noise instead of learning patterns, which makes test performance worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff02a3",
   "metadata": {},
   "source": [
    "### Training vs test error\n",
    "\n",
    "Training error: Percentage of misclassified examples on the training set (data used to fit the tree).\n",
    "\n",
    "Test error: Percentage of misclassified examples on unseen test data held out from training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0dd9ab",
   "metadata": {},
   "source": [
    "### Key relationships for a decision tree:\n",
    "\n",
    "As tree depth (or number of nodes) increases:\n",
    "\n",
    "- Training error monotonically decreases, often reaching 0% (perfect accuracy), because the model keeps creating splits that separate training points.\n",
    "\n",
    "- Test error first decreases (model fits true patterns), then increases (model fits noise), giving a U-shaped curve when plotted against tree size.\n",
    "\n",
    "This pattern—low training error but high test error at large tree sizes—is the hallmark of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73849855",
   "metadata": {},
   "source": [
    "### Overfitting in decision trees\n",
    "\n",
    "Overfitting means the model captures random fluctuations (noise) in the training data rather than the underlying signal.\n",
    "\n",
    "For decision trees:\n",
    "\n",
    "- Early splits capture real structure (e.g., separating species by petal length in Iris).\n",
    "- Later splits can become extremely specific, carving out tiny regions that only contain a few training points.\n",
    "- When the tree keeps growing until leaves are pure (only one class), training accuracy becomes 100%, but generalization to new data degrades.\n",
    "\n",
    "Typical symptoms:\n",
    "- Very deep tree.\n",
    "- Almost zero training error.\n",
    "- Noticeably higher test error than simpler (shallower) trees.\n",
    "\n",
    "Mitigation strategies (conceptual):\n",
    "- Limit maximum depth (max_depth).\n",
    "- Require a minimum number of samples per leaf (min_samples_leaf).\n",
    "- Use pruning strategies or ensembles (Random Forests, Gradient Boosted Trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364e93c",
   "metadata": {},
   "source": [
    "### Why the Iris tree didn’t reach 100% training accuracy\n",
    "In the Iris example, a decision tree built with scikit-learn achieves about 99.3% training accuracy, not 100%.\n",
    "\n",
    "Reason:\n",
    "- There is a leaf (terminal node) that contains 3 samples from different classes, i.e., it is impure.\n",
    "\n",
    "- All three samples in that leaf:\n",
    "    - Have exactly the same petal_length and petal_width (4.8 and 1.8 in the example).\n",
    "    - But belong to different species (e.g., two from one class, one from another).\n",
    "\n",
    "Given only these two features (petal_length, petal_width), no further rule can separate these three points:\n",
    "- Any split on petal_length or petal_width would put all three points on the same side, because their feature values are identical.\n",
    "- scikit-learn therefore stops splitting at this node, leaving it impure.\n",
    "\n",
    "Resulting behavior:\n",
    "- scikit-learn assigns the majority class in that leaf as the prediction (e.g., if 2 are class A and 1 is class B, prediction is A).\n",
    "- The one minority-class sample becomes a training error, which prevents 100% accuracy.\n",
    "\n",
    "This leads to an important generalization:\n",
    "\n",
    "*A scikit-learn decision tree (with default settings and unlimited depth) will achieve perfect training accuracy, except when there exist training samples from different classes with exactly the same feature vector.*\n",
    "\n",
    "If one of those three flowers had slightly different features (e.g., petal length 4.8001 instead of 4.8), the tree could separate it with another split and achieve 100% training accuracy.\n",
    "\n",
    "This strong tendency toward perfect training accuracy is a warning sign: such trees are highly prone to overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
