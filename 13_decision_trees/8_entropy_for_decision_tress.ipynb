{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b2657f",
   "metadata": {},
   "source": [
    "## Weighted entropy \n",
    "\n",
    "Weighted entropy is a way for a decision tree to score how good a split is: it looks at the entropy of each child node, weights each by how many samples go there, adds them up, and then compares this to the parent’s entropy to see how much uncertainty was reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3eef5",
   "metadata": {},
   "source": [
    "#### Recap: entropy and nodes\n",
    "\n",
    "- **Entropy** at a node measures how uncertain the class label is at that node.  \n",
    "  - Mixed classes → high entropy (more uncertainty).  \n",
    "  - Mostly one class → low entropy (less uncertainty).\n",
    "- At the **root**, all training samples are together, so entropy is usually high.  \n",
    "- As you move down the tree and ask more questions, nodes generally become purer and entropy goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2ced7",
   "metadata": {},
   "source": [
    "#### What is *weighted* entropy (WS)?\n",
    "\n",
    "Entropy by itself ignores how many samples are in a node. Weighted entropy fixes that by scaling entropy by the node’s size relative to the whole dataset.\n",
    "\n",
    "- Let a node contain some number of samples (say 54 out of 150).  \n",
    "- Its **weighted entropy** (WS) is:  \n",
    "  - WS = (fraction of all samples in this node) × (entropy of this node).\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Node with 54 samples, entropy = 0.445:  \n",
    "  - Fraction of samples: 54 / 150.  \n",
    "  - WS = (54/150) × 0.445 ≈ 0.16.  \n",
    "- Terminal node with 3 samples, entropy = 0.918:  \n",
    "  - Fraction of samples: 3 / 150.  \n",
    "  - WS = (3/150) × 0.918 ≈ 0.018.\n",
    "\n",
    "Key intuition:\n",
    "\n",
    "- Big nodes count more than tiny nodes.  \n",
    "- A large node becoming “tidier” (lower entropy) is more important than a tiny node getting cleaner, because it affects more of the data.\n",
    "\n",
    "At the **root**:\n",
    "\n",
    "- The root contains all samples, so fraction = 150 / 150 = 1.  \n",
    "- Therefore WS(root) = entropy(root) = 1.58."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3937c5fc",
   "metadata": {},
   "source": [
    "#### Using weighted entropy to measure a split\n",
    "\n",
    "For **one split rule** (e.g., `petal_length ≤ 2.45`), you have:\n",
    "\n",
    "- One **parent** node (before the question).  \n",
    "- Two **child** nodes (after the question: “Yes” branch and “No” branch).\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Compute entropy of each child node.  \n",
    "2. Compute WS for each child = (size fraction) × (its entropy).  \n",
    "3. Add child WS values to get the **total child WS**.  \n",
    "4. Compare parent WS and total child WS:\n",
    "   - **ΔWS = (parent WS) − (sum of child WS)**.  \n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- ΔWS is the **reduction in weighted entropy** – how much uncertainty was removed by asking that question.\n",
    "- Higher ΔWS means a better split (you learned more and made nodes purer overall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620eb57",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "\n",
    "`petal_length ≤ 2.45`\n",
    "\n",
    "Root:\n",
    "\n",
    "- WS(root) = 1.58 (since it has all 150 samples).  \n",
    "\n",
    "Split: `petal_length ≤ 2.45`:\n",
    "\n",
    "- Left child: all setosa → entropy = 0 → WS(left) = 0.  \n",
    "- Right child: mixed versicolor/virginica → total WS(right) = 0.67.  \n",
    "\n",
    "Total child weighted entropy:\n",
    "\n",
    "- Sum = 0 + 0.67 = 0.67.  \n",
    "\n",
    "Reduction:\n",
    "\n",
    "- ΔWS = 1.58 − 0.67 = 0.91.  \n",
    "\n",
    "Meaning:\n",
    "\n",
    "- This split reduces uncertainty by 0.91, a large drop.  \n",
    "- It creates a perfectly pure left node and a much cleaner right node, so it’s a very strong first rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bb119",
   "metadata": {},
   "source": [
    "#### How the algorithm uses ΔWS in practice\n",
    "\n",
    "Putting it into the decision tree generation algorithm:\n",
    "\n",
    "1. At a node, list candidate questions like:\n",
    "   - `petal_length ≥ 4.0?`  \n",
    "   - `sepal_length ≥ 0.5?`  \n",
    "   - `petal_width ≥ 0.8?`  \n",
    "2. For each candidate:\n",
    "   - Simulate splitting the data.  \n",
    "   - Compute child entropies and WS.  \n",
    "   - Compute ΔWS (reduction in weighted entropy).  \n",
    "3. Choose the rule with the **largest ΔWS** (greatest drop in uncertainty).  \n",
    "4. Apply that split, creating child nodes.  \n",
    "5. Repeat recursively on each child node until stopping criteria are met.\n",
    "\n",
    "This is a **greedy** strategy:\n",
    "\n",
    "- At each step, it chooses the best split *locally* (largest ΔWS at that node).  \n",
    "- It does not look ahead to future levels of the tree, but in practice this local rule works very well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747796c",
   "metadata": {},
   "source": [
    "#### Big picture\n",
    "\n",
    "- **Entropy** measures impurity / uncertainty at a node.  \n",
    "- **Weighted entropy (WS)** combines entropy with node size, so big nodes count more.  \n",
    "- **ΔWS** (information gain) measures how much a split reduces overall uncertainty.  \n",
    "- At each step, the decision tree **chooses the split with the largest ΔWS**, i.e., where it learns the most and creates the cleanest children overall.  \n",
    "\n",
    "This mechanism is exactly how scikit‑learn and similar libraries use entropy to select decision tree splits and explains why, in the Iris example, the rule `petal_width ≥ 0.8` is chosen as the first, strongest split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e76a68",
   "metadata": {},
   "source": [
    "Sources:  \n",
    "\n",
    "[1](https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c/)\n",
    "[2](https://bricaud.github.io/personal-blog/entropy-in-decision-trees/)\n",
    "[3](https://stackoverflow.com/questions/1132805/weighted-decision-trees-using-entropy)\n",
    "[4](https://ashutoshtripathi.com/2022/03/29/a-complete-guide-to-decision-tree-formation-and-interpretation-in-machine-learning/)\n",
    "[5](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "[6](https://www.youtube.com/watch?v=_L39rN6gz7Y)\n",
    "[7](https://www2.cs.arizona.edu/~debray/Publications/wt_dec_tree.pdf)\n",
    "[8](https://www.geeksforgeeks.org/machine-learning/gini-impurity-and-entropy-in-decision-tree-ml/)\n",
    "[9](https://www.geeksforgeeks.org/data-science/how-to-calculate-entropy-in-decision-tree/)\n",
    "[10](https://www.educative.io/answers/how-is-entropy-used-to-build-a-decision-tree)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
