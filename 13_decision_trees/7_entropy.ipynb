{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745e39b9",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy in decision trees is a way to **measure how mixed or uncertain the class labels are at a node**, and the tree uses this to decide which split is best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f06af8",
   "metadata": {},
   "source": [
    "#### Decision tree basics and nodes\n",
    "\n",
    "A decision tree is built **top‑down**, starting from a single root node that contains all the training data.\n",
    "\n",
    "- At each step, the algorithm:\n",
    "  - Looks at all features (like `petal_length`, `petal_width` in Iris).\n",
    "  - Tries possible split values (for numeric features, thresholds like “≥ 2.0”).\n",
    "  - Chooses the split that best separates the classes according to an impurity measure such as entropy.\n",
    "- The root node is the starting point; deeper levels correspond to more detailed decisions about the data.\n",
    "\n",
    "Two special types of nodes:\n",
    "\n",
    "- **Pure node**: All samples in that node have the same class label (e.g., all are “setosa”).  \n",
    "- **Unsplittable node**: Contains samples from different classes that are **exactly overlapping in feature space**, so no split on available features can separate them (e.g., multiple flowers with identical petal length and width but different species).\n",
    "The algorithm stops when:\n",
    "\n",
    "- All nodes are pure, or\n",
    "- Nodes are unsplittable, or\n",
    "- Some stopping rule is met (max depth, min samples per leaf, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583ba23",
   "metadata": {},
   "source": [
    "#### Intuitive look at “best split”\n",
    "\n",
    "In the Iris example, think of a 2D scatter plot with:\n",
    "\n",
    "- X‑axis: `petal_length`.  \n",
    "- Y‑axis: `petal_width`.  \n",
    "- Points colored by species.\n",
    "\n",
    "Possible splits are straight **horizontal or vertical lines**:\n",
    "\n",
    "- Example 1: `petal_width < 1.5` vs `≥ 1.5`.  \n",
    "  - One side has mostly virginica plus some others (e.g., 0 setosa, 15 versicolor, 49 virginica are on one side; others on the opposite side).  \n",
    "  - This split is **somewhat useful** because it gathers many virginica together, but still mixes classes.[6][2]\n",
    "- Example 2: `petal_length ≥ 4.0`.  \n",
    "  - This may put all virginicas into one node, which feels **better** because the node is more homogeneous.[6]\n",
    "- Example 3: `petal_width ≥ 0.5`.  \n",
    "  - This keeps most setosa points together (e.g., 48 of them on one side), compressing one class strongly into a single node.[6]\n",
    "- Example 4: `petal_width ≥ 0.8`.  \n",
    "  - This line can practically separate setosa entirely from the others, creating a very pure node for that class.[6]\n",
    "\n",
    "From these examples:\n",
    "\n",
    "- Different candidate lines (splits) give different **purities** in the child nodes.\n",
    "- Some splits “feel” better because one side is mostly a single class.\n",
    "- The goal of the algorithm is to **automate** this intuitive search: try many possible splits and pick the one that makes the resulting nodes as pure as possible.\n",
    "\n",
    "Entropy provides the **numeric score** for “how good” a node or a split is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ec80e",
   "metadata": {},
   "source": [
    "#### What is entropy at a node?\n",
    "\n",
    "Entropy is a number that measures **uncertainty, randomness, or impurity** in the class labels of a node.\n",
    "\n",
    "- If all samples in a node belong to **one** class:\n",
    "  - The node is fully predictable.\n",
    "  - Entropy is **0** (perfectly pure, no uncertainty).\n",
    "- If the node has a **mix** of classes with similar proportions:\n",
    "  - The node is hard to predict.\n",
    "  - Entropy is **high**.\n",
    "\n",
    "To compute entropy for a node:\n",
    "\n",
    "1. Look at all samples in that node.  \n",
    "2. Count how many belong to each class.  \n",
    "3. Convert counts to proportions $ p_C $ (e.g., 34 out of 110 is about 0.31 for class 0).  \n",
    "4. Plug these proportions into the entropy formula as the sum of $ p_C \\log $ terms, but you don’t need the exact math for intuition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7116c77",
   "metadata": {},
   "source": [
    "\n",
    "Example from Iris Dataset:\n",
    "\n",
    "- At the root:\n",
    "  - Class 0: 34 samples → $ p_0 ≈ 0.31 $.  \n",
    "  - Class 1: 36 samples → $ p_1 ≈ 0.33 $.  \n",
    "  - Class 2: 40 samples → $ p_2 ≈ 0.36 $.  \n",
    "- Using the formula, the entropy of this node is **about 1.58**.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- The three classes are present in similar proportions.\n",
    "- If you see a random sample at this root, you are quite uncertain which class it belongs to.\n",
    "- This high entropy reflects that uncertainty.\n",
    "\n",
    "Another node in the tree (left child):\n",
    "\n",
    "- Class 0: 31 samples.  \n",
    "- Class 1: 4 samples.  \n",
    "- Class 2: 1 sample.  \n",
    "- Proportions:\n",
    "  - $ p_0 ≈ 0.86 $.  \n",
    "  - $ p_1 ≈ 0.11 $.  \n",
    "  - $ p_2 ≈ 0.028 $.  \n",
    "- Using the formula, entropy is about **0.68**.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- This node is **much more dominated** by class 0 (e.g., setosa).  \n",
    "- A sample reaching this node is very likely to be class 0.  \n",
    "- The node is more predictable, so entropy is lower.\n",
    "\n",
    "Some key numeric patterns:\n",
    "\n",
    "- If all samples are in **one class** → entropy = 0 (pure node).  \n",
    "- If data are **evenly split between 2 classes** (50–50) → entropy = 1 (maximum uncertainty for binary classification).\n",
    "- If data are **evenly split between 3 classes** (about 1/3 each) → entropy ≈ 1.58.\n",
    "- More generally, if there are C classes and they are evenly split, entropy = log₂(C). This grows with the number of equally likely classes, reflecting increasing uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c8036",
   "metadata": {},
   "source": [
    "#### How entropy changes as you go down the tree\n",
    "\n",
    "As you move from the root down to leaves:\n",
    "\n",
    "- At the **root**, the class distribution is very mixed → entropy is high.  \n",
    "- After a good split, each child node has more skewed distributions:\n",
    "  - One child might be mostly setosa.\n",
    "  - Another might be a mix of versicolor and virginica.  \n",
    "- Entropy **decreases** in nodes where one class becomes more likely than the others.\n",
    "\n",
    "Eventually:\n",
    "\n",
    "- At a **leaf node** where all samples belong to the same class (e.g., all setosa), entropy becomes **0.0** (no uncertainty).\n",
    "- In the Iris tree you built earlier, if you check the entropy values on the visualization, you will see:\n",
    "  - High entropy near the top.\n",
    "  - Lower entropy near the bottom.\n",
    "  - Zero entropy at pure leaves.\n",
    "\n",
    "So entropy acts like a **“purity meter”** along the path from root to leaf:\n",
    "\n",
    "- High value → messy / unpredictable node.  \n",
    "- Low value → clean / predictable node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1157609",
   "metadata": {},
   "source": [
    "#### Why entropy is useful for splitting (information idea)\n",
    "\n",
    "Even without explicit formulas for information gain, the key idea is:\n",
    "\n",
    "- **Goal at each split**: make child nodes that are **more pure** (lower entropy) than the parent.\n",
    "- For a candidate split on a feature:\n",
    "  - Compute entropy of the left child node.\n",
    "  - Compute entropy of the right child node.\n",
    "  - Take a **weighted average** based on how many samples go left vs right.\n",
    "- A split is considered **better** if:\n",
    "  - The weighted average entropy of its children is **smaller**.  \n",
    "  - That means the split produced cleaner, more homogeneous nodes.\n",
    "\n",
    "Conceptually:\n",
    "\n",
    "- **Information gain** = “how much uncertainty was removed by this split.”  \n",
    "- High information gain (big drop in entropy) means that feature choice and threshold did a good job in organizing the data.\n",
    "\n",
    "In practice:\n",
    "\n",
    "- For each feature and split value, the algorithm computes the child-node entropies and picks the split with the **largest drop** from parent entropy to child weighted average.  \n",
    "- This is how the tree automatically finds splits like “petal_width ≥ 0.8” that nearly isolate a single class (e.g., setosa) in one branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46caca63",
   "metadata": {},
   "source": [
    "#### Putting it all together \n",
    "\n",
    "- A decision tree keeps **asking questions** (“Is petal_width ≥ 0.8?”, “Is petal_length ≥ 4.0?”, etc.) to split data into more homogeneous groups.\n",
    "- **Entropy** is the number the tree uses to measure how “mixed” the labels are at each node: high means mixed and uncertain, low means dominated by one class and predictable.\n",
    "- At each step, the algorithm:\n",
    "  - Tries many possible splits.\n",
    "  - For each split, looks at how much the entropy of the child nodes drops compared to the parent.\n",
    "  - Picks the split that makes child nodes as pure (low entropy) as possible.\n",
    "- As you go down the tree:\n",
    "  - Entropy generally decreases along each path.\n",
    "  - Pure leaves have entropy 0 and correspond to confident predictions.\n",
    "\n",
    "This is why node entropy is central to top‑down decision tree construction: it gives a **rigorous, numerical way** to define “best split,” instead of relying only on visual intuition about class boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261116e",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[1](https://www.educative.io/answers/how-is-entropy-used-to-build-a-decision-tree)\n",
    "[2](https://www.geeksforgeeks.org/data-science/how-to-calculate-entropy-in-decision-tree/)\n",
    "[3](https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html)\n",
    "[4](https://100daysofml.github.io/Week_06/Lesson_29.html)\n",
    "[5](https://en.wikipedia.org/wiki/Information_gain_(decision_tree))\n",
    "[6](https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c/)\n",
    "[7](https://www.geeksforgeeks.org/machine-learning/gini-impurity-and-entropy-in-decision-tree-ml/)\n",
    "[8](https://www.baeldung.com/cs/impurity-entropy-gini-index)\n",
    "[9](https://community.deeplearning.ai/t/explanation-of-the-formula-for-information-gain-in-the-decision-nodes/540497)\n",
    "[10](https://www.reddit.com/r/learnmachinelearning/comments/1e4nnb2/what_actually_the_entropy_in_decision_trees/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
