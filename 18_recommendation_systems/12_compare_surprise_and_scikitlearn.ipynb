{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff8318e",
   "metadata": {},
   "source": [
    "### SURPRISE vs scikit‑learn (high‑level differences)\n",
    "\n",
    "Both libraries are written in Python and have similar flavors, but they are optimized for **different goals**. [towardsdatascience](https://towardsdatascience.com/a-complete-guide-to-recommender-system-tutorial-with-sklearn-surprise-keras-recommender-5e52e8ceace1/)\n",
    "\n",
    "#### Purpose and focus\n",
    "\n",
    "- **SURPRISE**  \n",
    "  - Goal: recommendation systems based on ratings.  \n",
    "  - Focus: collaborative filtering, matrix factorization, neighborhood methods, and evaluation metrics suited to rating prediction. [pypi](https://pypi.org/project/scikit-surprise/1.0.4/)\n",
    "\n",
    "- **Scikit‑learn**  \n",
    "  - Goal: general‑purpose machine learning.  \n",
    "  - Focus: broad tasks like classification, regression, clustering, dimensionality reduction, etc. [towardsdatascience](https://towardsdatascience.com/a-complete-guide-to-recommender-system-tutorial-with-sklearn-surprise-keras-recommender-5e52e8ceace1/)\n",
    "\n",
    "#### Algorithms\n",
    "\n",
    "- **SURPRISE**  \n",
    "  - SVD, SVD++, NMF, KNN‑based recommenders, baseline and other CF‑specific algorithms. [the-odd-dataguy](https://www.the-odd-dataguy.com/2022/03/14/surprise/)\n",
    "\n",
    "- **Scikit‑learn**  \n",
    "  - Linear/logistic regression, SVMs, decision trees, random forests, k‑means, PCA, and many others for generic ML problems. [towardsdatascience](https://towardsdatascience.com/a-complete-guide-to-recommender-system-tutorial-with-sklearn-surprise-keras-recommender-5e52e8ceace1/)\n",
    "\n",
    "SURPRISE gives you a **deep toolbox** for recommenders; scikit‑learn gives you a **wide toolbox** for many ML tasks.\n",
    "\n",
    "#### Evaluation and metrics\n",
    "\n",
    "- **SURPRISE**  \n",
    "  - Emphasizes rating‑prediction metrics: RMSE, MAE, etc., plus cross‑validation tailored for recommender workflows. [surprise.readthedocs](https://surprise.readthedocs.io/en/v1.0.5/evaluate.html)\n",
    "\n",
    "- **Scikit‑learn**  \n",
    "  - Emphasizes general metrics: accuracy, precision, recall, F1, ROC‑AUC, as well as regression metrics and generic cross‑validation utilities. [towardsdatascience](https://towardsdatascience.com/a-complete-guide-to-recommender-system-tutorial-with-sklearn-surprise-keras-recommender-5e52e8ceace1/)\n",
    "\n",
    "#### Data handling\n",
    "\n",
    "- **SURPRISE**  \n",
    "  - Designed for user–item–rating data.  \n",
    "  - Provides Dataset, Trainset, testset abstractions and automatically encodes user/item ids. [surpriselib](https://surpriselib.com)\n",
    "\n",
    "- **Scikit‑learn**  \n",
    "  - Expects general feature matrices \\(X\\) and target vectors \\(y\\).  \n",
    "  - You are responsible for preprocessing, encoding IDs, and constructing features. [towardsdatascience](https://towardsdatascience.com/a-complete-guide-to-recommender-system-tutorial-with-sklearn-surprise-keras-recommender-5e52e8ceace1/)\n",
    "\n",
    "#### Model tuning\n",
    "\n",
    "- **SURPRISE**  \n",
    "  - Provides grid search and cross‑validation specialized for its recommender algorithms and metrics. [surprise.readthedocs](https://surprise.readthedocs.io/en/v1.0.1/getting_started.html)\n",
    "\n",
    "- **Scikit‑learn**  \n",
    "  - Provides grid search, randomized search, and cross‑validation that work for many different model families and tasks. [towardsdatascience](https://towardsdatascience.com/a-complete-guide-to-recommender-system-tutorial-with-sklearn-surprise-keras-recommender-5e52e8ceace1/)\n",
    "\n",
    "#### Typical use cases\n",
    "\n",
    "- **Use SURPRISE when**  \n",
    "  - You want to build a system that recommends items (movies, products, songs, etc.) based on explicit user ratings. [pub.aimind](https://pub.aimind.so/implementing-a-collaborative-filtering-recommendation-system-using-surprise-a-step-by-step-guide-2e879a34e021)\n",
    "\n",
    "- **Use scikit‑learn when**  \n",
    "  - You are working on tasks like predicting labels, clustering data, or doing dimensionality reduction with generic features. [towardsdatascience](https://towardsdatascience.com/a-complete-guide-to-recommender-system-tutorial-with-sklearn-surprise-keras-recommender-5e52e8ceace1/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c0941",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Why SURPRISE is useful in practice\n",
    "\n",
    "Taken together, these features make SURPRISE:\n",
    "\n",
    "- **Specialized**: everything is oriented around recommenders and rating prediction. [businessanalyticsinstitute](https://businessanalyticsinstitute.com/implementing-matrix-factorization-with-surprise-library/)\n",
    "- **Convenient**: built‑in loading, splitting, training, evaluation, and tuning for recommender‑specific setups. [pub.aimind](https://pub.aimind.so/implementing-a-collaborative-filtering-recommendation-system-using-surprise-a-step-by-step-guide-2e879a34e021)\n",
    "- **Experiment‑friendly**: easy to switch algorithms and hyperparameters to find what works best on your data. [blog.4geeks](https://blog.4geeks.io/implementing-a-recommender-system-with-surprise-in-python/)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
