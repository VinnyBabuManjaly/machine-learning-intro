{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872ab667",
   "metadata": {},
   "source": [
    "Gradient descent for collaborative filtering is a way to train a recommendation model faster and more systematically by adjusting both user and item factors together to reduce prediction error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b5dbe",
   "metadata": {},
   "source": [
    "\n",
    "### Setup: users, items, and factors\n",
    "\n",
    "- We represent each user by a small list of numbers called **user factors** (their hidden tastes or preferences).  \n",
    "- We represent each item (movies, songs, etc.) by **item factors** (hidden properties of the item).  \n",
    "- If we choose $Z$ factors, we put all user factors into a matrix $P$ (size $M \\times Z$, $M$ = number of users) and all item factors into a matrix $Q$ (size $Z \\times N$, $N$ = number of items).  \n",
    "- The predicted rating of user $i$ for item $j$ is the **dot product** of row $i$ of $P$ and column $j$ of $Q$: “how much the user likes each factor” multiplied by “how strongly the item has each factor,” summed up.\n",
    "\n",
    "Intuitively: users and items both live in the same hidden “preference space”; prediction is how aligned they are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1a469",
   "metadata": {},
   "source": [
    "### Error and loss (what we want to minimize)\n",
    "\n",
    "- For each known rating $r_{i,j}$ (what the user actually gave), the model produces a prediction $\\hat{r}_{i,j}$.  \n",
    "- The **error** for that pair is $\\hat{r}_{i,j} - r_{i,j}$.  \n",
    "- The **squared error** is $(\\hat{r}_{i,j} - r_{i,j})^2$. Squaring makes all errors positive and punishes big mistakes more.\n",
    "\n",
    "To get the overall quality of the model:\n",
    "\n",
    "- We sum the squared errors over all **existing** ratings only.  \n",
    "- We explicitly **skip missing ratings** (cases where the user never rated that item), because treating them as 0 would incorrectly teach the model that “no interaction means the user hates the item.”\n",
    "\n",
    "So the mean squared error is “average squared difference between predictions and true ratings, but only where we know the rating.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b0d2b1",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: find the best user and item factors\n",
    "\n",
    "- Our aim is to choose the numbers in $P$ and $Q$ so that this mean squared error is as small as possible.  \n",
    "- This means all the user and item factors (every entry in $P$ and $Q$) are **parameters** of one big model.  \n",
    "- If there are $M$ users, $N$ items, and $Z$ factors, there are $M \\times Z$ user parameters plus $Z \\times N$ item parameters in total.\n",
    "\n",
    "Instead of separately solving many small problems (like “fit a regression for each user” or “for each item”), we treat it as **one big optimization problem**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19125a7e",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient descent idea\n",
    "\n",
    "Gradient descent is a general method to minimize a function by taking small steps in the direction that reduces it fastest.\n",
    "\n",
    "Applied here:\n",
    "\n",
    "- The function we want to minimize = mean squared error over all known ratings.  \n",
    "- The inputs we can change = every element of $P$ and $Q$.  \n",
    "- The algorithm repeatedly:\n",
    "  - Computes how changing each parameter (each $P_{a,b}$, each $Q_{b,j}$) would change the error.  \n",
    "  - Moves that parameter slightly in the direction that **reduces** the error.\n",
    "\n",
    "This uses a **learning rate** $\\alpha$: a small number that controls how big each adjustment step is. Too big: unstable; too small: very slow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9f85c",
   "metadata": {},
   "source": [
    "\n",
    "### Update rule (in plain language)\n",
    "\n",
    "For a user factor $P_{a,b}$ (user $a$, factor $b$):\n",
    "\n",
    "- Look at all items that user $a$ actually rated.  \n",
    "- For each such item $j$:  \n",
    "  - Compute the prediction error $e_{a,j} = \\hat{r}_{a,j} - r_{a,j}$.  \n",
    "  - Weight that error by the corresponding item factor $Q_{b,j}$.  \n",
    "- Add up these “error × item-factor” terms.  \n",
    "- Adjust $P_{a,b}$ by subtracting learning_rate × (this sum).\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- If a factor tends to be **too low** for items the user likes, the update nudges that user factor **up**.  \n",
    "- If a factor tends to be **too high** for items the user rates low, the update nudges it **down**.\n",
    "\n",
    "A similar rule updates each item factor $Q_{b,j}$, using errors over all users who rated that item and the corresponding user factors.\n",
    "\n",
    "### Why this is more efficient than the earlier CF method\n",
    "\n",
    "Earlier collaborative filtering description:\n",
    "\n",
    "- Alternated between:\n",
    "  - Holding item factors fixed, fitting separate models for each user.  \n",
    "  - Holding user factors fixed, fitting separate models for each item.  \n",
    "- That means many separate optimization problems and can be inefficient.\n",
    "\n",
    "Gradient descent view:\n",
    "\n",
    "- Treats **all** user and item factors as one parameter set.  \n",
    "- Runs **one** optimization loop (one global gradient descent) that updates everything together.  \n",
    "- This is conceptually similar to how we train neural networks: one loss function, many parameters, one gradient descent process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0becc520",
   "metadata": {},
   "source": [
    "\n",
    "### Handling model complexity and interpretation\n",
    "\n",
    "- The number of factors $Z$ is flexible: could be 2 (toy case) or hundreds/thousands (real systems).  \n",
    "- Larger $Z$ allows the model to capture more subtle patterns but risks overfitting; this is often controlled with **regularization** (extra penalty on large factor values).  \n",
    "- The learned factors usually **do not have clear human labels**. They are just abstract directions in a learned space that happen to be useful for predicting ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f7f31",
   "metadata": {},
   "source": [
    "\n",
    "### Final picture (Funk SVD)\n",
    "\n",
    "- Start with random user and item factors.  \n",
    "- Repeatedly use gradient descent to adjust them to reduce mean squared error on known ratings.  \n",
    "- End up with:\n",
    "  - Matrix $P$: inferred user factors (each row = a user).  \n",
    "  - Matrix $Q$: inferred item factors (each column = an item).  \n",
    "- The product $P \\times Q$ approximates the original rating matrix: it predicts missing ratings and underlies the recommendations.\n",
    "\n",
    "This whole process—factorizing the rating matrix into user and item factors via gradient descent—is commonly called **Funk SVD** in recommender system literature."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
