{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60a378e",
   "metadata": {},
   "source": [
    "Matrix factorization is a way to turn a big table of user–item ratings into two smaller tables that capture hidden patterns about users and items, so we can predict new ratings and make recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce32892",
   "metadata": {},
   "source": [
    "\n",
    "### Core idea of matrix factorization\n",
    "\n",
    "- You start with a **rating matrix** $R$:  \n",
    "  - Rows = users, columns = items, cells = ratings (with many missing).  \n",
    "- Matrix factorization tries to find two smaller matrices:\n",
    "  - A **user matrix** $P$ (users × factors).  \n",
    "  - An **item matrix** $Q$ (factors × items).  \n",
    "- When you multiply them ($P \\times Q$), you get an **approximate rating matrix** $\\tilde{R}$:  \n",
    "  - Each entry in $\\tilde{R}$ is the **predicted rating** for a user–item pair.  \n",
    "- The numbers in these smaller matrices are **latent features** (hidden factors) that describe:\n",
    "  - How much each user likes each factor.  \n",
    "  - How much each item has each factor.  \n",
    "\n",
    "So instead of explicitly saying “this user likes action movies,” the model learns numeric factors that implicitly capture such patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457d612",
   "metadata": {},
   "source": [
    "\n",
    "### Matrix factorization as a view of collaborative filtering\n",
    "\n",
    "- In collaborative filtering, we use only user–item ratings (no explicit genres, tags, etc.).  \n",
    "- Matrix factorization is a **collaborative filtering technique** where:\n",
    "  - Users are represented by latent factor vectors (rows of $P$).  \n",
    "  - Items are represented by latent factor vectors (columns or rows of $Q$).  \n",
    "  - A predicted rating is the **dot product** between a user’s vector and an item’s vector.  \n",
    "- This dot product is exactly what you compute to get a predicted rating: user factors × item factors.\n",
    "\n",
    "This gives a compact way to summarize all the relationships in the rating data using a relatively small number of hidden factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b7678",
   "metadata": {},
   "source": [
    "\n",
    "### Connection to Funk SVD\n",
    "\n",
    "- The gradient descent method you saw earlier (Funk SVD) is **one specific matrix factorization algorithm** for recommendation.  \n",
    "- It:\n",
    "  - Chooses a factor dimension $Z$ (for example, 2, 20, 100, etc.).  \n",
    "  - Learns $P$ (size $M \\times Z$) and $Q$ (size $Z \\times N$) by minimizing prediction error with gradient descent.  \n",
    "  - Uses only the observed ratings and can handle many missing entries.  \n",
    "- This is often called “SVD” in recommendation contexts, but it is **not** the same as classical linear-algebra singular value decomposition.\n",
    "\n",
    "So: matrix factorization is the general concept; Funk SVD is a practical, gradient-descent-based way to perform it on sparse rating data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c0bbc",
   "metadata": {},
   "source": [
    "\n",
    "### How this differs from true SVD\n",
    "\n",
    "Classical **singular value decomposition (SVD)** (from linear algebra):\n",
    "\n",
    "- Decomposes a full $M \\times N$ matrix into **three** matrices (often written $U \\Sigma V^T$).  \n",
    "- The shapes are fixed: $U$ is $M \\times M$, $\\Sigma$ is $M \\times N$, $V$ is $N \\times N$.  \n",
    "- The columns of $U$ and $V$ are **orthogonal** (a strong mathematical property).  \n",
    "- Assumes the matrix is **complete** (no missing entries).\n",
    "\n",
    "In contrast, the matrix factorization used in Funk SVD:\n",
    "\n",
    "- Uses **two** matrices $P$ and $Q$ of sizes $M \\times Z$ and $Z \\times N$, where **$Z$ is chosen by you** (e.g., 50 or 100 factors).  \n",
    "- The factors are learned via **gradient descent**, aiming to minimize rating prediction error.  \n",
    "- Does **not** enforce orthogonality on the factor matrices.  \n",
    "- Crucially, it is designed to work with **missing entries**, which are standard in real-world recommendation data.\n",
    "\n",
    "That’s why we can’t just apply standard SVD directly to a typical user–item rating matrix with lots of missing ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb23d6f",
   "metadata": {},
   "source": [
    "\n",
    "### Why matrix factorization is useful in recommender systems\n",
    "\n",
    "- It **handles sparsity**: most users rate only a tiny fraction of items, but the model can still learn useful patterns and predict missing ratings.  \n",
    "- It **captures complex relationships**: users who have similar hidden factor vectors will tend to like similar items, even if they’ve never rated the exact same ones.  \n",
    "- It **scales**: with efficient implementations, matrix factorization can be applied to very large datasets (millions of users and items).  \n",
    "- It forms the backbone of many modern collaborative filtering systems (e.g., variants used in the Netflix Prize era).\n",
    "\n",
    "In short, matrix factorization is the viewpoint that your prediction model is effectively factoring the rating matrix into user and item factor matrices, and Funk SVD is a popular, gradient-descent-driven way to learn those factors from incomplete data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
