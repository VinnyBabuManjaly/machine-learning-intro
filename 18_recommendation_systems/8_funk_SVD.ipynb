{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63149cb",
   "metadata": {},
   "source": [
    "Funk SVD is a way to turn a big, sparse table of ratings (users × items) into two smaller tables of numbers that capture hidden “taste” patterns, and then use those to predict missing ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ab87f",
   "metadata": {},
   "source": [
    "\n",
    "### Big picture: what Funk SVD does\n",
    "\n",
    "- You start with a **rating matrix** $R$: rows = users, columns = items, entries = ratings (many are missing).  \n",
    "- Funk SVD learns:\n",
    "  - A **user matrix $P$**: each row is a short list of numbers describing that user’s latent preferences.  \n",
    "  - An **item matrix $Q$**: each row (or column, depending on convention) is a short list of numbers describing that item’s latent properties.  \n",
    "- The goal is:  \n",
    "  $$\n",
    "  R \\approx P Q^T\n",
    "  $$ \n",
    "  meaning: if you multiply $P$ and $Q^T$, you get an approximate version of the rating matrix where missing entries are filled in with predictions. [freecodecamp](https://www.freecodecamp.org/news/singular-value-decomposition-vs-matrix-factorization-in-recommender-systems-b1e99bc73599/)\n",
    "\n",
    "Think of it as compressing users and items into a shared hidden space where similar users and similar items end up close to each other. [arxiv](https://arxiv.org/pdf/2203.11026.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998603b0",
   "metadata": {},
   "source": [
    "\n",
    "### Model pieces: users, items, and biases\n",
    "\n",
    "- **User matrix (P)**  \n",
    "  - Each user $u$ is represented by a vector $p_u$ of latent features (for example, 20 or 50 numbers).  \n",
    "  - These numbers say how strongly the user likes each hidden factor (e.g., “action vs. romance,” “mainstream vs. niche”), but we don’t name these factors explicitly. [riverml](https://riverml.xyz/0.8.0/examples/matrix-factorization-for-recommender-systems-part-1/)\n",
    "\n",
    "- **Item matrix (Q)**  \n",
    "  - Each item $i$ is represented by a vector $q_i$ of latent features of the same length.  \n",
    "  - These numbers say how much the item has each hidden factor (e.g., “how action-heavy,” “how niche”), again without explicit labels. [arxiv](https://arxiv.org/pdf/2203.11026.pdf)\n",
    "\n",
    "- **Bias terms (optional but common)**  \n",
    "  - A **global bias**: the overall average rating across all users and items.  \n",
    "  - A **user bias**: some users rate higher or lower than average.  \n",
    "  - An **item bias**: some items tend to get higher or lower ratings than average.  \n",
    "  - These help capture simple patterns like “this user is strict” or “this movie is broadly liked” before considering detailed factors. [datajobs](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7099016",
   "metadata": {},
   "source": [
    "\n",
    "### Objective: what Funk SVD tries to optimize\n",
    "\n",
    "For every known rating $r_{u,i}$:\n",
    "\n",
    "- The model predicts a rating $\\hat{r}_{u,i}$ using the user vector $p_u$ and item vector $q_i$ (plus biases if used).  \n",
    "- The **error** is the difference: $r_{u,i} - \\hat{r}_{u,i}$.  \n",
    "- The loss usually uses **mean squared error (MSE)**: sum of squared errors over all known ratings, divided by how many there are. [robinwitte](https://robinwitte.com/wp-content/uploads/2019/10/RecommenderSystem.pdf)\n",
    "\n",
    "To avoid overfitting (memorizing the training data), the objective also adds **regularization**:\n",
    "\n",
    "- This penalizes very large values in $p_u$ and $q_i$.  \n",
    "- A regularization strength $\\lambda$ controls how strong this penalty is. [riverml](https://riverml.xyz/0.8.0/examples/matrix-factorization-for-recommender-systems-part-1/)\n",
    "\n",
    "So the training goal is: choose all user and item vectors so that:\n",
    "\n",
    "- Predicted ratings are close to the real ratings on known entries.  \n",
    "- Vectors stay reasonably small (controlled by regularization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bec164",
   "metadata": {},
   "source": [
    "\n",
    "### Training with stochastic gradient descent (SGD)\n",
    "\n",
    "Funk SVD does **not** run a heavy math routine like classical SVD; instead, it uses **stochastic gradient descent** (SGD): [freecodecamp](https://www.freecodecamp.org/news/singular-value-decomposition-vs-matrix-factorization-in-recommender-systems-b1e99bc73599/)\n",
    "\n",
    "1. **Initialize**  \n",
    "   - Start with small random values for all user vectors $p_u$ and item vectors $q_i$.  \n",
    "   - Initialize biases (if used) to simple values, like 0 or global average.\n",
    "\n",
    "2. **Loop over known ratings**  \n",
    "   For each observed rating $r_{u,i}$:  \n",
    "   - Compute the current prediction $\\hat{r}_{u,i}$ using $p_u$ and $q_i$ (and biases).  \n",
    "   - Compute the error $e_{u,i} = r_{u,i} - \\hat{r}_{u,i}$.  \n",
    "   - Update:\n",
    "     - The user vector $p_u$ a little in the direction that reduces this error.  \n",
    "     - The item vector $q_i$ similarly.  \n",
    "     - Biases (if present) are also nudged based on the error.  \n",
    "   - The step size is controlled by a **learning rate** (a small number).\n",
    "\n",
    "3. **Repeat**  \n",
    "   - Make many passes (epochs) over all known ratings until the error stops improving significantly.\n",
    "\n",
    "Because Funk SVD uses **only known ratings** and updates parameters online, it works well with sparse matrices and scales to large datasets. [rpubs](https://rpubs.com/Argaadya/recommender-svdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e11a9",
   "metadata": {},
   "source": [
    "\n",
    "### Making predictions\n",
    "\n",
    "Once training is done:\n",
    "\n",
    "- To predict how user $u$ will rate item $i$:  \n",
    "  - Compute the **dot product** of their vectors: $p_u \\cdot q_i$.  \n",
    "  - Add any bias terms if the model uses them (global, user, item biases).  \n",
    "- This predicted rating fills in the missing entry in the original rating matrix.  \n",
    "- To recommend items to a user, you:\n",
    "  - Predict ratings for items they haven’t rated.  \n",
    "  - Sort by predicted score.  \n",
    "  - Show the top items. [freecodecamp](https://www.freecodecamp.org/news/singular-value-decomposition-vs-matrix-factorization-in-recommender-systems-b1e99bc73599/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9cb3dc",
   "metadata": {},
   "source": [
    "\n",
    "### Why Funk SVD is useful\n",
    "\n",
    "- Handles **sparse data** well (lots of missing ratings). [ceur-ws](https://ceur-ws.org/Vol-3842/paper18.pdf)\n",
    "- Finds **latent patterns** in user taste and item style without hand-designed features.  \n",
    "- Scales to large systems (e.g., Netflix-size) when implemented efficiently with SGD or related methods. [datajobs](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
