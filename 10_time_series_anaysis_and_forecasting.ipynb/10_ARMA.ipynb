{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5739269",
   "metadata": {},
   "source": [
    "## Overview of ARMA\n",
    "\n",
    "The ARMA model stands for Autoregressive Integrated Moving Average. It is a popular statistical model used in time series forecasting, particularly for stationary time series data. ARMA combines two simpler models:\n",
    "\n",
    "- **Autoregressive (AR) model:** This uses past values (lags) of the time series itself to predict future values.\n",
    "- **Moving Average (MA) model:** This uses past forecast errors (residuals) to improve predictions.\n",
    "\n",
    "The term \"Integrated\" in ARIMA refers to making a time series stationary by differencing; however, ARMA assumes the time series is already stationary.\n",
    "\n",
    "In essence, ARMA models the current value of a time series as a combination of previous values and past errors, suitable for data whose statistical properties, like mean and variance, do not change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533931d",
   "metadata": {},
   "source": [
    "## What is a Stationary Time Series?\n",
    "\n",
    "A stationary time series is a sequence of data points whose statistical properties, such as mean, variance, and autocorrelation, are constant over time. This stability means that patterns in the data do not systematically change, making the process easier to model and forecast.\n",
    "\n",
    "Non-stationary data often show trends or seasonal patterns. In ARMA modeling, the data needs to be stationary, which may require preprocessing steps like differencing to remove trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51aea3",
   "metadata": {},
   "source": [
    "## Autoregressive (AR) Process\n",
    "\n",
    "The AR process predicts the current value based on a linear combination of previous values. For an AR process of order p, denoted as AR(p), the current value is calculated using the last p observations weighted by coefficients (usually called phi coefficients).\n",
    "\n",
    "For example, an AR(2) model predicts the current value based on the immediate past two values. The model \"feeds back\" on itself, meaning the series depends on its own history.\n",
    "\n",
    "Higher order AR models (larger p) include more past values, capturing longer dependencies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373013f",
   "metadata": {},
   "source": [
    "## Moving Average (MA) Process\n",
    "\n",
    "In contrast, an MA process models the current value based on past forecast errors (called residuals or white noise) rather than past values themselves. An MA process of order q, MA(q), uses the last q error terms weighted by coefficients (theta coefficients) to explain the current observation.\n",
    "\n",
    "This helps correct for noise or shocks that affected previous observations and improves the model by smoothing out erratic movements.\n",
    "\n",
    "A zero-order MA or AR process is equivalent to pure white noise (randomness without structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcce46",
   "metadata": {},
   "source": [
    "## Combining AR and MA: ARMA Process\n",
    "\n",
    "An ARMA process of order (p, q), written ARMA(p, q), combines both the AR(p) and MA(q) processes. This means the model uses both lagged observations and past errors to predict the current value.\n",
    "\n",
    "- Setting q = 0 yields a pure AR model.\n",
    "- Setting p = 0 yields a pure MA model.\n",
    "- Choosing p and q suitably allows modeling a wide range of stationary time series more flexibly and accurately.\n",
    "\n",
    "The ARMA model can represent any zero-mean stationary process to a desired accuracy by adjusting p, q, and the phi and theta coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21ee61",
   "metadata": {},
   "source": [
    "## Understanding Model Order (p and q)\n",
    "\n",
    "- **Order p** is the number of past values (lags) used in the AR part.\n",
    "- **Order q** is the number of past errors used in the MA part.\n",
    "\n",
    "Identifying the correct values of p and q is crucial for a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75850e",
   "metadata": {},
   "source": [
    "## Tools for Order Identification: ACF and PACF\n",
    "\n",
    "Two key analytical tools help identify the order of AR and MA components from observed data:\n",
    "\n",
    "- **Autocorrelation Function (ACF):** Measures the correlation of a time series with its own lagged values. For an MA(q) process, the ACF cuts off sharply after lag q, meaning only the first q autocorrelations are non-zero, then zero afterwards.\n",
    "- **Partial Autocorrelation Function (PACF):** Measures the correlation between the time series and its lagged values, removing the effects of intervening lags. For an AR(p) process, the PACF cuts off sharply after lag p, meaning only the first p partial autocorrelations are non-zero, then zero for higher lags.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "- ACF helps identify the MA order (q).\n",
    "- PACF helps identify the AR order (p).\n",
    "\n",
    "For mixed ARMA models, the interpretation is more complex, and model diagnostics or information criteria (like AIC or BIC) are often used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3d3c9",
   "metadata": {},
   "source": [
    "## Practical Notes\n",
    "\n",
    "- High-order AR or MA models (e.g., p or q = 10) are rare because distinguishing the exact order becomes difficult and models can overfit.\n",
    "- The starting point for identifying orders p and q is usually to visually inspect ACF and PACF plots from the data.\n",
    "- The ARMA model is most effective when applied to stationary time series; otherwise, differencing or other preprocessing is needed (ARIMA includes this integrated step)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3f69a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- The ARMA model is a flexible, powerful tool for modeling stationary time series by combining autoregressive and moving average components.\n",
    "- It relies on understanding past values and past errors.\n",
    "- Order selection is facilitated by examining autocorrelations (ACF) and partial autocorrelations (PACF).\n",
    "- ARMA can approximate any zero-mean stationary time series given appropriate parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ec890",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "[1](https://www.ureason.com/resources/understanding-auto-regressive-moving-average/)\n",
    "[2](https://en.wikipedia.org/wiki/Autoregressive_model)\n",
    "[3](https://rainbow.ldeo.columbia.edu/~alexeyk/Papers/AndersonOD1975.pdf)\n",
    "[4](https://www.tigerdata.com/learn/stationary-time-series-analysis)\n",
    "[5](https://espressomd.github.io/tutorials/error_analysis/error_analysis_part2.html)\n",
    "[6](https://towardsdatascience.com/a-step-by-step-guide-to-calculating-autocorrelation-and-partial-autocorrelation-8c4342b784e8/)\n",
    "[7](https://ieeexplore.ieee.org/document/1007146/)\n",
    "[8](https://apxml.com/courses/time-series-analysis-forecasting/chapter-4-arima-models-forecasting/arma-models)\n",
    "[9](https://www.geeksforgeeks.org/data-science/arma-time-series-model/)\n",
    "[10](https://aws.amazon.com/what-is/autoregressive-models/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
