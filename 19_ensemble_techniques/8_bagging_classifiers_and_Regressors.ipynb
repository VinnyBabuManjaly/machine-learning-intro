{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae3c11f",
   "metadata": {},
   "source": [
    "Bagging classifiers and bagging regressors use the same core idea, but they are applied to different types of prediction problems.\n",
    "\n",
    "\n",
    "\n",
    "### Shared idea: bagging as a meta-estimator\n",
    "\n",
    "- A **bagging meta-estimator** (classifier or regressor) builds many copies of a base model on **random subsets** of the original training data.  \n",
    "- These subsets are usually created using **bootstrap sampling**: sample with replacement from the original dataset to form each subset.  \n",
    "- Each base model makes its own prediction, and the bagging meta-estimator **aggregates** these predictions into a single final prediction.  \n",
    "- The purpose is to introduce **randomness** into model building and then combine the results to **reduce variance** and make predictions more stable.\n",
    "\n",
    "You can think of it as asking many similar models, each trained on slightly different data, and then combining their answers so that the overall result is less noisy.\n",
    "\n",
    "\n",
    "\n",
    "### Bagging classifiers\n",
    "\n",
    "- Used when the target is **categorical** (classification problem).  \n",
    "- Each base classifier (for example, a decision tree classifier) is trained on a different bootstrap sample of the original training set.  \n",
    "- At prediction time:\n",
    "  - Each classifier outputs a **class label** for a given input.  \n",
    "  - The bagging classifier aggregates these outputs:\n",
    "    - Typically by **majority vote**: the class predicted by the largest number of classifiers becomes the final prediction.  \n",
    "    - In some implementations, probabilities can be averaged and then the most probable class is chosen.\n",
    "\n",
    "Key points:\n",
    "- The randomness comes from which training samples each classifier sees.  \n",
    "- The ensemble reduces variance by combining many slightly different classifiers.  \n",
    "- The final output is a **category** (class).\n",
    "\n",
    "\n",
    "\n",
    "### Bagging regressors\n",
    "\n",
    "- Used when the target is **numeric** (regression problem).  \n",
    "- Each base regressor (for example, a decision tree regressor) is trained on a different bootstrap sample of the original training set.  \n",
    "- At prediction time:\n",
    "  - Each regressor outputs a **numeric value** for a given input.  \n",
    "  - The bagging regressor aggregates these outputs by taking the **average** of all the predicted values.\n",
    "\n",
    "Key points:\n",
    "- The training procedure is conceptually the same as for bagging classifiers: many models, each trained on a random subset.  \n",
    "- The difference is in the **type of target** and the **aggregation rule**:\n",
    "  - Classifier → vote/aggregate over **classes**.  \n",
    "  - Regressor → average over **numbers**.  \n",
    "\n",
    "\n",
    "\n",
    "#### Beginner-friendly summary\n",
    "\n",
    "- **Both** bagging classifiers and regressors:\n",
    "  - Train many base models on randomly sampled versions of the training data.  \n",
    "  - Introduce randomness during training and then **aggregate** predictions.  \n",
    "  - Aim to reduce variance and improve stability.\n",
    "\n",
    "- **Difference**:\n",
    "  - Bagging **classifier**: for class labels (yes/no, cat/dog, etc.), combine predictions by **voting**.  \n",
    "  - Bagging **regressor**: for numeric values (prices, temperatures, etc.), combine predictions by **averaging**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
