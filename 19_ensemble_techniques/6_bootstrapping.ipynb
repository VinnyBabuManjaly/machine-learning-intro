{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4342d7",
   "metadata": {},
   "source": [
    "### The wisdom of the crowd in machine learning\n",
    "When we combine the predictions of many models, the overall result can become **more accurate** and **more stable**. This is because different models can compensate for each other’s weaknesses — some may get certain parts wrong, while others get them right.\n",
    "\n",
    "This idea helps us in two major ways:\n",
    "- **Reducing bias** (fixing overly simple models)\n",
    "- **Reducing variance** (fixing overly complex models)\n",
    "\n",
    "### Bias and variance\n",
    "- **Bias** means the model is **too simple** and cannot capture the real patterns in the data (it underfits).  \n",
    "- **Variance** means the model is **too sensitive** to the training data and performs inconsistently on new data (it overfits).  \n",
    "\n",
    "To build good models, we want **low bias** and **low variance**. But usually, there is a trade-off between them.\n",
    "\n",
    "### Using ensembles\n",
    "An **ensemble** means combining several models to make a single, stronger one.  \n",
    "Researchers found that ensembles work best when the models inside the ensemble are **variations of the same type of model**.  \n",
    "\n",
    "Depending on the problem:\n",
    "- If each base model has **high variance**, we use **bagging (Bootstrap Aggregation)**.\n",
    "- If each base model has **high bias**, we use **boosting**.\n",
    "\n",
    "In this video, the focus is on **bagging**.\n",
    "\n",
    "### The high variance problem\n",
    "High variance models depend too much on their specific training data.  \n",
    "If the training set is small or noisy, these models memorize it too closely and fail to generalize.  \n",
    "Ideally, we’d fix this by collecting more data, but that’s often expensive or impossible.\n",
    "\n",
    "### The bootstrap idea\n",
    "When we can’t get new data, we can **simulate** new training datasets using the one we already have.  \n",
    "This approach is called the **bootstrap** — named after the expression “pull yourself up by your bootstraps,” meaning to help yourself without outside help.\n",
    "\n",
    "How does bootstrapping work?\n",
    "- We treat our existing dataset as if it were the entire population.\n",
    "- We then **create new datasets by sampling from it**, with an important detail: **sampling is done with replacement**.\n",
    "\n",
    "### Sampling with replacement\n",
    "“Sampling with replacement” means after selecting a data point from our dataset, we **put it back** and allow it to be picked again on the next draw.  \n",
    "This allows some data points to appear **multiple times**, while others might **not appear at all**, making each new dataset slightly different.\n",
    "\n",
    "### What percentage of the original data appears?\n",
    "Mathematically, for a dataset with $N$ items:\n",
    "- The probability that a specific item **is not chosen** in one draw = $1 - \\frac{1}{N}$\n",
    "- The probability that it is **never chosen** after $N$ draws = $(1 - \\frac{1}{N})^N$\n",
    "- So, the probability it **is chosen at least once** = $1 - (1 - \\frac{1}{N})^N$\n",
    "\n",
    "As $N$ becomes very large, this value approaches $1 - \\frac{1}{e} \\approx 0.632$.  \n",
    "\n",
    "This means each bootstrap dataset contains roughly **63% unique data points** from the original set — the rest (about **37%**) are duplicates due to resampling.\n",
    "\n",
    "\n",
    "\n",
    "In summary:\n",
    "- **Bootstrapping** creates multiple new datasets by randomly sampling (with replacement) from the original data.  \n",
    "- **Bagging** uses these bootstrapped datasets to train multiple models and then combines their predictions to reduce variance and improve stability.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
