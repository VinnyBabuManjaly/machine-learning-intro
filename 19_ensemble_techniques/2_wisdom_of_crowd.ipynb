{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905d51dd",
   "metadata": {},
   "source": [
    "The “wisdom of the crowd” is the idea that many independent opinions, when combined, can give a better answer than most individuals on their own. In ensemble learning, we apply the same idea to many models instead of many people.\n",
    "\n",
    "### Single models vs many opinions\n",
    "\n",
    "So far, you have seen models like KNN, linear regression, logistic regression, decision trees, and SVMs. Each of these is a **single** function that takes input data (a sample) and outputs a prediction. For example, a classifier outputs a class label, and a regressor outputs a number.\n",
    "\n",
    "Up to this point, the pattern has been: pick one model, train it, and use only that model to predict. In real life, though, we rarely rely on a single opinion. We ask multiple doctors, debate in parliaments, and elect leaders by voting. Ensemble learning brings this “many opinions” idea into machine learning by combining multiple models’ predictions.\n",
    "\n",
    "### What is ensemble learning?\n",
    "\n",
    "Ensemble learning means training several models and then combining their outputs to make a final decision. Each individual model might be weak or “not great” on its own, but together they can form a strong overall predictor.\n",
    "\n",
    "The key idea is: instead of trusting one model completely, you build a group of models and then aggregate their predictions (for example, by averaging or majority vote). Whether this works well depends on how these models behave: if their errors are different and not all pointing in the same direction, the combination can reduce overall error.\n",
    "\n",
    "### Why can combining “bad” models work?\n",
    "\n",
    "At first, it seems strange: if each model is pretty bad, why would combining them help? The answer is that “bad” here usually means “noisy but not systematically wrong.” If different models make different mistakes, their individual errors can cancel each other out when you average or vote.\n",
    "\n",
    "What matters is:\n",
    "- The models’ individual biases (do they tend to be too high or too low?).  \n",
    "- Their variances (how much their predictions fluctuate).  \n",
    "- How correlated they are (do they all make the same mistakes or different ones?).  \n",
    "\n",
    "If the models are not all biased in the same direction and are somewhat independent, combining them can reduce variance and improve reliability.\n",
    "\n",
    "### Wisdom of the crowd, step by step\n",
    "\n",
    "Imagine many people each making a guess about some unknown quantity. Each person might be a bit biased (some guess too low, some too high), and each person’s guesses vary (they don’t always say the same number). However:\n",
    "\n",
    "- Across the whole population, some people are low, some are high, and on average these biases balance out.  \n",
    "- If you take the **average** of many independent guesses, the average tends to be close to the true value.  \n",
    "- As you increase the number of people, the average becomes more stable: it jumps around less from one group to another.\n",
    "\n",
    "In statistics, this stability of the average comes from a core result: the variance of the average of independent samples decreases as $1 / n$, where $n$ is the number of samples. You don’t need the exact formula now; the intuition is: “more independent opinions → averaged result becomes less noisy.”\n",
    "\n",
    "### Importance of independence\n",
    "\n",
    "A crucial requirement is that each person (or model) makes their decision independently. If everyone copies one “expert” before answering, then the crowd is no longer diverse and independent. The group’s bias then starts to look exactly like that expert’s bias, and you lose the benefit of averaging.\n",
    "\n",
    "The same is true in machine learning:\n",
    "- If all models are essentially the same and make the same mistakes, combining them doesn’t help much.  \n",
    "- If models are trained in ways that make their errors different (for example, on different data samples or with different features), then combining them can reduce overall error.\n",
    "\n",
    "### Connecting to ensemble methods in ML\n",
    "\n",
    "In this module, you will study two major families of ensemble methods:\n",
    "\n",
    "- Bagging: build many models in parallel (often on different subsets of data) and combine them, which mainly reduces variance. Random forests are a specific algorithm that comes from this idea.\n",
    "- Boosting: build models sequentially, where each new model focuses on correcting the mistakes of the previous ones. AdaBoost and gradient-boosted trees are key examples.\n",
    "\n",
    "In all of these, the core wisdom-of-the-crowd principle is the same: use multiple, preferably diverse and not overly correlated models, and combine them so that individual weaknesses cancel out and the final prediction is more reliable than any single model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
