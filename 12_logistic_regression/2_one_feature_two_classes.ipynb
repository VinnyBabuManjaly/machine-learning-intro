{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d508be3",
   "metadata": {},
   "source": [
    "You want to decide which class something belongs to (class 0 or class 1) using just one feature, called **X**. Logistic regression helps us do that.\n",
    "\n",
    "1. **Basic Idea**\n",
    "   If you look at many data points and group them by similar X values, you will see how often each class occurs within each group. As you move from low X values to high X values, the probability of belonging to class 1 usually changes smoothly.\n",
    "\n",
    "2. **Goal**\n",
    "   We want to find the point where both classes are equally likely — that is, the probability of class 1 is 0.5. That point becomes our decision boundary.\n",
    "\n",
    "3. **Mathematical Setup**\n",
    "   To derive logistic regression, we assume:\n",
    "\n",
    "   * The feature X behaves like a normally distributed (Gaussian) variable inside each class.\n",
    "   * Both classes have the **same variance** but different means.\n",
    "\n",
    "4. **Probability Inside a Bin**\n",
    "   For a small range of X values, we want the probability that the item is class 1. This probability depends on how likely class 1 is compared to class 0 at that value of X.\n",
    "   The **ratio** of these two likelihoods is called the **odds ratio**.\n",
    "\n",
    "5. **Using the Gaussian Assumption**\n",
    "   When we substitute the normal distribution formulas for each class and simplify:\n",
    "\n",
    "   * The odds ratio becomes an exponential of a **linear function** of X.\n",
    "   * This is why logistic regression can use a straight-line formula inside an exponential.\n",
    "\n",
    "6. **Arriving at the Logistic (Sigmoid) Function**\n",
    "   From the odds ratio, we get the final formula for the probability of class 1:\n",
    "   [\n",
    "   \\sigma(x) = \\frac{1}{1 + e^{-z}}\n",
    "   ]\n",
    "   where (z = \\beta_0 + \\beta_1 x).\n",
    "   This S-shaped curve is called the **sigmoid** or **logistic** function.\n",
    "\n",
    "7. **Finding the Threshold**\n",
    "   The threshold is where the probability equals 0.5.\n",
    "   Setting σ(x) = 0.5 gives a simple condition:\n",
    "   [\n",
    "   \\beta_0 + \\beta_1 x = 0\n",
    "   ]\n",
    "   Solving this gives the decision boundary.\n",
    "\n",
    "8. **How to Find β₀ and β₁ in Real Life**\n",
    "   Earlier we assumed we knew the means and variances, but in reality we don’t. Instead, we use an optimization method to find the best values of β₀ and β₁.\n",
    "\n",
    "9. **Likelihood and Cross-Entropy**\n",
    "\n",
    "   * The **likelihood** tells us how well our model explains the actual data.\n",
    "   * We want to pick β₀ and β₁ that make the data as probable as possible.\n",
    "   * To make this easier, we take the **logarithm** of the likelihood.\n",
    "   * This turns into a formula known as **cross-entropy**.\n",
    "   * Instead of maximizing likelihood, we **minimize cross-entropy**.\n",
    "\n",
    "10. **Final Logistic Regression Problem**\n",
    "    Logistic regression becomes an optimization task:\n",
    "\n",
    "* Find β₀ and β₁ that minimize cross-entropy.\n",
    "* This gives us the best-fitting logistic curve.\n",
    "* This curve gives us probabilities and the class boundary.\n",
    "\n",
    "### **In One Sentence**\n",
    "\n",
    "Logistic regression finds the best S-shaped curve that turns a single input value into a probability between 0 and 1, and it chooses its parameters by minimizing cross-entropy so it can correctly separate two classes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
