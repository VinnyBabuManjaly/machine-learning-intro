{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0231eff",
   "metadata": {},
   "source": [
    "### **1. Moving From One Feature to Many**\n",
    "\n",
    "Previously, logistic regression was used with only **one measurement** to classify between **two classes**.\n",
    "But real problems often have **many measurements**.\n",
    "This explains how to extend logistic regression so it can use **many features at once**.\n",
    "\n",
    "\n",
    "### **2. Using Two Features First**\n",
    "\n",
    "Now each data point has two values, not just one.\n",
    "You can imagine each flower as a point in a 2-D space (one axis per measurement).\n",
    "The job is to decide the class of a new point using these two measurements.\n",
    "\n",
    "The idea is still the same as before:\n",
    "\n",
    "* If the nearby points mostly belong to one class, the new point should be assigned to that class.\n",
    "\n",
    "\n",
    "### **3. Extending the Theory**\n",
    "\n",
    "If we assume the data from each class follows a certain distribution, the math eventually shows something important:\n",
    "\n",
    "The **logistic regression formula looks the same as before**, but with more inputs.\n",
    "\n",
    "Even though the math becomes more complicated, the **final model is still simple**:\n",
    "\n",
    "* The output is computed using a linear combination of all the inputs.\n",
    "* This gets passed through the logistic (sigmoid) function to give a probability.\n",
    "\n",
    "\n",
    "### **4. General Logistic Regression With Many Inputs**\n",
    "\n",
    "For many features:\n",
    "\n",
    "* The input is now a **vector** (not a single number).\n",
    "* The model has one coefficient for each feature.\n",
    "* The final probability still comes from the **sigmoid** function.\n",
    "\n",
    "The parameters (the coefficients) are still found by **minimizing cross-entropy**, just as in the one-feature case.\n",
    "\n",
    "\n",
    "### **5. A Big Issue: Perfectly Separable Data**\n",
    "\n",
    "Sometimes two classes are completely separate with no overlap.\n",
    "When that happens:\n",
    "\n",
    "* Logistic regression tries to push the boundary sharper and sharper.\n",
    "* The optimization process keeps increasing the coefficients without stopping.\n",
    "* The model “blows up” and fails to converge.\n",
    "\n",
    "This problem arises because the algorithm is trying to fit a perfect, infinitely steep boundary.\n",
    "\n",
    "\n",
    "### **6. Fixing the Problem With Regularization**\n",
    "\n",
    "To prevent the coefficients from growing endlessly, we add **regularization**, which:\n",
    "\n",
    "* punishes extremely large coefficient values\n",
    "* keeps the model stable\n",
    "* avoids overfitting\n",
    "\n",
    "There are two common types:\n",
    "\n",
    "* **L1 (lasso)** → pushes unnecessary coefficients toward zero\n",
    "* **L2 (ridge)** → keeps coefficients small by penalizing their squared size\n",
    "\n",
    "Both methods keep logistic regression under control, even when the classes are perfectly separable.\n",
    "\n",
    "\n",
    "### **7. Important Note About scikit-learn**\n",
    "\n",
    "In scikit-learn:\n",
    "\n",
    "* The regularization parameter is called **C**, not lambda.\n",
    "* **C is the inverse of lambda.**\n",
    "  So:\n",
    "\n",
    "  * Higher C = weaker regularization\n",
    "  * Lower C = stronger regularization\n",
    "\n",
    "This is important when tuning the model.\n",
    "\n",
    "\n",
    "## **In One Sentence**\n",
    "\n",
    "Logistic regression naturally extends to many features, why it breaks when data is perfectly separable, and how regularization fixes that problem.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
