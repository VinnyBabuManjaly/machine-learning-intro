{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd43255d",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression\n",
    "\n",
    "#### Why We Need Multiclass Models\n",
    "\n",
    "Logistic regression naturally predicts two classes (binary classification).\n",
    "But real-world problems often have more than two classes — for example:\n",
    "- Handwritten digits (0–9)\n",
    "- Sentiment categories (positive/neutral/negative)\n",
    "- Flower species (setosa, versicolor, virginica)\n",
    "\n",
    "So, to handle multiple classes, logistic regression must be extended. This video explains three ways to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6160e62",
   "metadata": {},
   "source": [
    "#### 1. One-vs-Rest (OvR) – “One Against All”\n",
    "\n",
    "This is a simple way to turn a binary classifier into a multiclass classifier:\n",
    "\n",
    "✔ You train K models if there are K classes.   \n",
    "✔ Each model learns to distinguish one class from all others.   \n",
    "✔ At prediction time, you run all K models on the same input and choose the class that returns the highest probability.   \n",
    "\n",
    "For example, with 4 classes (K = 4), you train:\n",
    "- Class 1 vs (2,3,4)\n",
    "- Class 2 vs (1,3,4)\n",
    "- Class 3 vs (1,2,4)\n",
    "- Class 4 vs (1,2,3)\n",
    "\n",
    "Each classifier predicts a probability that the input belongs to its class. The model picks the class with the largest probability.\n",
    "\n",
    "Why One-vs-Rest Works Well\n",
    "- Easy to train and understand.\n",
    "- The number of models grows linearly with K.\n",
    "- Works with most binary classifiers (as long as they produce probabilities).\n",
    "\n",
    "Limitations\n",
    "- If the binary classifier does not produce probabilities (like some versions of perceptron), you can’t pick “most likely class”.\n",
    "- The boundaries between classes can be weird when there’s overlap.\n",
    "\n",
    "How Boundaries Look\n",
    "\n",
    "Even though each OvR classifier is binary, the decision regions join together into linear regions in the input space (just like binary logistic regression). They meet where two classifiers give equal output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919d2fc",
   "metadata": {},
   "source": [
    "#### 2. One-vs-One (OvO) – Pairwise Classifiers\n",
    "\n",
    "Instead of training K models, OvO trains one model for every pair of classes:\n",
    "\n",
    "✔ For K classes, number of binary models = K × (K−1) / 2.   \n",
    "✔ Example: For K = 4, you train 6 classifiers:    \n",
    "\n",
    "- 1 vs 2, 1 vs 3, 1 vs 4\n",
    "- 2 vs 3, 2 vs 4\n",
    "- 3 vs 4\n",
    "\n",
    "Each classifier decides between two specific classes.\n",
    "At prediction time, you run all pairwise models and use a majority vote — whichever class gets the most votes wins. \n",
    "\n",
    "Benefits\n",
    "- Can be used with any binary classifier (even if it doesn’t give probabilities).\n",
    "- Because each model sees only two classes, training is often faster per model.\n",
    "\n",
    "Drawbacks\n",
    "- The number of models grows quadratically with K — too many if you have many classes.\n",
    "- Sometimes voting ties can happen, so the method “can’t decide”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e308f0ac",
   "metadata": {},
   "source": [
    "#### 3. Multinomial Logistic Regression (“Softmax”)\n",
    "\n",
    "Instead of breaking the problem into many binary tasks, we can model all classes at once.\n",
    "\n",
    "This uses a function called softmax that generalizes the logistic (sigmoid) function to multiple outputs. The model learns weights for each class so that:\n",
    "\n",
    "✔ For a given input, it outputs K probabilities (one per class).   \n",
    "✔ These probabilities add up to 1 (because softmax enforces this).   \n",
    "✔ The predicted class is the one with the highest probability.  \n",
    "\n",
    "This approach is mathematically more elegant because it directly optimizes the probability of the correct class across all classes at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a377517",
   "metadata": {},
   "source": [
    "#### Intuition: OvR vs Multinomial\n",
    "\n",
    "| Method      | Treats Classes Independently? | Optimizes All Classes Together? |\n",
    "| ----------- | ----------------------------- | ------------------------------- |\n",
    "| One-vs-Rest | ✔                             | ✘                               |\n",
    "| One-vs-One  | ✘                             | ✘                               |\n",
    "| Multinomial | ✘                             | ✔                               |\n",
    "\n",
    "Multinomial logistic regression often produces **better-calibrated probabilities** because it considers classes together in one optimization problem, while OvR trains them independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac468f21",
   "metadata": {},
   "source": [
    "#### Python Implementaions\n",
    "\n",
    "\n",
    "1. **One-vs-Rest Multiclass**\n",
    "\n",
    "By default, scikit-learn uses OvR when `multi_class='ovr'` or if you use solver settings that default to OvR:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "model_ovr.fit(X_train, y_train)\n",
    "predictions_ovr = model_ovr.predict(X_test)\n",
    "```\n",
    "\n",
    "* `liblinear` solver often defaults to OvR.\n",
    "* Good for small datasets and binary/multiclass with OvR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1447f45",
   "metadata": {},
   "source": [
    "2. **Multinomial Logistic Regression**\n",
    "\n",
    "For true multiclass softmax logic:\n",
    "\n",
    "```python\n",
    "model_multi = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model_multi.fit(X_train, y_train)\n",
    "predictions_multi = model_multi.predict(X_test)\n",
    "```\n",
    "\n",
    "* `multi_class='multinomial'` tells scikit-learn to use multiclass softmax.\n",
    "* `lbfgs` or `newton-cg` or `saga` solvers are recommended for true multinomial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221639bb",
   "metadata": {},
   "source": [
    "3. **Prediction and Probabilities**\n",
    "\n",
    "For both strategies, you can also ask the model for probabilities:\n",
    "\n",
    "```python\n",
    "probabilities = model.predict_proba(X_test)\n",
    "```\n",
    "\n",
    "* This gives you the probability per class.\n",
    "* With OvR: the probability is computed per binary model and then transformed.\n",
    "* With Multinomial: softmax directly gives class probabilities that add to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37a56e",
   "metadata": {},
   "source": [
    "#### Examples of Decision Regions\n",
    "\n",
    "Visual examples show that OvR and multinomial logistic regression produce different decision boundaries (even if both are linear), because:\n",
    "\n",
    "* OvR trains each class independently.\n",
    "* Multinomial considers all classes in one shot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43297cf4",
   "metadata": {},
   "source": [
    "#### Summary of Pros and Cons\n",
    "\n",
    "**One-vs-Rest (OvR)**\n",
    "\n",
    "✔ Easy to implement  \n",
    "✔ Works with many binary classifiers  \n",
    "✔ Fewer models (linear growth)  \n",
    "✘ Each classifier sees full “other class” set imbalance  \n",
    "✘ Might not capture joint relationships  \n",
    " \n",
    "**One-vs-One (OvO)**\n",
    "\n",
    "✔ Works with any binary classifier  \n",
    "✔ Smaller training sets per classifier  \n",
    "✘ Many binary models (quadratic growth)  \n",
    "✘ Voting ties possible  \n",
    "\n",
    "\n",
    "**Multinomial**\n",
    "\n",
    "✔ One unified model  \n",
    "✔ Class probabilities optimized jointly  \n",
    "✔ Good calibration of probabilities  \n",
    "✘ Needs solvers that support multinomial (lbfgs, saga, etc.)  \n",
    "\n",
    "**Recap**\n",
    "\n",
    "To handle more than two classes in logistic regression:\n",
    "\n",
    "1. **One-vs-Rest (OvR)** — train K binary models — simplest.\n",
    "2. **One-vs-One (OvO)** — train pairwise models — more models, smaller datasets.\n",
    "3. **Multinomial** — train a single model for all classes — best integrated softmax approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43bb906",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "[1](https://www.digitalocean.com/community/tutorials/logistic-regression-with-scikit-learn?utm_source=chatgpt.com \"Mastering Logistic Regression with Scikit-Learn: A Complete Guide | DigitalOcean\")\n",
    "[2](https://www.geeksforgeeks.org/machine-learning/one-vs-rest-strategy-for-multi-class-classification/?utm_source=chatgpt.com \"One-vs-Rest strategy for Multi-Class Classification - GeeksforGeeks\")\n",
    "[3](https://scikit-learn.org/1.7/auto_examples/linear_model/plot_logistic_multinomial.html?utm_source=chatgpt.com \"Decision Boundaries of Multinomial and One-vs-Rest Logistic Regression — scikit-learn 1.7.2 documentation\")\n",
    "[4](https://scikit-learn.org/0.16//_downloads/scikit-learn-docs.pdf?utm_source=chatgpt.com \"scikit-learn user guide\")\n",
    "[5](https://www.geeksforgeeks.org/artificial-intelligence/multiclass-logistic-regression/?utm_source=chatgpt.com \"Multiclass logistic regression - GeeksforGeeks\")\n",
    "[6](https://www.geeksforgeeks.org/plot-multinomial-and-one-vs-rest-logistic-regression-in-scikit-learn/?utm_source=chatgpt.com \"Plot Multinomial and One-vs-Rest Logistic Regression in Scikit Learn - GeeksforGeeks\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
