{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dccee49c",
   "metadata": {},
   "source": [
    "A maximum margin classifier is a linear classifier that chooses the separating line / hyperplane that is **as far as possible from all training points**, and it’s the core geometric idea behind SVMs. [bioinformatics-training.github](https://bioinformatics-training.github.io/intro-machine-learning-2017/svm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37bbf77",
   "metadata": {},
   "source": [
    "### 1. Geometric picture: hyperplanes, margin, and support vectors\n",
    "\n",
    "- In a binary classification problem, many different hyperplanes can separate the two classes perfectly if the data are linearly separable. [en.wikipedia](https://en.wikipedia.org/wiki/Margin_(machine_learning))\n",
    "\n",
    "- Each separating hyperplane has a **margin**: the perpendicular distance from the hyperplane to the **closest** training points in either class. [courses.grainger.illinois](https://courses.grainger.illinois.edu/cs446/sp2015/Slides/Lecture10.pdf)\n",
    "\n",
    "- The **maximum‑margin hyperplane** is the separating hyperplane whose margin is largest; the classifier it defines is the **maximum margin classifier**. [alan-turing-institute.github](https://alan-turing-institute.github.io/Intro-to-transparent-ML-course/08-glm-svm/support-vec-classifier.html)\n",
    "\n",
    "Key terms:\n",
    "\n",
    "- **Hyperplane**:  \n",
    "  - Line in 2D, plane in 3D, or flat surface of dimension $p-1$ in $p$-dimensional space. [bioinformatics-training.github](https://bioinformatics-training.github.io/intro-machine-learning-2017/svm.html)\n",
    "\n",
    "- **Support vectors**:  \n",
    "  - The training points that lie exactly on the margin boundaries (the parallel hyperplanes closest to the decision boundary). [pages.hmc](https://pages.hmc.edu/ruye/MachineLearning/lectures/ch9/node6.html)\n",
    "  - They alone “support” or determine the position of the maximum‑margin hyperplane.\n",
    "\n",
    "- The **margin region** is the band between two parallel lines (or hyperplanes) that pass through the support vectors, one for each class. [courses.grainger.illinois](https://courses.grainger.illinois.edu/cs446/sp2015/Slides/Lecture10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38e022",
   "metadata": {},
   "source": [
    "### 2. Why maximum margin is a good idea\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- Test points are likely to appear **near** the training data.\n",
    "\n",
    "- A decision boundary that sits right next to points is fragile: small noise or slight shifts can make new points fall on the wrong side.\n",
    "\n",
    "- A boundary that is as **far away as possible** from all training points (large margin) tends to be **more stable** and generalize better to unseen data. [alan-turing-institute.github](https://alan-turing-institute.github.io/Intro-to-transparent-ML-course/08-glm-svm/support-vec-classifier.html)\n",
    "\n",
    "This leads to the principle:\n",
    "\n",
    "> Among all linear separators that correctly classify the training data, choose the one with the **largest margin**.\n",
    "\n",
    "That is exactly what the maximum margin classifier does. [en.wikipedia](https://en.wikipedia.org/wiki/Margin_(machine_learning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c384aa4",
   "metadata": {},
   "source": [
    "### 3. Linear model in feature space and normal vector β\n",
    "\n",
    "We work in a (possibly nonlinear) feature space using a feature map $\\phi(x)$:\n",
    "\n",
    "- Features: $\\phi_1(x), \\dots, \\phi_{M-1}(x)$.  \n",
    "- Linear model:  \n",
    "  $$\n",
    "  y(x) = \\beta_0 + \\beta_1 \\phi_1(x) + \\cdots + \\beta_{M-1} \\phi_{M-1}(x).\n",
    "  $$\n",
    "\n",
    "For convenience:\n",
    "\n",
    "- Put the coefficients (except bias) into a vector  \n",
    "  $\\beta = [\\beta_1, \\dots, \\beta_{M-1}]^\\top$.  \n",
    "- Put the features into a vector  \n",
    "  $\\phi(x) = [\\phi_1(x), \\dots, \\phi_{M-1}(x)]^\\top$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "y(x) = \\beta_0 + \\beta^\\top \\phi(x).\n",
    "$$\n",
    "\n",
    "The **decision boundary** is the set of points where $y(x) = 0$, i.e.,\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\beta^\\top \\phi(x) = 0.\n",
    "$$\n",
    "\n",
    "Geometric fact:\n",
    "\n",
    "- The vector $\\beta$ is **perpendicular (normal)** to this decision boundary in feature space. [pages.hmc](https://pages.hmc.edu/ruye/MachineLearning/lectures/ch9/node6.html)\n",
    "- This is why rewriting the model in terms of $\\beta$ and $\\phi(x)$ (with bias separated) is useful: the size and direction of $\\beta$ directly control the orientation and margin.\n",
    "\n",
    "A line intersecting the $x_1$ and $x_2$ axes (at 1 and 2) is just a geometric demonstration that $\\beta$ points normal to the boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583896bf",
   "metadata": {},
   "source": [
    "### 4. 1D intuition: constraints and margin\n",
    "\n",
    "To build intuition, consider a 1D feature $\\phi_1(x)$ and two classes:\n",
    "\n",
    "- Encode the positive (green) class as $+1$.  \n",
    "- Encode the negative (orange) class as $-1$.  \n",
    "\n",
    "The model is:\n",
    "\n",
    "$$\n",
    "y(x) = \\beta_1 \\phi_1(x) + \\beta_0.\n",
    "$$\n",
    "\n",
    "- On a simple 1D plot, this is a straight line.\n",
    "- The **decision boundary** is where $y(x) = 0$.  \n",
    "- The **margin** is the horizontal distance from this decision point to the closest data point.\n",
    "\n",
    "We want to find the line that **maximizes that distance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e19ff",
   "metadata": {},
   "source": [
    "#### 4.1 Hard constraints for separable data\n",
    "\n",
    "To **exclude bad solutions** that come too close to points, we introduce inequalities:\n",
    "\n",
    "- For a green point $x_i$ with label $y_i = +1$:  \n",
    "  $$\n",
    "  \\beta_1 \\phi_1(x_i) + \\beta_0 \\ge 1.\n",
    "  $$\n",
    "- For an orange point $x_i$ with label $y_i = -1$:  \n",
    "  $$\n",
    "  \\beta_1 \\phi_1(x_i) + \\beta_0 \\le -1.\n",
    "  $$\n",
    "\n",
    "These say:\n",
    "\n",
    "- Positives lie **above** the decision boundary by at least 1 in model output.\n",
    "- Negatives lie **below** by at least 1.\n",
    "\n",
    "You can combine both into one condition using the label $y_i$:\n",
    "\n",
    "$$\n",
    "y_i \\big( \\beta_1 \\phi_1(x_i) + \\beta_0 \\big) \\ge 1 \\quad \\forall i.\n",
    "$$\n",
    "\n",
    "- If $y_i = +1$ (green), this reduces to $\\beta_1 \\phi_1(x_i) + \\beta_0 \\ge 1$.\n",
    "- If $y_i = -1$ (orange), multiplying reverses the inequality sign and gives $\\beta_1 \\phi_1(x_i) + \\beta_0 \\le -1$.\n",
    "\n",
    "These are the **hard‑margin constraints** in 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0664fa",
   "metadata": {},
   "source": [
    "#### 4.2 Maximizing margin via slope\n",
    "\n",
    "For all lines that satisfy these constraints, the **margin** in 1D is inversely related to $|\\beta_1|$:\n",
    "\n",
    "- A **steeper** slope (large $|\\beta_1|$) means a smaller margin.\n",
    "- A **flatter** slope (small $|\\beta_1|$) means a larger margin, while still respecting the constraints.\n",
    "\n",
    "So the optimization becomes:\n",
    "\n",
    "$$\n",
    "\\text{Minimize } |\\beta_1|\n",
    "\\quad \\text{subject to } y_i (\\beta_1 \\phi_1(x_i) + \\beta_0) \\ge 1 \\quad \\forall i.\n",
    "$$\n",
    "\n",
    "If positives are on the other side (green left, orange right), the sign of $\\beta_1$ flips, but minimizing $|\\beta_1|$ still means “find the **shallowest** separating line,” regardless of whether it slopes up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61af42d",
   "metadata": {},
   "source": [
    "### 5. Full multidimensional formulation\n",
    "\n",
    "In higher dimensions, $\\beta$ is a vector and the margin is inversely proportional to $\\|\\beta\\|$. [cs.cornell](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html)\n",
    "\n",
    "The general **hard‑margin maximum margin classifier** problem is:\n",
    "\n",
    "$$\n",
    "\\text{Minimize } \\|\\beta\\|^2\n",
    "\\quad\\text{subject to } y_i (\\beta^\\top \\phi(x_i) + \\beta_0) \\ge 1 \\quad \\forall i,\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\phi(x_i)$ is the feature vector for point $x_i$.\n",
    "- $y_i \\in \\{+1, -1\\}$ is the class label.\n",
    "- $\\beta$ is the normal vector to the decision hyperplane.\n",
    "- $\\beta_0$ is the bias.\n",
    "\n",
    "We minimize $\\|\\beta\\|^2$ instead of $\\|\\beta\\|$ because:\n",
    "\n",
    "- They have the same minimizer (square is monotonic on nonnegative numbers).\n",
    "- $\\|\\beta\\|^2$ makes the optimization problem smoother and easier to solve numerically.\n",
    "\n",
    "This is a **convex optimization** problem (quadratic objective, linear constraints), which is important because convex problems can be solved efficiently and have a unique global optimum. [cs.cornell](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275c133",
   "metadata": {},
   "source": [
    "### 6. Support vectors: only a few points matter\n",
    "\n",
    "When we solve this optimization problem:\n",
    "\n",
    "- Only some training points lie exactly on the margin boundaries (where $y_i (\\beta^\\top \\phi(x_i) + \\beta_0) = 1$).\n",
    "\n",
    "- These points are the **support vectors**. [alan-turing-institute.github](https://alan-turing-institute.github.io/Intro-to-transparent-ML-course/08-glm-svm/support-vec-classifier.html)\n",
    "\n",
    "- All other points satisfy the inequality strictly with some margin to spare.\n",
    "\n",
    "Crucial property:\n",
    "\n",
    "- The final decision boundary **depends only on the support vectors**. \n",
    "\n",
    "- Moving non‑support points slightly won’t change the boundary as long as they stay outside the margin region.\n",
    "\n",
    "Contrast with:\n",
    "\n",
    "- **Linear regression** and **logistic regression**: the solution typically depends on **all** training points simultaneously. [alan-turing-institute.github](https://alan-turing-institute.github.io/Intro-to-transparent-ML-course/08-glm-svm/support-vec-classifier.html)\n",
    "\n",
    "- SVM / maximum margin classifier: more **sparse** and robust, which is why it works especially well with the **kernel trick**. [cs.cornell](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54dfc68",
   "metadata": {},
   "source": [
    "### 7. Non‑separable case: soft margin, slack variables, and C\n",
    "\n",
    "The above derivation assumed the classes are perfectly separable by some hyperplane (no overlap).  \n",
    "In practice, data are often noisy or overlapping, so strict constraints\n",
    "\n",
    "$$\n",
    "y_i (\\beta^\\top \\phi(x_i) + \\beta_0) \\ge 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "might be impossible to satisfy. [pub.aimind](https://pub.aimind.so/soft-margin-svm-exploring-slack-variables-the-c-parameter-and-flexibility-1555f4834ecc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e546c",
   "metadata": {},
   "source": [
    "#### 7.1 Slack variables\n",
    "\n",
    "To handle overlapping or noisy data, we introduce **slack variables** $\\delta_i \\ge 0$:\n",
    "\n",
    "- They allow some constraints to be “relaxed” by sliding the class boundaries inward.\n",
    "- The constraint becomes:\n",
    "\n",
    "$$\n",
    "y_i (\\beta^\\top \\phi(x_i) + \\beta_0) \\ge 1 - \\delta_i, \\quad \\delta_i \\ge 0 \\quad \\forall i.\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- $\\delta_i = 0$: point is on or outside the correct margin boundary.\n",
    "- $0 < \\delta_i < 1$: point is inside the margin but still correctly classified.\n",
    "- $\\delta_i > 1$: point is misclassified (on the wrong side of the decision boundary). [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/using-a-hard-margin-vs-soft-margin-in-svm/)\n",
    "\n",
    "In the rod analogy from the transcript:\n",
    "\n",
    "- You imagine rigid “rods” at margin level $+1$ for positive and $-1$ for negative.\n",
    "- When data overlap, some rods must be shifted (by $\\delta_i$) to allow **any** separating line to pass throug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7093e",
   "metadata": {},
   "source": [
    "#### 7.2 Soft‑margin objective with C\n",
    "\n",
    "We now need to balance:\n",
    "\n",
    "1. Having a **large margin** (small $\\|\\beta\\|^2$), and  \n",
    "2. Allowing **few and small violations** (small $\\sum \\delta_i$).\n",
    "\n",
    "The **soft‑margin SVM** objective becomes: [people.eecs.berkeley](https://people.eecs.berkeley.edu/~jrs/189s20/lec/04.pdf)\n",
    "\n",
    "$$\n",
    "\\text{Minimize } \\|\\beta\\|^2 + C \\sum_{i=1}^N \\delta_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i (\\beta^\\top \\phi(x_i) + \\beta_0) \\ge 1 - \\delta_i, \\quad \\delta_i \\ge 0 \\quad \\forall i.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $C > 0$ is a **hyperparameter** controlling the trade‑off between margin size and constraint violations. [people.eecs.berkeley](https://people.eecs.berkeley.edu/~jrs/189s20/lec/04.pdf)\n",
    "- Large $C$:\n",
    "  - Strongly penalizes violations.\n",
    "  - Favors low training error but can reduce margin (more complex boundary; higher risk of overfitting). [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/using-a-hard-margin-vs-soft-margin-in-svm/)\n",
    "- Small $C$:\n",
    "  - Allows more violations.\n",
    "  - Favors a larger margin and more regularization (simpler boundary; can underfit).  \n",
    "\n",
    "This soft‑margin formulation is still **convex**, just with more variables ($\\beta, \\beta_0, \\delta_i$), so it remains tractable. [people.eecs.berkeley](https://people.eecs.berkeley.edu/~jrs/189s20/lec/04.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565588b",
   "metadata": {},
   "source": [
    "### 8. Relation to SVMs and the kernel trick\n",
    "\n",
    "The **maximum margin classifier** described here is the geometric and optimization foundation of **support vector machines (SVMs)**: [en.wikipedia](https://en.wikipedia.org/wiki/Margin_(machine_learning))\n",
    "\n",
    "- Hard‑margin SVM = maximum margin classifier when data is perfectly separable.\n",
    "- Soft‑margin SVM = maximum margin classifier with slack variables and $C$ when data overlaps.\n",
    "\n",
    "Because:\n",
    "\n",
    "- Only **support vectors** matter, and  \n",
    "- The solution has a dual form that uses only inner products $\\phi(x_i)^\\top \\phi(x_j)$,  \n",
    "\n",
    "we can apply the **kernel trick**:\n",
    "\n",
    "- Replace $\\phi(x_i)^\\top \\phi(x_j)$ with a **kernel function** $k(x_i, x_j)$ (e.g., polynomial, RBF).  \n",
    "- This yields nonlinear decision boundaries in the original input space while still solving a convex optimization problem in terms of α‑coefficients and kernels. [pages.hmc](https://pages.hmc.edu/ruye/MachineLearning/lectures/ch9/node6.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
