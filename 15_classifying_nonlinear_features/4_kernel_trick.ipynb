{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ad4e91",
   "metadata": {},
   "source": [
    "A **kernel** lets you solve nonlinear problems using methods that were originally designed for linear models, by working with a similarity function between points instead of explicit coordinates in a feature space. \n",
    "\n",
    "The idea is developed first for regularized linear regression, then generalized toward the “kernel trick” used in SVMs and other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ec2f6",
   "metadata": {},
   "source": [
    "### 1. Intuitive picture: why kernels?\n",
    "\n",
    "#### 1.1 Linear vs nonlinear separation\n",
    "\n",
    "- A linear model (like plain linear regression or logistic regression) uses a prediction of the form  \n",
    "  $\\hat{y}(x) = \\beta^\\top \\phi(x)$,  \n",
    "  where $\\phi(x)$ is a vector of features (maybe just the original inputs, maybe some nonlinear transforms).\n",
    "- In a 2D plot, such a model produces **straight** decision boundaries (lines).  \n",
    "- Many real datasets are **not** linearly separable in the original space: you might need a curve or complex boundary.\n",
    "\n",
    "A common trick is to **map the data into a higher-dimensional feature space** using nonlinear features—e.g., quadratics, cubics, trigonometric functions—where a straight boundary becomes possible.\n",
    "\n",
    "Example (conceptual):\n",
    "\n",
    "- In 2D, points of one class form a ring around the origin; the other class is in the center.\n",
    "- No straight line can separate them in 2D.\n",
    "- If you add the feature $r^2 = x_1^2 + x_2^2$, in the new space $(x_1, x_2, r^2)$ you can separate them using a hyperplane.\n",
    "\n",
    "Kernels formalize and generalize this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a7743",
   "metadata": {},
   "source": [
    "#### 1.2 What is a kernel?\n",
    "\n",
    "A **kernel** is a function $k(x, x')$ that takes two data vectors and returns a single number measuring their **similarity** in some (possibly high-dimensional) feature space.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- You can think of a kernel as computing  \n",
    "  $k(x, x') = \\phi(x)^\\top \\phi(x')$  \n",
    "  for some feature map $\\phi$, even if you never explicitly compute $\\phi(x)$.\n",
    "- Different kernels correspond to different choices of $\\phi$ and, therefore, different kinds of nonlinear structure the model can capture.\n",
    "\n",
    "Common kernels (for SVM, etc.):\n",
    "\n",
    "- Linear: $k(x, x') = x^\\top x'$ (no extra features)\n",
    "- Polynomial: $k(x, x') = (\\gamma x^\\top x' + r)^d$\n",
    "- RBF/Gaussian: $k(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$\n",
    "\n",
    "The **kernel trick**: if an algorithm can be written purely in terms of inner products $x_i^\\top x_j$, you can replace those inner products with a kernel $k(x_i, x_j)$. That implicitly moves you to a high-dimensional feature space without ever computing coordinates there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7e588",
   "metadata": {},
   "source": [
    "### 2. Linear regression with nonlinear features (primal view)\n",
    "\n",
    "Start with **regularized linear regression** with nonlinear features, because the math is clean and the structure generalizes to other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493b1f1",
   "metadata": {},
   "source": [
    "#### 2.1 Model and notation\n",
    "\n",
    "We have data:\n",
    "\n",
    "- Inputs: $x_1, \\dots, x_N$\n",
    "- Outputs: $y_1, \\dots, y_N$\n",
    "\n",
    "We define a feature map $\\phi(x)$ (vector of $M$ features):\n",
    "\n",
    "- The 0th feature is always $1$ (bias).\n",
    "- The remaining $M - 1$ features are nonlinear transforms of $x$ (e.g., polynomials, etc.).\n",
    "\n",
    "We collect:\n",
    "\n",
    "- Parameter vector $\\beta \\in \\mathbb{R}^M$: $\\beta = (\\beta_0, \\dots, \\beta_{M-1})^\\top$\n",
    "- Feature vector for sample $i$: $\\phi(x_i) \\in \\mathbb{R}^M$\n",
    "\n",
    "The model is:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_i) = \\beta^\\top \\phi(x_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99266dc",
   "metadata": {},
   "source": [
    "#### 2.2 Regularized least squares loss\n",
    "\n",
    "We want to find $\\beta$ that minimizes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta)\n",
    "= \\sum_{i=1}^N (\\beta^\\top \\phi(x_i) - y_i)^2\n",
    "+ \\frac{\\lambda}{2} \\|\\beta\\|^2,\n",
    "$$\n",
    "\n",
    "where $\\lambda > 0$ is a regularization strength:\n",
    "\n",
    "- First term: sum of squared errors.\n",
    "- Second term: L2 penalty (ridge) on coefficients, discouraging very large $\\beta$.\n",
    "\n",
    "The factor $\\frac{\\lambda}{2}$ is just for algebraic convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fb5b7",
   "metadata": {},
   "source": [
    "#### 2.3 Matrix form (primal / β-formulation)\n",
    "\n",
    "Define:\n",
    "\n",
    "- Feature matrix $\\Phi \\in \\mathbb{R}^{N \\times M}$: each row is $\\phi(x_i)^\\top$.\n",
    "- Target vector $Y \\in \\mathbb{R}^N$: entries $y_i$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta)\n",
    "= \\|\\Phi \\beta - Y\\|^2 + \\frac{\\lambda}{2} \\|\\beta\\|^2.\n",
    "$$\n",
    "\n",
    "This is a convex quadratic in $\\beta$. The minimizer is found by setting the derivative to zero.\n",
    "\n",
    "Compute gradient and set to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\beta}\n",
    "= 2\\Phi^\\top (\\Phi\\beta - Y) + \\lambda \\beta = 0.\n",
    "$$\n",
    "\n",
    "Rearrange:\n",
    "\n",
    "$$\n",
    "(\\Phi^\\top \\Phi + \\lambda I)\\beta = \\Phi^\\top Y.\n",
    "$$\n",
    "\n",
    "So the **closed-form solution** (primal solution) is:\n",
    "\n",
    "$$\n",
    "\\beta^\\ = (\\Phi^\\top \\Phi + \\lambda I)^{-1} \\Phi^\\top Y.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $\\Phi^\\top \\Phi$ is an $M \\times M$ matrix.\n",
    "- You invert an $M \\times M$ matrix: complexity ~ $O(M^3)$.\n",
    "\n",
    "Prediction for a new point $x_{\\text{new}}$:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_{\\text{new}}) = \\beta^{*\\top} \\phi(x_{\\text{new}}).\n",
    "$$\n",
    "\n",
    "Notice:\n",
    "\n",
    "- After computing $\\beta^{*\\top}$, you can forget the training data and just keep $\\beta^{*\\top}$.\n",
    "- This is the **primal**, or **β-based**, viewpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18885d",
   "metadata": {},
   "source": [
    "### 3. Alternative formulation: α-parameters (dual-like view)\n",
    "\n",
    "An alternative set of parameters $\\alpha_i$, one per data point, to expose where kernels will appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7416cc",
   "metadata": {},
   "source": [
    "#### 3.1 Define α in terms of errors\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "\\alpha_i = -\\frac{\\beta^\\top \\phi(x_i) - y_i}{\\lambda}.\n",
    "$$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{1}{\\lambda} (y_i - \\beta^\\top \\phi(x_i)).\n",
    "$$\n",
    "\n",
    "So $\\alpha_i$ is (up to scaling) the negative prediction error for point $i$.\n",
    "\n",
    "We can write:\n",
    "\n",
    "- Vector $\\alpha \\in \\mathbb{R}^N$: entries $\\alpha_i$.\n",
    "\n",
    "From the gradient condition:\n",
    "\n",
    "$$\n",
    "2\\Phi^\\top (\\Phi\\beta - Y) + \\lambda \\beta = 0\n",
    "$$\n",
    "\n",
    "Divide by 2 for simplicity:\n",
    "\n",
    "$$\n",
    "\\Phi^\\top (\\Phi\\beta - Y) + \\frac{\\lambda}{2}\\beta = 0\n",
    "$$\n",
    "\n",
    "But it’s simpler to view the derivation in the way presented in the transcript:\n",
    "\n",
    "Using the definition of $\\alpha_i$ and some algebra, you can show:\n",
    "\n",
    "$$\n",
    "\\beta = \\Phi^\\top \\alpha.\n",
    "$$\n",
    "\n",
    "This says: **β is a linear combination of the feature vectors of the training points**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4ee34",
   "metadata": {},
   "source": [
    "#### 3.2 Matrix equations for α\n",
    "\n",
    "We also plug the definition of $\\alpha$ into matrix form:\n",
    "\n",
    "$$\n",
    "- \\lambda \\alpha = \\Phi \\beta - Y.\n",
    "$$\n",
    "\n",
    "Call that equation (4) in the transcript. Combine:\n",
    "\n",
    "- $\\beta = \\Phi^\\top \\alpha$\n",
    "- $-\\lambda \\alpha = \\Phi\\beta - Y$\n",
    "\n",
    "Substitute $\\beta = \\Phi^\\top \\alpha$ into the second:\n",
    "\n",
    "$$\n",
    "- \\lambda \\alpha = \\Phi (\\Phi^\\top \\alpha) - Y = (\\Phi\\Phi^\\top)\\alpha - Y.\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "(\\Phi\\Phi^\\top + \\lambda I)\\alpha = Y.\n",
    "$$\n",
    "\n",
    "Therefore, the **α-solution** is:\n",
    "\n",
    "$$\n",
    "\\alpha^* = (\\Phi\\Phi^\\top + \\lambda I)^{-1} Y.\n",
    "$$\n",
    "\n",
    "Now:\n",
    "\n",
    "- $\\Phi\\Phi^\\top$ is an $N \\times N$ matrix.\n",
    "- You invert an $N \\times N$ matrix: complexity ~ $O(N^3)$.\n",
    "\n",
    "This looks **worse** than the β-formulation if $N \\gg M$ (many more data points than features), which is typical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5c470",
   "metadata": {},
   "source": [
    "#### 3.3 Predictions in terms of α\n",
    "\n",
    "We still know:\n",
    "\n",
    "$$\n",
    "\\beta^* = \\Phi^\\top \\alpha^*.\n",
    "$$\n",
    "\n",
    "So for a new point $x_{\\text{new}}$:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_{\\text{new}}) \n",
    "= \\beta^{*\\top} \\phi(x_{\\text{new}})\n",
    "= \\alpha^{*\\top} \\Phi \\phi(x_{\\text{new}}).\n",
    "$$\n",
    "\n",
    "Expand $\\Phi \\phi(x_{\\text{new}})$:\n",
    "\n",
    "- The $i$-th entry is $\\phi(x_i)^\\top \\phi(x_{\\text{new}})$.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_{\\text{new}}) = \\sum_{i=1}^N \\alpha_i^* \\phi(x_i)^\\top \\phi(x_{\\text{new}}).\n",
    "$$\n",
    "\n",
    "Key observation:\n",
    "\n",
    "- With β-formulation, predictions only need $\\beta$.\n",
    "- With α-formulation, predictions require **all training points** (through the sums).\n",
    "- Even without kernels, this seems computationally and memory-wise worse.\n",
    "\n",
    "So why is this useful? Because of what $\\phi(x_i)^\\top \\phi(x_j)$ really is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffad8c9",
   "metadata": {},
   "source": [
    "### 4. The kernel matrix and kernel function\n",
    "\n",
    "Look at $\\Phi\\Phi^\\top$:\n",
    "\n",
    "- Entry $(i, j)$ is $\\phi(x_i)^\\top \\phi(x_j)$.\n",
    "- This is the **dot product** of feature vectors for points $i$ and $j$.\n",
    "- It measures **similarity** between the two data points **in feature space**.\n",
    "\n",
    "We call:\n",
    "\n",
    "- $K_{ij} = \\phi(x_i)^\\top \\phi(x_j)$  \n",
    "  the **kernel matrix** $K$.\n",
    "- The function  \n",
    "  $k(x, x') = \\phi(x)^\\top \\phi(x')$  \n",
    "  the **kernel function**.\n",
    "\n",
    "In the α-formulation:\n",
    "\n",
    "- Training uses $\\Phi\\Phi^\\top$, i.e., **only dot products** of the form $\\phi(x_i)^\\top \\phi(x_j)$.\n",
    "- Prediction uses sums of $\\phi(x_i)^\\top \\phi(x_{\\text{new}})$, again **only dot products**.\n",
    "\n",
    "This reveals the main insight:\n",
    "\n",
    "> If an algorithm (in this alternative form) only needs dot products of feature vectors $\\phi(x)$, then you never need to know $\\phi(x)$ explicitly—only a function that gives you these dot products.\n",
    "\n",
    "That function is the kernel $k(x, x')$.\n",
    "\n",
    "So we can:\n",
    "\n",
    "- **Replace** $\\phi(x_i)^\\top \\phi(x_j)$ with **any** valid kernel $k(x_i, x_j)$.\n",
    "- This lets us work in a **potentially infinite-dimensional feature space** implicitly.\n",
    "- We avoid ever computing explicit coordinates in that space.\n",
    "\n",
    "This is the **kernel trick**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abe81b",
   "metadata": {},
   "source": [
    "### 5. Why the kernel trick matters\n",
    "\n",
    "#### 5.1 Computation and representation\n",
    "\n",
    "Instead of:\n",
    "\n",
    "- Choosing a finite set of nonlinear features and building $\\phi(x)$ explicitly.\n",
    "- Constructing a potentially huge $\\Phi$ (with possibly thousands or millions of features).\n",
    "- Inverting an $M \\times M$ matrix.\n",
    "\n",
    "We can:\n",
    "\n",
    "- Choose a kernel $k(x, x')$ that corresponds to an **implicit** feature map.\n",
    "- Work directly with the kernel matrix $K$ of size $N \\times N$.\n",
    "- Solve for α and make predictions using only kernel evaluations.\n",
    "\n",
    "Even if the implicit feature space is very high-dimensional (or infinite-dimensional, as with the RBF kernel), computations are done in terms of $N$ and kernel calls, not $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c435b88",
   "metadata": {},
   "source": [
    "#### 5.2 Applicability to many algorithms\n",
    "\n",
    "Kernels are not just for SVMs. Any algorithm that can be written in terms of inner products can be “kernelized,” including:\n",
    "\n",
    "- Linear regression (as shown).\n",
    "- Logistic regression.\n",
    "- Principal Component Analysis (PCA) → kernel PCA.\n",
    "- Others that rely on distances/inner products.\n",
    "\n",
    "The general pattern:\n",
    "\n",
    "1. Rewrite the algorithm in terms of dot products $\\phi(x_i)^\\top \\phi(x_j)$.\n",
    "2. Replace those with $k(x_i, x_j)$.\n",
    "3. Choose a kernel appropriate to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e0ba6",
   "metadata": {},
   "source": [
    "### 6. Simple Python implementations (from basics)\n",
    "\n",
    "Below are simple examples (for intuition) showing:\n",
    "\n",
    "- Primal ridge regression (β-form).\n",
    "- Kernel ridge regression (α-form) using a linear kernel.\n",
    "- How to generalize to, e.g., an RBF kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7e8dc",
   "metadata": {},
   "source": [
    "\n",
    "#### 6.1 Primal ridge regression with nonlinear features\n",
    "\n",
    "We’ll illustrate in 1D for clarity: fitting a nonlinear curve with explicit polynomial features.\n",
    "\n",
    "Here:\n",
    "\n",
    "- We chose the features explicitly (up to degree 3).\n",
    "- We solved for $\\beta$ via $(\\Phi^\\top \\Phi + \\lambda I)^{-1} \\Phi^\\top Y$.\n",
    "- All computations are in the **feature space** via $\\Phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715d7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Generate toy data\n",
    "np.random.seed(0)\n",
    "N = 30\n",
    "X = np.linspace(-3, 3, N).reshape(-1, 1)\n",
    "y = np.sin(X).ravel() + 0.1 * np.random.randn(N)\n",
    "\n",
    "# Build feature matrix Phi with polynomial features [1, x, x^2, x^3]\n",
    "def poly_features(x, degree=3):\n",
    "    # x is shape (N, 1)\n",
    "    N = x.shape[0]\n",
    "    Phi = np.ones((N, degree + 1))  # columns: x^0, x^1, ..., x^degree\n",
    "    for d in range(1, degree + 1):\n",
    "        Phi[:, d] = x[:, 0] ** d\n",
    "    return Phi\n",
    "\n",
    "Phi = poly_features(X, degree=3)\n",
    "Y = y.reshape(-1, 1)\n",
    "\n",
    "lam = 0.1  # regularization strength\n",
    "\n",
    "# Closed-form ridge solution in primal\n",
    "M = Phi.shape[1]\n",
    "beta = inv(Phi.T @ Phi + lam * np.eye(M)) @ Phi.T @ Y  # shape (M, 1)\n",
    "\n",
    "# Prediction for new x values\n",
    "X_new = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "Phi_new = poly_features(X_new, degree=3)\n",
    "y_pred = Phi_new @ beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc4e60",
   "metadata": {},
   "source": [
    "\n",
    "#### 6.2 Kernel ridge regression: linear kernel (as a sanity check)\n",
    "\n",
    "Now we implement the α-form using a kernel.\n",
    "\n",
    "For a **linear kernel** and certain choices of features, the primal and dual give essentially the same function class, but this already shows:\n",
    "\n",
    "- Training involves inverting an $N \\times N$ matrix.\n",
    "- Prediction involves sums of kernel evaluations against all training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93c57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(X1, X2):\n",
    "    # X1: (N, d), X2: (M, d)\n",
    "    return X1 @ X2.T\n",
    "\n",
    "# Kernel matrix K = Phi Phi^T, but here we use X directly\n",
    "K = linear_kernel(X, X)  # shape (N, N)\n",
    "\n",
    "# Solve for alpha: (K + lam * I) alpha = y\n",
    "alpha = np.linalg.inv(K + lam * np.eye(N)) @ y.reshape(-1, 1)  # shape (N, 1)\n",
    "\n",
    "# Predict for new points\n",
    "K_new = linear_kernel(X_new, X)  # (N_new, N)\n",
    "y_pred_kernel = K_new @ alpha  # (N_new, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5a5d0",
   "metadata": {},
   "source": [
    "#### 6.3 Kernel ridge regression with RBF kernel\n",
    "\n",
    "Now replace the linear kernel with a more powerful one, e.g., RBF.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- We never constructed explicit high-dimensional features $\\phi(x)$.\n",
    "- The RBF kernel implicitly corresponds to an **infinite-dimensional** feature space.\n",
    "- Yet training and prediction both use only kernel evaluations.\n",
    "\n",
    "This is a concrete example of the **kernel trick** in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9feb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, gamma=1.0):\n",
    "    # squared Euclidean distance (broadcasting)\n",
    "    # X1: (N, d), X2: (M, d)\n",
    "    X1_sq = np.sum(X1**2, axis=1).reshape(-1, 1)  # (N, 1)\n",
    "    X2_sq = np.sum(X2**2, axis=1).reshape(1, -1)  # (1, M)\n",
    "    sq_dists = X1_sq + X2_sq - 2 * X1 @ X2.T      # (N, M)\n",
    "    return np.exp(-gamma * sq_dists)\n",
    "\n",
    "gamma = 0.5\n",
    "K = rbf_kernel(X, X, gamma=gamma)\n",
    "alpha = np.linalg.inv(K + lam * np.eye(N)) @ y.reshape(-1, 1)\n",
    "\n",
    "K_new = rbf_kernel(X_new, X, gamma=gamma)\n",
    "y_pred_rbf = K_new @ alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727f371",
   "metadata": {},
   "source": [
    "### 7. Connecting to SVMs and beyond\n",
    "\n",
    "In SVMs (which the module will focus on next):\n",
    "\n",
    "- The optimization problem can be written in “dual” form using only dot products of inputs.\n",
    "- Replacing dot products with kernels turns a **linear SVM** into a **nonlinear SVM**.\n",
    "- The resulting decision boundary in the original space can be dramatically nonlinear, while the algorithm itself uses only a kernel function and constraints on α-like coefficients.\n",
    "\n",
    "Similarly, kernel versions exist for:\n",
    "\n",
    "- Logistic regression (kernel logistic regression).\n",
    "- PCA (kernel PCA).\n",
    "- Other algorithms that rely fundamentally on inner products.\n",
    "\n",
    "The big picture:\n",
    "\n",
    "- Many linear algorithms **depend only on geometry** (relative positions, angles, distances).\n",
    "- Geometry can often be captured fully by inner products.\n",
    "- Kernels let us **redefine geometry** in a richer feature space without ever working in that space explicitly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
