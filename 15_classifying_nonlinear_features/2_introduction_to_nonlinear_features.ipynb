{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89a5eef",
   "metadata": {},
   "source": [
    "Nonlinear features let us turn simple linear models into powerful tools that can learn curved decision boundaries and complex patterns, without immediately jumping to “heavy” models like deep nets. [inria.github](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_models_feature_engineering_classification.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbab872",
   "metadata": {},
   "source": [
    "### 1. From linear to nonlinear: the core idea\n",
    "\n",
    "#### 1.1 Linear models and straight boundaries\n",
    "\n",
    "A basic linear classifier (like logistic regression) predicts using a score of the form  \n",
    "$\\text{score}(x) = w_0 + w_1 x_1 + \\dots + w_d x_d$. \n",
    "\n",
    "The decision boundary where the model flips class is given by setting this score to 0, which produces a line in 2D, a plane in 3D, or a hyperplane in higher dimensions. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2022-extra/modules/machine-learning/non-linear-features.pdf)\n",
    "\n",
    "This works well when classes can be separated by such a straight surface, but fails on patterns like:\n",
    "\n",
    "- Circles or rings around a center.\n",
    "- XOR patterns where “different corners” belong to one class.\n",
    "- Curved clusters that bend around each other. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2022-extra/modules/machine-learning/non-linear-features.pdf)\n",
    "\n",
    "In these cases, no single straight line (or hyperplane) can separate the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da8ddc",
   "metadata": {},
   "source": [
    "#### 1.2 Nonlinear features: bending the space\n",
    "\n",
    "The trick is: instead of changing the model, change the **features**.  \n",
    "You build new features from the original ones using nonlinear functions, for example:\n",
    "\n",
    "- Polynomial terms: $x_1^2$, $x_2^2$, $x_1 x_2$. [inria.github](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_models_feature_engineering_classification.html)\n",
    "- Higher powers: $x_1^3$, $x_1^2 x_2$, etc. [data36](https://data36.com/polynomial-regression-python-scikit-learn/)\n",
    "- Other transforms: $\\log(x)$, $\\exp(x)$, $\\sin(x)$, $\\cos(x)$. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2020-extra/modules/machine-learning/non-linear-features.pdf)\n",
    "\n",
    "The model is still linear in these new features, but since the features themselves are nonlinear in the original inputs, the decision boundary in the original space becomes curved. [data36](https://data36.com/polynomial-regression-python-scikit-learn/)\n",
    "\n",
    "Formally: you define a feature map $\\phi(x)$ (vector of nonlinear features) and apply a linear model to $\\phi(x)$ instead of $x$. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2022-extra/modules/machine-learning/non-linear-features.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d798e",
   "metadata": {},
   "source": [
    "### 2. The wine example: understanding the geometry\n",
    "\n",
    "#### 2.1 Dataset and setup\n",
    "\n",
    "The classic scikit-learn wine dataset: 178 wines, each with 12 numeric attributes (alcohol, malic acid, magnesium, total phenols, flavonoids, color intensity, hue, etc.), and a class label 0, 1, or 2 for three wine types. [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)\n",
    "\n",
    "For visualization, pick only:\n",
    "\n",
    "- $x_0$: total_phenols  \n",
    "- $x_1$: color_intensity  \n",
    "\n",
    "and plots the wines in 2D with color-coded classes. [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)\n",
    "\n",
    "From the scatterplot:\n",
    "\n",
    "- Class 1: low color_intensity (bottom of the plot).\n",
    "- Class 2: higher color_intensity but low total_phenols.\n",
    "- Class 0: higher color_intensity and higher total_phenols than class 2. [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)\n",
    "\n",
    "So each class occupies a different region, and these regions are clearly not separated by a single straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea34c3",
   "metadata": {},
   "source": [
    "#### 2.2 Trying existing models\n",
    "\n",
    "Compare:\n",
    "\n",
    "- A decision tree of depth 2.\n",
    "- k-nearest neighbors with k=5.\n",
    "- Multinomial logistic regression (a linear model in the original features). [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)\n",
    "\n",
    "Observations:\n",
    "\n",
    "- **Decision tree (depth=2)**  \n",
    "  - Splits the plane with a small number of axis-aligned cuts:  \n",
    "    “If color_intensity < threshold ⇒ class 1; else if total_phenols < threshold ⇒ class 2; else ⇒ class 0.”  \n",
    "  - Matches human intuition and is easy to explain. [emagine](https://www.emagine.org/blogs/example-of-non-linear-machine-learning-algorithms-decision-trees/)\n",
    "  - If you increase depth (say to 10), you get many tiny, irregular rectangles and overfitting. [emagine](https://www.emagine.org/blogs/example-of-non-linear-machine-learning-algorithms-decision-trees/)\n",
    "\n",
    "- **k-nearest neighbors (k=5)**  \n",
    "  - Decision boundary “wraps” around points, making many tight turns to classify training examples correctly. [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)\n",
    "  - High training accuracy but very wiggly boundaries → high variance, likely overfitting. [willett.psd.uchicago](https://willett.psd.uchicago.edu/research/nonlinear-models-in-machine-learning/)\n",
    "\n",
    "- **Multinomial logistic regression (linear features only)**  \n",
    "  - Very stable; main hyperparameter is regularization strength, but here changing it doesn’t change the boundary much. [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)\n",
    "  - Boundaries are almost straight lines, so the model underfits the curved structure and misclassifies many points. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2020-extra/modules/machine-learning/non-linear-features.pdf)\n",
    "\n",
    "This motivates nonlinear features: we want something as stable and well-behaved as logistic regression, but flexible enough to bend the boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99bbd2",
   "metadata": {},
   "source": [
    "### 3. Building nonlinear features in the wine example\n",
    "\n",
    "#### 3.1 From two linear features to five nonlinear ones\n",
    "\n",
    "We start with the two original features:\n",
    "\n",
    "- $x_0$: total_phenols  \n",
    "- $x_1$: color_intensity. [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)\n",
    "\n",
    "They are **linear** features because they appear to the first power in the model. Then construct three **quadratic** features:\n",
    "\n",
    "- $\\phi_0 = x_0 x_1$ (interaction term).\n",
    "- $\\phi_1 = x_0^2$.\n",
    "- $\\phi_2 = x_1^2$. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2020-extra/modules/machine-learning/non-linear-features.pdf)\n",
    "\n",
    "The new feature vector is:\n",
    "\n",
    "$$\n",
    "\\phi(x) = [x_0, x_1, x_0 x_1, x_0^2, x_1^2].\n",
    "$$\n",
    "\n",
    "We build a new DataFrame $X$ containing these five columns and keep the class label as the target. [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739719b",
   "metadata": {},
   "source": [
    "### 3.2 Effect on the decision boundary\n",
    "\n",
    "Training multinomial logistic regression on these five features gives decision boundaries that are no longer straight lines in the original 2D plot. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2020-extra/modules/machine-learning/non-linear-features.pdf)\n",
    "\n",
    "Instead, they become smooth curves that more closely wrap around the three clusters, improving classification accuracy compared with the purely linear case. [inria.github](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_models_feature_engineering_classification.html)\n",
    "\n",
    "Mathematically, the model is still linear in $\\phi(x)$ (it’s logistic regression), but the mapping $x \\mapsto \\phi(x)$ is nonlinear, so the boundary in the original $x_0$-$x_1$ space is curved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e50a3e",
   "metadata": {},
   "source": [
    "### 4. Coding it in Python (scikit-learn)\n",
    "\n",
    "#### 4.1 Loading and preparing the wine data\n",
    "\n",
    "Below is a beginner-friendly implementation that mirrors the transcript’s logic using scikit-learn’s wine dataset. [stackoverflow](https://stackoverflow.com/questions/55937244/how-to-implement-polynomial-logistic-regression-in-scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f16901d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     total_phenols  color_intensity\n",
      "12            2.60             5.60\n",
      "30            3.00             5.70\n",
      "36            2.60             4.60\n",
      "31            2.86             6.90\n",
      "120           2.90             3.25\n",
      "[0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the wine dataset\n",
    "data = load_wine()\n",
    "X_all = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target  # 0, 1, or 2\n",
    "\n",
    "# 2. Keep only two features: total_phenols and color_intensity\n",
    "X_two = X_all[[\"total_phenols\", \"color_intensity\"]]\n",
    "\n",
    "# Train/test split for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_two, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe653ed",
   "metadata": {},
   "source": [
    "This replicates the “reduced table” with two features plus the class column  [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee42565",
   "metadata": {},
   "source": [
    "#### 4.2 Plain multinomial logistic regression (linear features)\n",
    "This gives the “straight boundary” version. [stat.cmu](https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4da65a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear logistic regression accuracy: 0.8703703703703703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinny/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "linear_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"log_reg\", LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000\n",
    "    )),\n",
    "])\n",
    "\n",
    "linear_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lin = linear_clf.predict(X_test)\n",
    "print(\"Linear logistic regression accuracy:\", accuracy_score(y_test, y_pred_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7538023",
   "metadata": {},
   "source": [
    "#### 4.3 Adding quadratic nonlinear features\n",
    "\n",
    "You can manually create the three quadratic features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1bd127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with quadratic features accuracy: 0.8703703703703703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinny/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_nl = X_train.copy()\n",
    "X_train_nl[\"phi0\"] = X_train_nl[\"total_phenols\"] * X_train_nl[\"color_intensity\"]\n",
    "X_train_nl[\"phi1\"] = X_train_nl[\"total_phenols\"] ** 2\n",
    "X_train_nl[\"phi2\"] = X_train_nl[\"color_intensity\"] ** 2\n",
    "\n",
    "X_test_nl = X_test.copy()\n",
    "X_test_nl[\"phi0\"] = X_test_nl[\"total_phenols\"] * X_test_nl[\"color_intensity\"]\n",
    "X_test_nl[\"phi1\"] = X_test_nl[\"total_phenols\"] ** 2\n",
    "X_test_nl[\"phi2\"] = X_test_nl[\"color_intensity\"] ** 2\n",
    "\n",
    "nonlinear_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"log_reg\", LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000\n",
    "    )),\n",
    "])\n",
    "\n",
    "nonlinear_clf.fit(X_train_nl, y_train)\n",
    "\n",
    "y_pred_nl = nonlinear_clf.predict(X_test_nl)\n",
    "print(\"Logistic regression with quadratic features accuracy:\", accuracy_score(y_test, y_pred_nl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6867ae6",
   "metadata": {},
   "source": [
    "Or, more generally, use `PolynomialFeatures` to automatically generate all polynomial terms up to a chosen degree: [stackoverflow](https://stackoverflow.com/questions/55937244/how-to-implement-polynomial-logistic-regression-in-scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52aaedc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with PolynomialFeatures (degree=2): 0.8703703703703703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinny/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "poly_clf = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"log_reg\", LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000\n",
    "    )),\n",
    "])\n",
    "\n",
    "poly_clf.fit(X_train, y_train)\n",
    "y_pred_poly = poly_clf.predict(X_test)\n",
    "print(\"Logistic regression with PolynomialFeatures (degree=2):\",\n",
    "      accuracy_score(y_test, y_pred_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3e039",
   "metadata": {},
   "source": [
    "`PolynomialFeatures(degree=2)` will generate exactly the original features, their squares, and pairwise products (interaction terms), analogous to the hand-built $\\phi_0$, $\\phi_1$, $\\phi_2$. [stackoverflow](https://stackoverflow.com/questions/55937244/how-to-implement-polynomial-logistic-regression-in-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79281c7a",
   "metadata": {},
   "source": [
    "### 5. Exploring richer nonlinear feature families\n",
    "\n",
    "The transcript mentions several families of nonlinear features that can be tried in exactly the same way: add columns, then fit logistic regression. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2020-extra/modules/machine-learning/non-linear-features.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b3dcdd",
   "metadata": {},
   "source": [
    "#### 5.1 Higher-degree polynomials\n",
    "\n",
    "You can go beyond degree 2:\n",
    "\n",
    "- Degree 3: includes terms like $x_0^3$, $x_0^2 x_1$, $x_0 x_1^2$, $x_1^3$. [data36](https://data36.com/polynomial-regression-python-scikit-learn/)\n",
    "- Higher degrees: more complex shapes but also higher risk of overfitting. [data36](https://data36.com/polynomial-regression-python-scikit-learn/)\n",
    "\n",
    "Code example: [inria.github](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_models_feature_engineering_classification.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b959377",
   "metadata": {},
   "source": [
    "#### 5.2 Exponential and log features\n",
    "\n",
    "For some problems, exponential or logarithmic growth fits domain knowledge:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe04d9d",
   "metadata": {},
   "source": [
    "#### 5.3 Trigonometric and Fourier features\n",
    "\n",
    "Trigonometric features are useful when you suspect periodicity or want very flexible wavy boundaries. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2022-extra/modules/machine-learning/non-linear-features.pdf)\n",
    "\n",
    "Fourier features with multiple frequencies produce “wavy” decision boundaries. [stanford-cs221.github](https://stanford-cs221.github.io/autumn2020-extra/modules/machine-learning/non-linear-features.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fd347",
   "metadata": {},
   "source": [
    "### 6. How do we choose which nonlinear features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fbb9c",
   "metadata": {},
   "source": [
    "#### 6.1 Domain knowledge\n",
    "\n",
    "If you know something about your problem, use it:\n",
    "\n",
    "- Biology/chemistry → saturating or sigmoidal relationships, logs, and exponentials may make sense.\n",
    "- Physics → polynomials, inverse-square laws, or specific formulas.\n",
    "- Periodic phenomena → sinusoids and Fourier features. [aml4td](https://aml4td.org/chapters/interactions-nonlinear.html)\n",
    "\n",
    "This reduces the search space to functions that reflect real-world behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153855a",
   "metadata": {},
   "source": [
    "#### 6.2 Regularization and feature selection\n",
    "\n",
    "You can also start with many candidate nonlinear features and let regularization prune them. [aml4td](https://aml4td.org/chapters/interactions-nonlinear.html)\n",
    "\n",
    "- L2 regularization (ridge): shrinks all coefficients but rarely sets them exactly to zero. Helps control overfitting but doesn’t explicitly drop features. [aml4td](https://aml4td.org/chapters/interactions-nonlinear.html)\n",
    "- L1 regularization (LASSO): encourages many coefficients to be exactly zero, effectively selecting a subset of features and ranking them by importance. [aml4td](https://aml4td.org/chapters/interactions-nonlinear.html)\n",
    "\n",
    "Using LASSO to **rank features by importance** and then trimming them by increasing the regularization weight. [aml4td](https://aml4td.org/chapters/interactions-nonlinear.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3eaa1b",
   "metadata": {},
   "source": [
    "### 7. Where SVMs with kernels fit into this story\n",
    "\n",
    "- Instead of explicitly constructing nonlinear features $\\phi(x)$, use a **kernel function** $k(x, x')$ that computes inner products in an implicit high-dimensional feature space. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/linear-vs-non-linear-classification-analyzing-differences-using-the-kernel-trick/)\n",
    "- You solve for the SVM in terms of these kernel evaluations; the feature mapping can even be infinite-dimensional, but you never explicitly construct it. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/linear-vs-non-linear-classification-analyzing-differences-using-the-kernel-trick/)\n",
    "\n",
    "Common kernels:\n",
    "\n",
    "- Polynomial kernel: $k(x, x') = (\\gamma x^\\top x' + r)^d$. Captures polynomial interactions up to degree $d$. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/linear-vs-non-linear-classification-analyzing-differences-using-the-kernel-trick/)\n",
    "- RBF/Gaussian kernel: $k(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$. Gives very flexible, smooth, local decision boundaries. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/linear-vs-non-linear-classification-analyzing-differences-using-the-kernel-trick/)\n",
    "- Sigmoid kernel: related to neural networks; used less commonly in practice. [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/linear-vs-non-linear-classification-analyzing-differences-using-the-kernel-trick/)\n",
    "\n",
    "Selecting kernels and their hyperparameters is usually done via cross-validation, similar to selecting polynomial degrees and regularization strengths. [inria.github](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_models_feature_engineering_classification.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d1ea8",
   "metadata": {},
   "source": [
    "### 8. Where “nonlinear feature selection” fits\n",
    "\n",
    "**Filter-based nonlinear feature selection**:\n",
    "\n",
    "- Each feature is scored individually using some measure of association with the target (e.g., correlation coefficient, mutual information, dependence measure). [aml4td](https://aml4td.org/chapters/interactions-nonlinear.html)\n",
    "- Features are ranked by this score, and those below a threshold are discarded. [aml4td](https://aml4td.org/chapters/interactions-nonlinear.html)\n",
    "\n",
    "These methods:\n",
    "\n",
    "- Are cheap and scale well to very high dimensions because they look at one feature at a time. [aml4td](https://aml4td.org/chapters/interactions-nonlinear.html)\n",
    "- Help address multicollinearity by removing redundant or weak features, but they **ignore interactions**, so they may miss features that are only useful in combination. [aml4td](https://aml4td.org/chapters/interactions-nonlinear.html)\n",
    "\n",
    "In contrast, embedded methods (like LASSO or tree-based models) consider the model structure and can capture interactions, often giving better predictive performance, though at higher computational cost. [emagine](https://www.emagine.org/blogs/example-of-non-linear-machine-learning-algorithms-decision-trees/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
