{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4ffea7",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Model selection refers to picking the best model architecture or set of features for your prediction task. In practical data science, this usually means deciding which columns (features or attributes) of your data actually matter for the prediction you want to make. The right combination of features helps your model make accurate predictions and avoids problems like overfitting or underfitting.\n",
    "\n",
    "- **Feature selection** is often used interchangeably with model selection, since the chosen features directly dictate your model’s structure and complexity.\n",
    "- The process commonly involves examining training and validation error, and sometimes applies statistical or algorithmic approaches to systematically select features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fcf594",
   "metadata": {},
   "source": [
    "#### Polynomial Features Explained\n",
    "\n",
    "Polynomial features involve expanding your dataset by creating new columns derived from existing features, raised to powers or multiplied together. This technique is often used for linear models when you suspect that relationships between variables aren’t strictly linear.\n",
    "\n",
    "- If you start with a dataset containing just one feature (e.g., horsepower), applying a polynomial transformation of degree 2 creates these features: horsepower, horsepower squared.\n",
    "- With multiple features (say, horsepower and weight), polynomial transformation yields: horsepower, weight, horsepower squared, horsepower * weight, weight squared.\n",
    "- The number of possible polynomial features grows quickly as you increase the degree or number of initial features, which can add complexity but also risk overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e687cc6",
   "metadata": {},
   "source": [
    "\n",
    "#### Model Complexity vs. Error\n",
    "\n",
    "As you add complexity (by increasing polynomial degree or adding more features), something interesting happens:\n",
    "- **Training error usually decreases** (the model fits your training data better).\n",
    "- **Validation/test error can decrease at first but may later increase** (the model overfits your training data and struggles with new/unseen data).\n",
    "- This effect is called the **bias-variance tradeoff**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc82b43",
   "metadata": {},
   "source": [
    "#### The Challenge: Too Many Feature Combinations\n",
    "\n",
    "When dealing with multiple features and polynomial expansions, the number of possible feature combinations explodes.\n",
    "- Example: With 5 initial features and using degree 3 polynomial transformation, you could end up with 55 generated features.\n",
    "- To find the best subset of features, you could, in theory, try every combination (exhaustive search), but that’s rarely practical. For 55 features, there are $2^{55}$ possible combinations—far more than you could ever test quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1102ca1e",
   "metadata": {},
   "source": [
    "### Sequential Feature Selection\n",
    "\n",
    "This is a systematic way to build models by **incrementally adding or removing features**:\n",
    "\n",
    "- **Forward selection**: Start with no features, then add them one by one, always choosing the feature that most improves performance until adding new features doesn’t help.\n",
    "- **Backward elimination**: Start with all features, then remove them one by one, always choosing the feature whose removal least hurts performance, stopping when further removal worsens model accuracy.\n",
    "- **Recursive feature elimination**: Features are recursively pruned from the entire set to find those most contributing to the prediction.\n",
    "\n",
    "These methods help you find a decent subset of features without having to search every combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d04239",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization is a set of techniques designed to prevent overfitting by adding a penalty for complexity to your model’s training process.\n",
    "\n",
    "- Common types include **L1 regularization (Lasso)** and **L2 regularization (Ridge)**.\n",
    "- By penalizing large weights in your regression model, regularization encourages models to use fewer features and simpler relationships, leading to better generalization on new data.\n",
    "- Regularization is particularly powerful when you have many features, as it can \"shrink\" the less useful ones toward zero, often effectively removing them from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19504b",
   "metadata": {},
   "source": [
    "\n",
    "#### Limitations of Exhaustive Search\n",
    "\n",
    "- For a small number of features (like fewer than 10), it’s possible to fit models for every possible combination and pick the best one.\n",
    "- For larger feature sets, exhaustive search becomes computationally infeasible—\"at a millisecond per model, it would take around a million years\" to test every possibility with 55 features.\n",
    "- Sequential selection and regularization make model selection practical and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a8557",
   "metadata": {},
   "source": [
    "\n",
    "### Real-World Example: Vehicle Dataset\n",
    "\n",
    "Suppose you’re predicting car price based on horsepower, weight, and displacement.\n",
    "- Using polynomial features of degree 2 gives you nine features: each original variable, each squared, and all pairwise products.\n",
    "- Sequential selection or regularization can help you figure out if only a few of these features actually matter, instead of blindly using them all."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
