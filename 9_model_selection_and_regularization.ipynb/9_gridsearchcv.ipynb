{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda2c93d",
   "metadata": {},
   "source": [
    "## Grid Search CV\n",
    "\n",
    "Grid Search CV is a powerful technique used in machine learning to tune hyperparameters like alpha in ridge regression by systematically trying out various values and selecting the one that optimizes model performance based on cross-validation results. \n",
    "\n",
    "It automates the tedious process of manually testing each hyperparameter and helps in preventing overfitting by finding the best generalization performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee5c19",
   "metadata": {},
   "source": [
    "### Explanation of Grid Search CV for Hyperparameter Tuning\n",
    "\n",
    "Grid Search CV works by:\n",
    "- Defining a grid of hyperparameter values to try.\n",
    "\n",
    "- For each value (or combination of values), the model is trained and evaluated using cross-validation.\n",
    "\n",
    "- The performance scores, often mean squared error for regression, are collected.\n",
    "\n",
    "- The best hyperparameters are selected based on the metric, such as the lowest dev set error.\n",
    "\n",
    "- The best model can then be used for prediction, with accessible attributes like coefficients for interpretation.\n",
    "\n",
    "This method reduces variance in model evaluation by using k-fold cross-validation, helping to avoid overfitting to a particular validation set. The GridSearchCV object in scikit-learn holds detailed results on training times, rankings, and scores, allowing you to analyze the tuning process for debugging or understanding model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384e78e",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "- GridSearchCV allows tuning multiple hyperparameters simultaneously.\n",
    "\n",
    "- It requires specifying the hyperparameter names with the step names in pipelines, joined by double underscores.\n",
    "\n",
    "- The fit method is called on the entire dataset, internally performing cross-validation splits for train/dev.\n",
    "\n",
    "- The best estimator can be accessed via `.best_estimator_`, and predictions should be made using this best estimator rather than the GridSearchCV object directly.\n",
    "\n",
    "- Cross-validation results are accessible through `.cv_results_` for detailed inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4173b1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 26.826957952797272\n",
      "Development set MSE with best alpha: 2863.8889199207347\n",
      "     Coefficient\n",
      "age     1.981092\n",
      "sex   -10.371246\n",
      "bmi    24.775212\n",
      "bp     15.756054\n",
      "s1     -6.968186\n",
      "s2     -3.357444\n",
      "s3     -8.494348\n",
      "s4      7.543101\n",
      "s5     19.927514\n",
      "s6      3.438558\n"
     ]
    }
   ],
   "source": [
    "# Example Python Implementation with Ridge Regression on a Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Toy dataset for demonstration\n",
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and development sets\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline with standard scaler and ridge regression\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "# Define alpha values to search over on log scale\n",
    "alpha_values = np.logspace(-5, 4, 50)\n",
    "\n",
    "# Setup GridSearchCV with the hyperparameter dictionary\n",
    "param_grid = {'ridge__alpha': alpha_values}\n",
    "\n",
    "# Create GridSearchCV object with negative mean squared error as scoring\n",
    "grid_search = GridSearchCV(pipe, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Fit GridSearchCV on training + dev data (cross-validation internally)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best alpha found\n",
    "best_alpha = grid_search.best_estimator_.named_steps['ridge'].alpha\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "# Predict on dev set with best estimator\n",
    "y_pred = grid_search.best_estimator_.predict(X_dev)\n",
    "dev_mse = mean_squared_error(y_dev, y_pred)\n",
    "print(f\"Development set MSE with best alpha: {dev_mse}\")\n",
    "\n",
    "# Extract coefficients from the best model\n",
    "coefficients = grid_search.best_estimator_.named_steps['ridge'].coef_\n",
    "coef_df = pd.DataFrame(coefficients, index=data.feature_names, columns=['Coefficient'])\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8565b",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- The code scales features to z-scores using StandardScaler.\n",
    "\n",
    "- It performs a grid search over 50 alpha values from $10^{-5}$ to $10^{4}$.\n",
    "\n",
    "- The best alpha minimizes the dev set mean squared error found by cross-validation.\n",
    "\n",
    "- The coefficients from the best ridge regression model can be interpreted relative to the standardized features.\n",
    "\n",
    "- This approach automates finding optimal regularization strength to balance underfitting and overfitting in the model.\n",
    "\n",
    "This method matches the experimental description where a wide range of alphas are tested, and the best value (around alpha=20-23) is selected, showing how regularization improves unseen data performance while managing model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde7dee",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[1](https://dev.to/anurag629/gridsearchcv-in-scikit-learn-a-comprehensive-guide-2a72)\n",
    "[2](https://www.youtube.com/watch?v=4XJX_Sb5Zf0)\n",
    "[3](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)\n",
    "[4](https://alfurka.github.io/2018-11-18-grid-search/)\n",
    "[5](https://www.geeksforgeeks.org/machine-learning/hyperparameter-tuning/)\n",
    "[6](https://stackoverflow.com/questions/43530761/cross-validation-with-grid-search-returns-worse-results-than-default)\n",
    "[7](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "[8](https://www.youtube.com/watch?v=cOos6wRMpAU)\n",
    "[9](https://www.reddit.com/r/datascience/comments/ydo3fj/thoughts_on_hyperparameter_tuning_using_sklearns/)\n",
    "[10](https://www.geeksforgeeks.org/machine-learning/comparing-randomized-search-and-grid-search-for-hyperparameter-estimation-in-scikit-learn/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
