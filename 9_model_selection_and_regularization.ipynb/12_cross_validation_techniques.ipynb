{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60c2627",
   "metadata": {},
   "source": [
    "Alternative cross-validation techniques in machine learning allow you to robustly estimate model performance and tune hyperparameters, especially when the dataset size or characteristics make “simple” train-test splits less reliable. Three major alternatives are holdout, k-fold, and leave-one-out cross-validation, each with distinct strengths, weaknesses, and suitable Python implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4131c93",
   "metadata": {},
   "source": [
    "### Holdout Cross-Validation\n",
    "\n",
    "Holdout cross-validation—often called train/test split—is the simplest approach. The dataset is randomly split into two sets: a training set (typically 70–80% of the data) and a test set (remaining 20–30%). The model is trained once on the training set, then evaluated on the test set. It’s fast and appropriate for large datasets, but since only part of the data is used for training, the final evaluation can be prone to variance if the split happens to be unrepresentative. In scikit-learn, this split is easily accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735935ff",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "LOOCV is most useful for small datasets. The procedure trains the model on all but one sample, using the remaining sample as the test fold, repeating this once for each sample in the dataset. Every data point is eventually used once as a test set and $ n-1 $ times for training. LOOCV yields nearly unbiased performance estimates, though it can be computationally expensive for large datasets due to the need to train the model $ n $ times.[2][1]\n",
    "\n",
    "Implementation in scikit-learn is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae424e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # Fit/train your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7bf44",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "\n",
    "K-fold cross-validation improves upon simple holdout by using each sample for both training and validation exactly once. The dataset is split into $ k $ equal-sized folds. For each round, one fold is held back for validation while the other $ k-1 $ folds are used for training. This repeats $ k $ times, and error results are averaged for a more robust and less noisy estimate of model quality. Choices for $ k $ typically include 5, 10, or even the full dataset size (n), which is equivalent to LOOCV.[4]\n",
    "\n",
    "In scikit-learn, you can perform k-fold cross-validation with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39398c",
   "metadata": {},
   "source": [
    "### Summary Table\n",
    "\n",
    "| Technique         | Usage             | Bias/Variance | Computational Cost | Python Implementation              |\n",
    "|-------------------|-------------------|---------------|-------------------|------------------------------------|\n",
    "| Holdout           | Large datasets    | High bias     | Low               | `train_test_split`             |\n",
    "| K-Fold (5 or 10)  | General datasets  | Moderate      | Moderate          | `KFold` + `cross_val_score`    |\n",
    "| LOOCV             | Small datasets    | Lowest bias   | Highest           | `LeaveOneOut`                  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
