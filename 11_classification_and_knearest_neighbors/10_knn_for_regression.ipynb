{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f59307a",
   "metadata": {},
   "source": [
    "## KNN for Regression: Same Idea, Different Output\n",
    "\n",
    "K-Nearest Neighbors works for **regression** just like classification, but instead of **majority vote**, it predicts the **average** (mean) of the k nearest neighbors' target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663aaf65",
   "metadata": {},
   "source": [
    "### How KNN Regression Works (Debt vs Income Example)\n",
    "\n",
    "**Key behaviors** (same as classification):\n",
    "- **Small k** (k=1): Jagged, wiggly prediction line (overfits training data)  \n",
    "- **Large k** (k=220): Flat, smooth line (underfits, ignores local patterns)  \n",
    "- **Sweet spot k**: Use cross-validation with **MSE** (mean squared error) as the metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fba2f9",
   "metadata": {},
   "source": [
    "### Scikit-learn Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor  # Note: Regressor, not Classifier\n",
    "\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=50)\n",
    "knn_reg.fit(X_train, y_train)  # X=income, y=debt\n",
    "y_pred = knn_reg.predict(X_test)  # Outputs numbers, not classes\n",
    "```\n",
    "\n",
    "**Output**: Continuous numbers (debt amounts), not categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4264d",
   "metadata": {},
   "source": [
    "### Visual Pattern (Same as Classification)\n",
    "\n",
    "```\n",
    "k=1:   Erratic line through training points\n",
    "k=50:  Smooth, locally adaptive curve  \n",
    "k=220: Almost flat (global average)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a25af3",
   "metadata": {},
   "source": [
    "### Why Less Common in Practice\n",
    "\n",
    "- **Computationally expensive**: Must compute distances to *all* training points for every prediction  \n",
    "- **Memory intensive**: Stores entire training dataset  \n",
    "- **Linear models** (ridge, lasso) or **tree ensembles** (random forest, gradient boosting) usually perform better + faster  \n",
    "\n",
    "**Still valuable**: Great for understanding \"local averaging,\" prototyping, or when you want interpretable \"similar examples\" reasoning.\n",
    "\n",
    "**Takeaway**: KNN classification = vote, KNN regression = average. Same distance-finding core, different aggregation. Tune k with CV using accuracy (classification) or MSE (regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2590b",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[1](https://www.geeksforgeeks.org/machine-learning/k-nearest-neighbours/)\n",
    "[2](https://neptune.ai/blog/knn-algorithm)\n",
    "[3](https://www.pinecone.io/learn/k-nearest-neighbor/)\n",
    "[4](https://www.youtube.com/watch?v=G275SvYjg2o)\n",
    "[5](https://datasciencebook.ca/regression1.html)\n",
    "[6](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "[7](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm)\n",
    "[8](https://www.ibm.com/think/topics/knn)\n",
    "[9](https://www.youtube.com/watch?v=b6uHw7QW_n4)\n",
    "[10](https://www.youtube.com/watch?v=HVXime0nQeI)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
