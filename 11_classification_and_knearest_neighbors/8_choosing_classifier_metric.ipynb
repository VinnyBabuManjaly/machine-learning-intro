{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cbbe0a",
   "metadata": {},
   "source": [
    "Precision, recall, thresholds, and curves are all about controlling *which* errors a classifier makes and how often. In the loan example, the model must balance making bad loans (false positives) versus missing good customers (false negatives), and that balance is controlled largely through the **decision threshold** on predicted probabilities.\n",
    "\n",
    "#### Precision vs Recall in Plain Language\n",
    "\n",
    "- **Precision (for “Paid” class)**  \n",
    "  - Question: *“If the model predicts this person will pay, how often is that correct?”*  \n",
    "  - High precision ⇒ very few bad loans approved (few risky customers labeled as “Paid”).  \n",
    "  - A **pessimistic** classifier (only very safe customers get “Paid”) tends to have **high precision** and **low recall**.\n",
    "\n",
    "- **Recall (for “Paid” class)**  \n",
    "  - Question: *“Of all people who would actually pay, how many does the model catch?”*  \n",
    "  - High recall ⇒ very few good customers missed (few solid customers labeled as “Did not pay”).  \n",
    "  - An **optimistic** classifier (approves almost everyone) tends to have **high recall** and **low precision**.\n",
    "\n",
    "For loans:\n",
    "- High precision = **low risk, low reward** (few bad loans, but you miss many good customers).  \n",
    "- High recall = **high risk, high reward** (you approve many good customers, but also many bad ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9342f5",
   "metadata": {},
   "source": [
    "#### Decision Threshold T: How It Controls Precision & Recall\n",
    "\n",
    "Most probabilistic classifiers (including KNN with `predict_proba`) output a probability for each class, e.g. P(Paid). The **decision threshold T** says:  \n",
    "- If P(Paid) → predict “Paid” (class 1)  \n",
    "- Else → predict “Did not pay” (class 0)\n",
    "\n",
    "Scikit-learn’s default is typically **T = 0.5** and “ties go to class 0” (i.e., P(Paid) $ \\leq $ 0.5 → “Did not pay”).\n",
    "\n",
    "Now see what happens as T changes:\n",
    "\n",
    "- **Lower T (e.g., 0.2)**  \n",
    "  - Easier to get classified as “Paid”.  \n",
    "  - More positives predicted → **higher recall**, but also more false positives → **lower precision**.\n",
    "\n",
    "- **Higher T (e.g., 0.8)**  \n",
    "  - Harder to get classified as “Paid”.  \n",
    "  - Fewer positives predicted → **higher precision** (when you say “Paid” you’re very confident), but more missed positives → **lower recall**.\n",
    "\n",
    "So:\n",
    "- **Raising T** makes the classifier more **pessimistic** (few “Paid” predictions) → higher precision, lower recall.  \n",
    "- **Lowering T** makes it more **optimistic** (many “Paid” predictions) → higher recall, lower precision.\n",
    "\n",
    "Hyperparameters like `n_neighbors` in KNN also affect this trade-off, but threshold tuning is often more direct and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba75957",
   "metadata": {},
   "source": [
    "#### Precision–Recall Curve: Seeing All Thresholds at Once\n",
    "\n",
    "Instead of picking a single T, you can vary T from 0 to 1 and compute precision and recall for each value. Plotting precision (y-axis) vs recall (x-axis) gives a **precision–recall (PR) curve**.\n",
    "\n",
    "- Small T (very optimistic) → bottom right region: **high recall, low precision**.  \n",
    "- Large T (very pessimistic) → top left region: **high precision, low recall**.  \n",
    "- As T moves from low to high, the curve arcs from bottom-right toward top-left.\n",
    "\n",
    "How to use the PR curve:\n",
    "\n",
    "1. **Choose T using domain knowledge**  \n",
    "   - For loans, maybe pick a point where:\n",
    "     - Precision is “good enough” (bad loans are rare).  \n",
    "     - Recall is not too low (you still get enough good customers). \n",
    "\n",
    "2. **Compare models independent of T**  \n",
    "   - Plot PR curves for two models on the same axes.  \n",
    "   - The **better model’s curve lies closer to the top-right** (higher precision at same recall and vice versa).  \n",
    "   - You can also summarize a PR curve with **Area Under the Curve (AUPRC)**: higher area ⇒ better overall precision–recall trade-off.\n",
    "\n",
    "A **no-skill (random)** classifier produces:\n",
    "- A roughly horizontal line at precision ≈ ratio of positives in the dataset (e.g., 0.55 if 55% are “Paid”).  \n",
    "- Any useful model’s PR curve should lie above this baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be3ce7",
   "metadata": {},
   "source": [
    "#### ROC Curve and ROC AUC (Related but Different View)\n",
    "\n",
    "Another common view is the **ROC curve**, which plots:\n",
    "\n",
    "- x-axis: False Positive Rate (FPR) = 1 − specificity  \n",
    "- y-axis: True Positive Rate (TPR) = recall\n",
    "\n",
    "As with PR:\n",
    "\n",
    "- Vary T; compute (FPR, TPR); plot all points → ROC curve.  \n",
    "- Being close to the **top-left** (high TPR, low FPR) is best.  \n",
    "- Summarize with **Area Under ROC Curve (ROC AUC)**:\n",
    "  - AUC = 1: perfect classifier  \n",
    "  - AUC = 0.5: random guessing\n",
    "\n",
    "ROC curves are popular enough that tools like cross-validation and model selection can target **“maximize ROC AUC”** instead of accuracy. GridSearch-style tuning over different `k` values in KNN could produce a best `k` for **ROC AUC** that was different from the best `k` for **accuracy**, showing that the “best model” depends on which metric matters for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d0ace",
   "metadata": {},
   "source": [
    "#### How to Choose a Metric (and Threshold) in Practice\n",
    "\n",
    "For the loan example (and many real problems), the steps are:\n",
    "\n",
    "- Decide what matters more:\n",
    "  - Avoiding bad loans (FPs) ⇒ **precision** is critical.  \n",
    "  - Not missing good customers (FNs) ⇒ **recall** is critical.  \n",
    "  - Balanced errors on imbalanced data ⇒ **F1 score** (harmonic mean of precision and recall).  \n",
    "  - Simple overall correctness on balanced data ⇒ **accuracy**.\n",
    "\n",
    "- Use:\n",
    "  - **PR curves** to tune the threshold T in terms of precision vs recall.  \n",
    "  - **ROC curves / ROC AUC** to compare overall discriminative power across models.  \n",
    "  - **Domain expertise** (e.g., cost of one bad loan vs profit from one good loan) to pick a reasonable operating point on the curve.\n",
    "\n",
    "Optimistic vs pessimistic classifiers, threshold tuning, and PR/ROC curves are all ways of expressing the same core idea: **a classifier is not just “good” or “bad”; you choose where to operate along its trade-off curve depending on real-world costs and goals.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
